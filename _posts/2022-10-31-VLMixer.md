---
layout: post
title:  "[ICML2022] VLMixer: Unpaired Vision-Language Pre-training via Cross-Modal CutMix"
date:   2022-10-31 17:44:00 +0900
categories: [Vision-and-Language, Transformer]
---
[[pdf]](https://arxiv.org/pdf/2206.08919.pdf)  &emsp;
[[github]](https://github.com/ttengwang/VLMixer) <br>

**Teng Wang<sup>1 2</sup>, Wenhao Jinag<sup>3</sup>, Zhichao Lu<sup>1</sup>, Feng Zheng<sup>1</sup>, Rang Cheng<sup>1</sup>, Chengguo Yin<sup>3</sup>, Ping Luo<sup>2</sup>**
<br><sup>1</sup>Department of Computer Science and Engineering, Southern University of Science and Technology  &emsp; <sup>2</sup>Department of Computer Science, The University of Hong Kong &emsp; <sup>3</sup>Data Platform, Tencent

![image](https://user-images.githubusercontent.com/42200027/198969481-c5dcb5e8-fd58-4348-bab7-fb94b34169f7.png)

# Abstract
- 기존의 vision-and-language pre-training (VLP) 방법들은 paired image-text dataset 에 의존하지만, 그 것들은 가공이 어렵고 human labor 가 많이 필요하다. 
- 이 논문은 large-scale Text-only corpora 와 image-only corpora 의 데이터로부터, <span style='color:green;font-weight:bold'> cross-modal CutMix (CMC) </span> 라는 augmentation 방법을 소개한다. 이 방법은 위의 그림처럼 자연어 문장 속의 visually-grounded words 를 이미지 패치로 바꾸어 <span style='color:green;font-weight:bold'> multi-modal sentence </span> 로 만드는 방법이다. CMC augmentation 방법을 통해 aligned pair 가 적은 data scarcity 를 극복 가능하고, token-level 의 denoising 이 향상된다. -
- 추가적으로, <span style='color:green;font-weight:bold'> VLMIXer </span> 라는 방법을 통해 contrastive learning 방법을 소개한다. 

# Introduction 
현재 많은 Vision-and-Language pre-training (VLP) 태스크들은 MSCOCO, Visual Genome 같은 <span style='color:green;font-weight:bold'> manually-labeled and well-aligned </span> dataset 들과 high-capacity transformer model 을 통해 학습된다. 이 transformer 의 학습에서, (1) global-level 에서는 pre-training loss 로 사용되는 image-text matching loss 를 통해 alignment 를 학습하고, (2) instance-level 에서는 self-attention layer 가 두 modality 의 input token 들의 fine-grained interaction 을 학습한다. 그러나 이러한 well-aligned dataset 을 학습하는 모델들의 성능은 saturate 되었고, weakly-aligned pair 를 학습하여 scale-up 하려는 시도가 존재해왔다. 한 Unsupervised VLP ([U-Visual BERT](https://arxiv.org/pdf/2010.12831.pdf)) 에서는 standalonge image and text corpus 로 multi-modal representation 을 학습한다.

그러나 이 연구에서는 image tag 를 두 modality 를 연결(bridge)하기 위한 intermediate representation 으로 활용하는데, 이는 complex image 에 적절하지 않다는 점을 지적한다. 또 이러한 방법으로는 NLVR, image-text retrieval 같은 fine-grained alignment 에 의존하는 downstream task 에 취약하다.

이 연구에서는 <span style='background-color: #dcffe4'> cross-modal CutMix (CMC) 방법을 통해 "multi-modal sentence" </span>를 생성하여 이를 해결한다. 그림의 방법처럼 image patch gallary 로 부터 자연어 문장의 visual-grounded word 를 patch 로 바꾸어 multimodal transformer 의 input 으로 넣어주면, 기존의 ***mask-then-predict*** 방법으로 token-level alignment 학습이 가능하다. 또 추가적으로, 두 모달리티의 효과적인 <span style='background-color: #dcffe4'> instance-level alignment 를 위한 contrastive learning framework </span> 를 제안한다. 이는 multimodal sentence 와 그에 해당하는 text sentence 를 같은 semantic correspondance 문장들끼리 가깝게하고 그렇지 않은 negative sample 을 멀게한다. 이를 통해  instance-level image-text alingment 학습을 효과적으로 수행할 수 있다.

# Related Works 
![image](https://user-images.githubusercontent.com/42200027/198978703-ab8c594b-b89c-4776-81e0-d0168070a6fc.png)

기존의 방법들은 위의 그림에서 A.B.C 에 해당하는데, A.B 에 해당하는 Vision-and-Language 학습 방법은 image-text pair를 필요로 한다. Vinalla 하게 multimodal input 을 다루는 A. 방법과 다르게 B. [Oscar](https://arxiv.org/abs/2004.06165) style 은 tag anchor를 활용한다. C. 의 [U-Visual BERT](https://arxiv.org/pdf/2010.12831.pdf) 에서는 text 와 image pair 가 아닌 unpaired set 으로도 학습이 가능하다. 하지만, U-Visual BERT 는 텍스트에 해당하는 image tag 만을 활요하기 때문에, <span style='background-color: #dcffe4'> visual region 과 linguistic cue 사이의 interaction 을 볼 수 없고, explicit 한 matching supervision(tag) 가 없을 경우 alignment 학습이 불가능하다</span>는 단점이 있다고 지적한다. 논문에서 제안하는 VLMIxer 의 경우, patch tag 를 통해 첫 번째 문제점을 해결할 수 있고, contrastive loss 를 이용하여 tag 가 없는 두 번째 경우도 해결 가능하다.

# VLMixer Pre-training
VLMIxer 는 두 가지 parallel pre-training branch 를 갖는데, 하나는 Visually-aided language pre-training (VALP) 이고, 다른 하나는 tag-aided visual pre-training (TAVP) 이다. VALP 는 cross-modal cutmix (CMC) 를 활용하고, TAVP는 image-only dataset 에서 image 만 주어질 때, image tag 를 text modality 로 하여 [U-Visual BERT](https://arxiv.org/pdf/2010.12831.pdf) 와 같은 방법으로 학습을 진행한다.

## Cross-Modal CutMix 
<span style='color:green;font-weight:bold'> Patch gallaery </span>

image-only dataset 에서 off-the-shelf concept(patch) detector (ex. [Faster RCNN](https://arxiv.org/pdf/1506.01497.pdf)) 를 활용하여 visual patch gallery 를 구성한다. *w* 는 concept label, *c* 는 confidence score 이다. 그리고 concept 주변의 "contextual concept" 을 추가적으로 저장한다. *i*-th concept 과 그에 해당하는 각 *j*-th contextual concept 들을 통해 다음 식과 같이 gallery 가 구성된다. 

![image](https://user-images.githubusercontent.com/42200027/198982844-c6ace1cf-5b02-42dc-b4ee-9b432be922ff.png)

<span style='color:green;font-weight:bold'> CutMix visual patches into sentence </span>
![image](https://user-images.githubusercontent.com/42200027/198983738-cfbc0b1a-7c00-48f0-9e3c-2a31effbe134.png)

CMC 의 각 word token 은 patcdh x_q with q ~ Norm({P_i}) 로 바뀐다. 식에서, G_i 는 *i-th* concept 의 "contextual concpet" 들이고, 식을 해석하면, <span style='background-color: #dcffe4'> 자연어 문장의 워드 토큰에 대하여 diversity 를 위해 patch gallery 속의 concept 들을 그 각 주변의 contextual concept 을 고려한 확률을 부여한 뒤 normalize 하여 q 라는 확률을 부여한 뒤, q-distribution 에서 patch x_q 를 뽑는다 </span> 는 것이다. 이후, word token 이 x_q patch 로 바뀌는 것은 r_cmc 확률을 통해 결정된다.

<span style='color:green;font-weight:bold'> K-shot CMC. </span>

Divesity 를 위하여 r_cmc 확률을 통해 patch 로 바뀌는 과정을 K 번 반복하여 K 개의 concept 을 patch 활용한다. 따라서 최종적인 multimodal token 으로 이뤄진 문장은 아래와 같다. 
![image](https://user-images.githubusercontent.com/42200027/198984999-c54c6e9a-8596-45fa-b7fa-c61505ded91f.png)

## Visually-Aided Language Pre-training (VALP)
![image](https://user-images.githubusercontent.com/42200027/198985457-f336f7c2-b2cc-4027-b353-c587d9d0a488.png)

Backbone 은 [Vaswani Transformer ](https://arxiv.org/pdf/1706.03762.pdf) 이며, Masekd Language Modeling (MLM) 과 cross-modal contrastive Loss (CMCL) 가 활용된다.

<span style='color:green;font-weight:bold'>Masked language modeling (MLM). </span>
![image](https://user-images.githubusercontent.com/42200027/198985944-e62f1978-6182-4014-b3d8-d987f7f4591d.png)

MLM 의 방법은 기존의 [BERT](https://aclanthology.org/N19-1423.pdf) 와 유사하다. 15% 의 확률로 Mask 된다.
![image](https://user-images.githubusercontent.com/42200027/198986167-8dec62f2-a318-4a3e-a3b2-3b65d5465b76.png)

<span style='color:green;font-weight:bold'> Cross-modal contrastive learning (CMCL). </span>

Unpaired VLP 에서 contrastive loss 를 구성하기 위해, multimodal sentence S_M 과 그에 해당하는 바뀌기 전의 자연어 문장 T_M 에 대해 matching 되는 것을 positive sample, 그렇지 않은 것을 negative sample 로 하여 아래와 같이 contrastive loss 를 구성한다. *f* 는 [[CLS]] token 의 cosine similarity 이다. 
![image](https://user-images.githubusercontent.com/42200027/198986466-959dcf34-276a-46a0-8b2e-48230e55981f.png)

## Tag-Aided Visual Pre-training (TAVP)

TAVP 는 visual-only data 로 부터 multi-modal knowledge 를 추출하기 위해 활용된다. TAVP는 image-only dataset 에서 image 만 주어질 때, image tag 를 text modality 로 하여 [U-Visual BERT](https://arxiv.org/pdf/2010.12831.pdf) 와 같은 방법으로 학습을 진행한다. [Oscar](https://arxiv.org/abs/2004.06165) 와 같이 15% 확률을 통한 *Mask-tehn-predict* pre-training 을 통해 loss 를 구성한다.

![image](https://user-images.githubusercontent.com/42200027/198987315-84af6075-4de5-4dd6-af85-ae039adf6e51.png)

최종적인 Loss 는 아래와 같다. 
![image](https://user-images.githubusercontent.com/42200027/198987367-3b3133eb-b8c2-4631-b502-4c827f0b0075.png)

# Experiments

Fair 한 비교를 위해 unpaired vision-and-language task 로의 진행을 위해 alignment information 없이 paired dataset 에 대해 성능 검증을 한다.
Pre-training dataset 은 아래와 같다.
![image](https://user-images.githubusercontent.com/42200027/198987782-6df7e6d3-30fd-4109-9fb7-3a93ca66c276.png)

<span style='color:green;font-weight:bold'> Comparison with State-of-the-Art Methods </span>
![image](https://user-images.githubusercontent.com/42200027/198987834-d3fffd5e-89e2-4b7a-b1ed-f18e941e3750.png)

<span style='color:green;font-weight:bold'> Ablation Study </span>
![image](https://user-images.githubusercontent.com/42200027/198987969-f9d10ed0-f830-494f-9489-1cc255e7a728.png)



