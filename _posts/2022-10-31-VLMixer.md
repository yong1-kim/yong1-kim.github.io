---
layout: post
title:  "[ICML2022] VLMixer: Unpaired Vision-Language Pre-training via Cross-Modal CutMix"
date:   2022-10-31 17:44:00 +0900
categories: [Vision-and-Language, Transformer]
---
[[pdf]](https://arxiv.org/pdf/2206.08919.pdf)  &emsp;
[[github]](https://github.com/ttengwang/VLMixer) <br>

**Teng Wang<sup>1 2</sup>, Wenhao Jinag<sup>3</sup>, Zhichao Lu<sup>1</sup>, Feng Zheng<sup>1</sup>, Rang Cheng<sup>1</sup>, Chengguo Yin<sup>3</sup>, Ping Luo<sup>2</sup>**
<br><sup>1</sup>Department of Computer Science and Engineering, Southern University of Science and Technology  &emsp; <sup>2</sup>Department of Computer Science, The University of Hong Kong &emsp; <sup>3</sup>Data Platform, Tencent

![image](https://user-images.githubusercontent.com/42200027/198969481-c5dcb5e8-fd58-4348-bab7-fb94b34169f7.png)

# Abstract
- 기존의 vision-and-language pre-training (VLP) 방법들은 paired image-text dataset 에 의존하지만, 그 것들은 가공이 어렵고 human labor 가 많이 필요하다. 
- 이 논문은 large-sclae Text-only corpora 와 image-only corpora 의 데이터로부터, <span style='color:green;font-weight:bold'> cross-modal CutMix (CMC) </span> 라는 augmentation 방법을 소개한다. 이 방법은 위의 그림처럼 자연어 문장 속의 visually-grounded words 를 이미지 패치로 바꾸어 <span style='color:green;font-weight:bold'> multi-modal sentence </span> 로 만드는 방법이다. CMC augmentation 방법을 통해 aligned pair 가 적은 data scarcity 를 극복 가능하고, token-level 의 denoising 이 향상된다. -
- 추가적으로, <span style='color:green;font-weight:bold'> VLMIXer </span> 라는 방법을 통해 contrastive learning 방법을 소개한다. 

# Introduction 
현재 많은 Vision-and-Language pre-training (VLP) 태스크들은 MSCOCO, Visual Genome 같은 <span style='color:green;font-weight:bold'> manually-labeled and well-aligned </span> dataset 들과 high-capacity transformer model 을 통해 학습된다. 이 transformer 의 학습에서, (1) gloabl-level 에서는 pre-training loss 로 사용되는 image-text matching loss 를 통해 alignment 를 학습하고, (2) instance-level 에서는 self-attention layer 가 두 modality 의 input token 들의 fine-grained interaction 을 학습한다. 그러나 이러한 well-aligned dataset 을 학습하는 모델들의 성능은 satruate 되었고, weakly-aligned pair 를 학습하여 scale-up 하려는 시도가 존재해왔다. 한 [Unsupervised VLP](https://arxiv.org/pdf/2010.12831.pdf) 에서는 standalonge image and text corpus 로 multi-modal represeentation 을 학습한다. 
