---
layout: post
title:  "[ICML2022] VLMixer: Unpaired Vision-Language Pre-training via Cross-Modal CutMix"
date:   2022-10-31 17:44:00 +0900
categories: [Vision-and-Language, Transformer]
---
[[pdf]](https://arxiv.org/pdf/2206.08919.pdf)  &emsp;
[[github]](https://github.com/ttengwang/VLMixer) <br>

**Teng Wang<sup>1 2</sup>, Wenhao Jinag<sup>3</sup>, Zhichao Lu<sup>1</sup>, Feng Zheng<sup>1</sup>, Rang Cheng<sup>1</sup>, Chengguo Yin<sup>3</sup>, Ping Luo<sup>2</sup>**
<br><sup>1</sup>Department of Computer Science and Engineering, Southern University of Science and Technology  &emsp; <sup>2</sup>Department of Computer Science, The University of Hong Kong &emsp; <sup>3</sup>Data Platform, Tencent

![image](https://user-images.githubusercontent.com/42200027/198969481-c5dcb5e8-fd58-4348-bab7-fb94b34169f7.png)

# Abstract
- 기존의 vision-and-language pre-training (VLP) 방법들은 paired image-text dataset 에 의존하지만, 그 것들은 가공이 어렵고 human labor 가 많이 필요하다. 
- 이 논문은 large-scale Text-only corpora 와 image-only corpora 의 데이터로부터, <span style='color:green;font-weight:bold'> cross-modal CutMix (CMC) </span> 라는 augmentation 방법을 소개한다. 이 방법은 위의 그림처럼 자연어 문장 속의 visually-grounded words 를 이미지 패치로 바꾸어 <span style='color:green;font-weight:bold'> multi-modal sentence </span> 로 만드는 방법이다. CMC augmentation 방법을 통해 aligned pair 가 적은 data scarcity 를 극복 가능하고, token-level 의 denoising 이 향상된다. -
- 추가적으로, <span style='color:green;font-weight:bold'> VLMIXer </span> 라는 방법을 통해 contrastive learning 방법을 소개한다. 

# Introduction 
현재 많은 Vision-and-Language pre-training (VLP) 태스크들은 MSCOCO, Visual Genome 같은 <span style='color:green;font-weight:bold'> manually-labeled and well-aligned </span> dataset 들과 high-capacity transformer model 을 통해 학습된다. 이 transformer 의 학습에서, (1) global-level 에서는 pre-training loss 로 사용되는 image-text matching loss 를 통해 alignment 를 학습하고, (2) instance-level 에서는 self-attention layer 가 두 modality 의 input token 들의 fine-grained interaction 을 학습한다. 그러나 이러한 well-aligned dataset 을 학습하는 모델들의 성능은 saturate 되었고, weakly-aligned pair 를 학습하여 scale-up 하려는 시도가 존재해왔다. 한 Unsupervised VLP ([U-Visual BERT](https://arxiv.org/pdf/2010.12831.pdf)) 에서는 standalonge image and text corpus 로 multi-modal representation 을 학습한다.

그러나 이 연구에서는 image tag 를 두 modality 를 연결(bridge)하기 위한 intermediate representation 으로 활용하는데, 이는 complex image 에 적절하지 않다는 점을 지적한다. 또 이러한 방법으로는 NLVR, image-text retrieval 같은 fine-grained alignment 에 의존하는 downstream task 에 취약하다.

이 연구에서는 <span style='background-color: #dcffe4'> cross-modal (CMC) 방법을 통해 "multi-modal sentence" </span>를 생성하여 이를 해결한다. 그림의 방법처럼 image patch gallary 로 부터 자연어 문장의 visual-grounded word 를 patch 로 바꾸어 multimodal transformer 의 input 으로 넣어주면, 기존의 ***mask-then-predict*** 방법으로 token-level alignment 학습이 가능하다. 또 추가적으로, 두 모달리티의 효과적인 <span style='background-color: #dcffe4'> instance-level alignment 를 위한 contrastive learning framework </span> 를 제안한다. 이는 multimodal sentence 와 그에 해당하는 text sentence 를 같은 semantic correspondance 문장들끼리 가깝게하고 그렇지 않은 negative sample 을 멀게한다. 이를 통해  instance-level image-text alingment 학습을 효과적으로 수행할 수 있다.

# Related Works 
![image](https://user-images.githubusercontent.com/42200027/198978703-ab8c594b-b89c-4776-81e0-d0168070a6fc.png)

기존의 방법들은 위의 그림에서 A.B.C 에 해당하는데, A.B 에 해당하는 Vision-and-Language 학습 방법은 image-text pair를 필요로 한다. Vinalla 하게 multimodal input 을 다루는 A. 방법과 다르게 B. [Oscar](https://arxiv.org/abs/2004.06165) style 은 tag anchor를 활용한다. C. 의 [U-Visual BERT](https://arxiv.org/pdf/2010.12831.pdf) 에서는 text 와 image pair 가 아닌 unpaired set 으로도 학습이 가능하다. 하지만, U-Visual BERT 는 텍스트에 해당하는 image tag 만을 활요하기 때문에, <span style='background-color: #dcffe4'> visual region 과 linguistic cue 사이의 interaction 을 볼 수 없고, explicit 한 matching supervision(tag) 가 없을 경우 alignment 학습이 불가능하다</span>는 단점이 있다고 지적한다. 논문에서 제안하는 VLMIxer 의 경우, patch tag 를 통해 첫 번째 문제점을 해결할 수 있고, contrastive loss 를 이용하여 tag 가 없는 두 번째 경우도 해결 가능하다.

# VLMixer Pre-training
VLMIxer 는 두 가지 parallel pre-training branch 를 갖는데, 하나는 Visually-aided language pre-training (VALP) 이고, 다른 하나는 tag-aided visual pre-training (TAVP) 이다. VALP 는 cross-modal cutmix (CMC) 를 활용하고, TAVP는 image-only dataset 에서 image 만 주어질 때, image tag 를 text modality 로 하여 [U-Visual BERT](https://arxiv.org/pdf/2010.12831.pdf) 와 같은 방법으로 학습을 진행한다.

## Cross-Modal CutMix 
d
