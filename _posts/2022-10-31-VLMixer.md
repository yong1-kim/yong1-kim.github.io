---
layout: post
title:  "[ICML2022] VLMixer: Unpaired Vision-Language Pre-training via Cross-Modal CutMix"
date:   2022-10-31 17:44:00 +0900
categories: [Vision-and-Language, Transformer]
---
[[pdf]](https://arxiv.org/pdf/2206.08919.pdf)  &emsp;
[[github]](https://github.com/ttengwang/VLMixer) <br>

**Teng Wang<sup>1 2</sup>, Wenhao Jinag<sup>3</sup>, Zhichao Lu<sup>1</sup>, Feng Zheng<sup>1</sup>, Rang Cheng<sup>1</sup>, Chengguo Yin<sup>3</sup>, Ping Luo<sup>2</sup>**
<br><sup>1</sup>Department of Computer Science and Engineering, Southern University of Science and Technology  &emsp; <sup>2</sup>Department of Computer Science, The University of Hong Kong &emsp; <sup>3</sup>Data Platform, Tencent

![image](https://user-images.githubusercontent.com/42200027/198969481-c5dcb5e8-fd58-4348-bab7-fb94b34169f7.png)

# Abstract
- 기존의 vision-and-language pre-training (VLP) 방법들은 paired image-text dataset 에 의존하지만, 그 것들은 가공이 어렵고 human labor 가 많이 필요하다. 
- 이 논문은 large-sclae Text-only corpora 와 image-only corpora 의 데이터로부터, <span style='color:green;font-weight:bold'> cross-modal CutMix (CMC) </span> 라는 augmentation 방법을 소개한다. 이 방법은 위의 그림처럼 자연어 문장 속의 visually-grounded words 를 이미지 패치로 바꾸어 <span style='color:green;font-weight:bold'> multi-modal sentence </span> 로 만드는 방법이다. CMC augmentation 방법을 통해 aligned pair 가 적은 data scarcity 를 극복 가능하고, token-level 의 denoising 이 향상된다. -
- 추가적으로, <span style='color:green;font-weight:bold'> VLMIXer </span> 라는 방법을 통해 contrastive learning 방법을 소개한다. 

# Introduction 
현재 많은 Vision-and-Language pre-training (VLP) 태스크들은 MSCOCO, Visual Genome 같은 <span style='color:green;font-weight:bold'> manually-labeled and well-aligned </span> dataset 들과 high-capacity transformer model 을 통해 학습된다. 이 transformer 의 학습에서, (1) gloabl-level 에서는 pre-training loss 로 사용되는 image-text matching loss 를 통해 alignment 를 학습하고, (2) instance-level 에서는 self-attention layer 가 두 modality 의 input token 들의 fine-grained interaction 을 학습한다. 그러나 이러한 well-aligned dataset 을 학습하는 모델들의 성능은 satruate 되었고, weakly-aligned pair 를 학습하여 scale-up 하려는 시도가 존재해왔다. 한 Unsupervised VLP ([U-Visual BERT](https://arxiv.org/pdf/2010.12831.pdf)) 에서는 standalonge image and text corpus 로 multi-modal represeentation 을 학습한다.

그러나 이 연구에서는 image tag 를 두 modality 를 연결(bridge)하기 위한 intermeidate represeentation 으로 활용하는데, 이는 complex image 에 적절하지 않다는 점을 지적한다. 또 이러한 방법으로는 NLVR, image-text retrieval 같은 fine-grained alignment 에 의존하는 downstream task 에 취약하다.

이 연구에서는 <span style='background-color: #dcffe4'> cross-modal (CMC) 방법을 통해 "multi-modal sentence" </span>를 생성하여 이를 해결한다. 그림의 방법처럼 image patch gallary 로 부터 자연어 문장의 visual-grounded word 를 patch 로 바꾸어 multimodal transformer 의 input 으로 넣어주면, 기존의 ***mask-then-predict*** 방법으로 token-level alignment 학습이 가능하다. 또 추가적으로, 두 모달리티의 효과적인 <span style='background-color: #dcffe4'> instance-level alignment 를 위한 contrastive learning framework </span> 를 제안한다. 이는 multimodal sentence 와 그에 해당하는 text sentence 를 같은 semantic correspondance 문장들끼리 가깝게하고 그렇지 않은 negative sample 을 멀게한다. 이를 통해  instance-level image-text alingment 학습을 효과적으로 수행할 수 있다.


