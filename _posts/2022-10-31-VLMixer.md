---
layout: post
title:  "[ICML2022] VLMixer: Unpaired Vision-Language Pre-training via Cross-Modal CutMix"
date:   2022-10-31 17:44:00 +0900
categories: [Vision-and-Language, Transformer]
---
[[pdf]](https://arxiv.org/pdf/2206.08919.pdf)  &emsp;
[[github]](https://github.com/ttengwang/VLMixer) <br>

**Teng Wang<sup>1 2</sup>, Wenhao Jinag<sup>3</sup>, Zhichao Lu<sup>1</sup>, Feng Zheng<sup>1</sup>, Rang Cheng<sup>1</sup>, Chengguo Yin<sup>3</sup>, Ping Luo<sup>2</sup>**
<br><sup>1</sup>Department of Computer Science and Engineering, Southern University of Science and Technology  &emsp; <sup>2</sup>Department of Computer Science, The University of Hong Kong &emsp; <sup>3</sup>Data Platform, Tencent

![image](https://user-images.githubusercontent.com/42200027/198969481-c5dcb5e8-fd58-4348-bab7-fb94b34169f7.png)

# Abstract
기존의 vision-and-language pre-training (VLP) 방법들은 paired image-text dataset 에 의존하지만, 그 것들은 가공이 어렵고 human labor 가 많이 필요하다. 이 논문은 large-sclae Text-only corpora 와 image-only corpora 의 데이터로부터, <span style='color:green;font-weight:bold'> cross-modal CutMix (CMC) </span> 라는 augmentation 방법을 소개한다. 이 방법은 위의 그림처럼 자연어 문장 속의 visually-grounded words 를 이미지 패치로 바꾸어 <span style='color:green;font-weight:bold'> multi-modal sentence </span> 로 만드는 방법이다. CMC augmentation 방법을 통해 aligned pair 가 적은 data scarcity 를 극복 가능하고, token-level 의 denoising 이 향상된다. 추가적으로, <span style='color:green;font-weight:bold'> VLMIXer </span> 라는 방법을 통해 contrastive learning 방법을 소개한다. 
