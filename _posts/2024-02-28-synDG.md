---
layout: post
title:  "[ACL2023] A Synthetic Data Generation Framework for Grounded Dialogues"
date:   2024-02-28 21:00:00 +0900
use_math: true
categories: [Dialogue, PLM]
---

[[pdf]](https://aclanthology.org/2023.acl-long.608v2.pdf) &emsp;
[[github]](https://github.com/HITSZ-HLT/SynDG)

**Jianzhu Bao<sup>1,5</sup>, Rui Wang<sup>1,6</sup>, Yasheng Wang<sup>3</sup>, Aixin Sun<sup>2</sup>, Yitong Li<sup>3,4</sup>, Fei Mi<sup>3</sup>, Ruifeng Xu<sup>1,5,6</sup>**
<br><sup>1</sup> Harbin Institute of Technology, Shenzhen, China <sup>2</sup> Nanyang Technological University, Singapore <sup>3</sup> Huawei Noah’s Ark Lab, Huawei Technologies Co., Ltd. <sup>4</sup> Peng Cheng Laboratory, Shenzhen, China <sup>5</sup> Guangdong Provincial Key Laboratory of Novel Security Intelligence Technologies
 &emsp;

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/478020be-925e-4d43-bcc9-d032a09fc3e3)

# Abstract
- (Motivation) 여타 다른 Dialogue 와 마찬가지로, <span style='background-color: #dcffe4'> grounded-dialog </span>를 generation 하는 것은 매우 costly 하다.
- (**SynDG**) Wikipedia, persona profile 등의 freely available knowledge data 와 pre-trained Language model 을 활용하는 synthetic data generation framework 인 <span style='background-color: #dcffe4'> synDG </span>를 제안한다.
- (**Dialog FLOW**) SynDG 의 key idea 는 Dialog flow 를 통해 coherence 를 유지하는 것이다.
- (Two-level filtering strategy) Synthetic dialog 와 dialog flow 의 coherence 를 위하여 two-level filtering (flow-level and utterance-level) strategy 를 제안한다.
- (Experiment) Full training data 와 low-resource scenario 에서 model performance 를 boost 한다.

# Introduction
이 연구에서는 **Grounded dialog system** 을 다루는데, 이는 knowledge 에 relevant 하고 informative 한 reponse 를 제공하는 대화이다. 
다른 여타 dialog 와 마찬가지로 grounded dialog 역시 데이터셋 부족 문제가 있다.
기존의 다른 방법들 ([RL 을 활용](https://aclanthology.org/2022.sigdial-1.3/)하거나, [user simulation 을 활용](https://aclanthology.org/2022.sigdial-1.21/)) 등이 제안되었지만, 이들은 **Dialog flow** 를 반영하지 않는다.

<span style='background-color: #dcffe4'> Dialog Flow 는 dialogue 의 outline 이라고 할 수 있다. </span> 
Dialog flow 에는 각 session 에서의 content 와 trajectory (topic shift 등) 이 담길 수 있다.
위의 그림에서와 같이, _"husky"_ 에서 _"sled dogs"_ 로, 그리고 다시 _"huskies as pets"_ 로 자연스럽게, dialog 가 흐르는 것을 알 수 있는데, 만약, _"Esquimaux"_ 와 같이, husky 와 같은 wikipedia page 에 등장하지만 다른 knowledge peice 로 대체하면 flow 가 inconsistent 해진다. 
따라서 가장 중요한 것은 dialog flow 을 정교하게 설계하여, coherence 와 smoothness 를 확보하는 것이다.

이에 이 연구에서는 **Syn**thetic **D**ialog **G**eneration (**SynDG**) 를 제안한다.
<span style='background-color: #dcffe4'> 이렇게 생성된 dilaog 는 auxiliary training data 로써 활용될 수 있다. </span>
SynDG 는 **Heuristic** 을 통해 Wikipeida 와 persona knowledge 로 부터 dialog flow 를 만들고, T5 를 이용해 generation 을 진행한다.
이후, Flow-level, Utterance-level 의 two-level filtering 을 통해 quality assurance 를 진행한다.
이후 실험에서, 생성된 두 데이터셋을 additional training dataset 으로 활용하였을 때, 더 좋은 성능을 보여주었다.

# Task Formulation

Training Grounded Dialog $D^t = (C^t_i,K^t_i,r^t_i )^{N_t}_{i=1}$ 에 대해, $C$ 는 dialog context, $K$ 는 knowledge, $r$은 response 일 때, $D$로 부터 $P(r|C,K)$ 를 학습하는 것이 목표이다. 
이후 이를 통해, synthetic data $D^s$ 생성 후, $\{D^t U D^s \}$ 를 통해 generation model 이 나아지는지 확인한다.

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/c7fcbe86-08cb-4a00-8da7-4ca413b756cd)

# Methodology
위의 그림은 SynDG 의 전체적인 Framework 구조이다. 
세 가지로 이뤄져 있는데, (1) task-specific heuristic 을 통한 dialog flow 생성, (2) dialog flow 을 바탕으로 utterance realization (3) two-level filtering 을 통한 quality 확보이다.

<span style='color:green;font-weight:bold'> 1. Dialogue Flow Construction </span>
<br>
Dialog Flow $F=(f_1,f_2,...,f_{n_f} )$ 는 각각 knowledge piece $f$ 들로 이루어진다.
각각 knowedge piece $f$ 는 Knowledge base $K$ 의 하나의 piece 거나, 여러 piece 들의 연속이거나, "[none]" 이 되어 knowledge 가 없을 수 있다.
<span style='background-color: #dcffe4'> 각각 하나의 knowledge piece 가 utterance 가 되며, 홀수 번째는 첫 번째 speaker, 짝수 번째는 두 번째 speaker 의 utterance 가 된다. </span>
학습과정에서는, 각각의 utterance 마다 knowledge piece 가 있으므로 손쉽게 flow $f$를 얻을 수 있다.
**중요한 것은 Inference 단계에서, dialog flow 를 확보하는 방법이다.**
논문에서는, heuristic 을 이용한다.
PersonaChat 에 대해서는 persona utterance 들을 모아 Knowledge Base $\K$로 만든 후, 이 중 zero, one, or more persona sentence 각각을 $f$로 활용한다.
WoW (Wizard of Wikipedia) 에 대해서는, chosen passage 와 첫 번째 turn 에서 retrieve 되는 passage 를 knowledge corpus $\K$ 로 한 뒤, 각각의 turn 에서 최소 한 개의 $f$ 를 추출해서 사용한다.

<span style='background-color: #dcffe4'> Heuristic 을 활용한 방법이 universally applicable 하지 않다는 것을 저자들도 인지하지만, minor modification 을 통해 모든 데이터셋에 적용가능하다고 주장하고 있다. </span>

<span style='color:green;font-weight:bold'> 2.  Dialogue Content Realization </span>
<br>
Dialog flow 를 통해 utterance 를 생성하도록 T5 를 Finetuning 한다
$u_i$ 를 생성하기 위하여 $(u_1, u_2, ..., u_{t-1},[t],f_i,[/t],f_{i+1},...,f_{i+m})$ 을 input 으로 한다. $[t]$ 와 $[/t]$ 는 $u_i$ 가 $f_i$로 부터 생성됨을 강조한다.
Practically,_ [user]_ 와 _[agent]_ special token 을 추가한다.


<span style='color:green;font-weight:bold'> 3. Two-level Filtering </span>
<br>
T5 의 text-infilling task (masked sentence modeling) 을 통해 filter 를 학습한다. 마치 Dialog Reconstruction 와 마찬가지로, Training dataset 에서 utterance 와 flow 를 mask 하고 T5 기반 filter 가 맞추는 방식으로 학습을 한뒤, Inference 단계에서는 filter 가 내놓는 log prob 을 score 로 활용한다.
이 방식을 utterance 와 flow 에서 모두 적용한 뒤 합하여 최종 score 로 활용한다.

# Experiment Settings
[Dataset]
PersonaChat, WoW

[Baseline]
Wow : [BlenderBot](https://aclanthology.org/2021.eacl-main.24/)
KA (Knowledge Available) 은 GT-knowledge 에서 response 를 생성하고, KU (Konwledge Unavailable)은 knowledge selection 부터 진행한다. Knowledge selection 은 RoBERTa 를 finetuning 하여 활용한다.

PersonChat : GPT-2 based [basline](https://aclanthology.org/2022.acl-long.550/)
(1) GPT-2 : 일반적인 GPT-2
(2) GPT-2-BT : [Cao et al.](https://aclanthology.org/2022.acl-long.550/) 에서 제시된 back translation 을 활용한 dialog data augmenation 적용 방법
(3) GPT-2-$D^3$ : $D^3$ 는 [Cao et al.](https://aclanthology.org/2022.acl-long.550/) 에서 제시된 PersonChat 을 위한 data augmentation 방법이다.

[Eval Metrics]
BLUE-4, ROUGE-L, PPL(Perplexity), F1 (only for WoW), KF1 (knowledge uni-gram overlapping), ACC (Knowledge selection for KU setting), Human Evaluation - (1) Human Likeness, (2) Informativeness

[Implementation Details]
Dialog generator 와 filter 는 T5-large 를 활용한다. (T5-base 도 성능이 증가하지만 폭이 크지는 않다고 한다)

# Experiment Results
<span style='color:green;font-weight:bold'> Automatic eval results on WoW </span>
<br>
![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/7b40151b-70c4-4c18-91bb-a94a9906eb98)

SynDG 가 reponse generation 뿐 아니라, ground knolwedge ability 도 증가시켰음을 알 수 있고, two-level filtering 이 모두 improvement 에 contribute 한다. w/o FF&UF 가 w/o FF 혹은 w/o UF 보다 훨씬 degradation 이 심하고, 각각 역시 degradation 된다.
BB-SynDG w/o FF&UF 가 Random Sampling 인 RS 보다 좋아서, Heuristic 이 도움이 됨을 알 수 있다.

**Low-resource 에서는 효과가 더욱 극명한데**, 특히 KA setting 에서 BB-SynDG 는 1/16 training dataset 만으로 BB 의 full training dataset 과 비교되는 성능을 보여, low resource problem 해결에 도움이 됨을 확인할 수 있다. 

<span style='color:green;font-weight:bold'> Automatic eval results on PersonaChat </span>
<br>
![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/8184c88c-d8d4-49ff-9370-43199b518ade)

PersonaChat 에서 역시 좋은 결과를 보인다. GPT-2-$D^3$ 가 augmentation 도 굉장히 정교하게 많이 하였지만, 그래도 SynDG 의 성능이 더 좋았다.

<span style='color:green;font-weight:bold'> Human Evaluation </span>
<br>
![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/ccf2e4e2-8eba-44c0-898a-e3f68c9c0dbd)

<span style='color:green;font-weight:bold'> Impact of the Number of Synthetic Dialogues </span>
<br>
![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/def5f63f-1749-4b2d-931c-1a78db5fa04a)

**How many synthetic dialogues are appropriate to integrate as extra training samples?** 라는 질문에 대한 대답을 위한 실험 결과이다.
BLEU-4 (a)와 ROUGE-L (b)은 지속적으로 좋아졌지만, KF-1 score (c)의 경우 처음에 rapid increase 를 보인뒤 stable 하다.
이에 저자들은 LM 의 scale 에 따라 한계가 있으며, augmentation 의 효과가 무기한적이라고 생각하지는 않으며, original data 의 두 배 정도일 때가 최대 가성비 효과 인 것 같다고 한다.

# Conclusion
```
In this paper, we propose a framework, SynDG, to automatically construct synthetic training data for the grounded dialogue task. We first construct dialogue flows based on unstructured knowledge, then transform them into synthetic dialogues by large LMs, and finally filter and retain the generated dialogues with high quality. The experimental results demonstrate the effectiveness of our proposed framework in both full training data and low-resource scenarios. Further analysis shows that the model performance tends to increase as the number of synthetic dialogues increases. For future work, we plan to investigate more efficient strategies for determining dialogue flows and take larger LMs to produce synthetic dialogues with higher quality.
```

# Limitation
여전히 SynDG 로 만든 Synthetic data 와 human-written dialog 사이에 quality 적인 gap 이 크다고 한다. 저자들은 더 큰 LM 을 쓰거나, knowledge graph 혹은 reasoning skill 을 도입하면 개선될 여지가 있다고 말한다. 
