 ---
layout: post
title:  "[Arxiv 2305] Trusting Your Evidence: Hallucinate Less with Context-aware Decoding"
date:   2024-03-15 16:00:00 +0900
use_math: true
categories: [Retrieval, LLM, PLM]
---

[[pdf]](https://arxiv.org/pdf/2305.14739.pdf) &emsp;
[[github]](https://github.com/hongshi97/CAD)

**Weijia Shi<sup>1*</sup>, Xiaochuang Han<sup>1*</sup>, Mike Lewis<sup>2</sup>, Yulia Tsvetkov<sup>1</sup>, Luke Zettlemoyer<sup>1</sup>, Scott Yih<sup>2</sup>**
<br><sup>1</sup> University of Washington, Seattle, WA, <sup>2</sup> Meta AI  &emsp;

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/e91e8ba5-39d6-40a4-b0e9-fe9c8d7dd42f)

# Abstract
- (**Context-Aware Decoding**) LM 의 decoding 과정에서, context 를 사용할 때와 사용하지 않을때 (with and without) 의 차이점을 극대화시키는 contrastive output distribution 을 따르는 context-aware decoding (CAD) 방법론을 제안한다.
- (**Improving Faithfulness**) Context-aware decoding 방법으로 OPT, GPT, LLaMA, FLAN-T5 summarization task 에서 faithfulness 향상을 이뤄낸다.
- (<span style='color:green;font-weight:bold'> Resolving Knowledge Conflict </span>) 추가적으로, CAD 는 provided contxet 가 prior knowledge 와 충돌할 때, 그 conflict 를 해결하는데 효과적이라고 주장한다.

## 1. Introduction

<span style='color:green;font-weight:bold'> ▶ How LM deal prior knowledge and context knowledge </span>
<br>
Language model (LM) 이 coherent 하고 fluent 한 generation 을 잘 생성하는 것은 공공연한 사실이다.
<span style='background-color: #ffdce0'> 
그러나, 현재까지도 LM 이 param 속에 갖고 있는 prior knowledge 와 외부 지식으로 주어지는 context knowledge 두 가지 타입의 knowledge source 를 generation 과정에서 어떻게 처리하는지에 대한 연구가 더 필요하다. </span>

초창기 연구에서는 prior knowledge 에 집중하여, context knowledge 를 사용하지 않았을 때 생기는 hallucination 에 대해 집중한 연구들이 많다.
최근에는 이런 hallucination 극복으로 외부 지식을 context knowledge 로 주어 LM 이 generation 과정에 활용하게 하는 Retrieval augmented approach 가 많다.
하지만, 둘 사이에 conflict 가 있을 때 LM 이 어떻게 처리하는지는 문제가 된다.
한 가지 예시로, LLaMA 에게 "아르헨티나가 1978, 1986, 2022 년에 월드컵 우승을 했다" 라는 외부 지식을 전달한 상태에서, "아르헨티나가 월드컵을 몇 번 우승했어?" 라고 질문을 한다면 prior knowledge (2022년 전 지식) 에 따라 "Two" 라고 대답을 한다.

<span style='color:green;font-weight:bold'> ▶ CAD : Context-Aware Decoding </span>
<br>

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/3e9654c9-604d-4c07-864f-3347c48acf41)

이 연구에서는 simple context-aware decoding 방법론을 제안한다.
위의 figure 처럼 cAD 는 _with context_ <-> _without context_ 에 대한 output distribution difference 를 amplify 하는 새로운 output distribution 을 sample 한다.
<span style='background-color: #dcffe4'> 이것은 새로운 형태의 contrastive decoding 이다. </span>
기존의 연구([[1]](https://arxiv.org/abs/2210.15097)) 에서, more relevant contextual information 이 주어졌을 때, prior knowledge 를 down-weight 하는 contrastive decoding 방식이 존재한다. 
CAD 는 추가적인 additional training 없이 사용 가능한 off-the-shelf 방법이다.

<span style='color:green;font-weight:bold'> ▶ Experiments </span>
<br>
실험 결과, OPT, GPT-Neo, LLaMA 등의 vanilla LM 뿐 아니라, FLAN 등의 instruction-finetuend LM 에서 모두 faithfullness 향상을 확인한다.
CNN-DM 데이터셋에 대해 LLaMA-30B 에 CAD 를 적용했을 때, ROUGE-L 이 21%, summary factuality evaluation metrics 에서 14.3% 향상을 이룬다.
<span style='background-color: #dcffe4'> 특히, CAD 는 knowledge conflicting task 에서 beneficial 하다. </span> CAD 는 knowledge conflicts QA dataset([[2]](https://aclanthology.org/2021.emnlp-main.565/)) 에서 LLaMA-30B 에 기존방법보다 2.9 배 향상을 이뤄냈고, model size 가 커지면 커질 수록 이 효과의 증가를 확인한다.



## 2. Method
# 2.1. Background

# 2.2. Context-aware decoding

## 3. Experimental Setup
# 3.1. Datasets and Metrics
- **Summarization**

- **Knowledge Conflicts**

# 3.2. Models and Baselines

## 4. Results
# 4.1. Main Results

<span style='color:green;font-weight:bold'> (1) Summarization </span>
<br>


<span style='color:green;font-weight:bold'> (2) Knowledge Conflicts </span>
<br>

# 4.2. Analysis
<span style='color:green;font-weight:bold'> (1) Quantitative Analysis </span>
<br>

<span style='color:green;font-weight:bold'> (2) CAD brings consistent improvement to LMs with different sizes. </span>
<br>

<span style='color:green;font-weight:bold'> (3) Effect of adjustment level </span>
<br>

## Conclusion
```
Off-the-shelf language models may suffer from an insufficient attention to the supplied context compared to its learned prior knowledge, leading toan unfaithful generation to the input context. We present context-aware decoding, a simple inferencetime method that downweights an output probability associated with the model’s prior knowledge to promote models’ attention to the contextual information. We experiment on two families of tasks that require a strong attention to the context, summarization and knowledge conflicts tasks. We show that CAD provides more reliable and factual outputs across different language models of various
sizes.
```

<span style='color:green;font-weight:bold'> 초록색볼드체 </span>
<br>
<span style='background-color: #dcffe4'> 초록색배경 </span>
<span style='background-color: #ffdce0'> 빨간색배경 </span>
