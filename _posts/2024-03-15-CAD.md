---
layout: post
title:  "[Arxiv 2305] Trusting Your Evidence: Hallucinate Less with Context-aware Decoding"
date:   2024-03-15 16:00:00 +0900
use_math: true
categories: [Retrieval, LLM, PLM]
---

[[pdf]](https://arxiv.org/pdf/2305.14739.pdf) &emsp;
[[github]](https://github.com/hongshi97/CAD)

**Weijia Shi<sup>1*</sup>, Xiaochuang Han<sup>1*</sup>, Mike Lewis<sup>2</sup>, Yulia Tsvetkov<sup>1</sup>, Luke Zettlemoyer<sup>1</sup>, Scott Yih<sup>2</sup>**
<br><sup>1</sup> University of Washington, Seattle, WA, <sup>2</sup> Meta AI  &emsp;

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/e91e8ba5-39d6-40a4-b0e9-fe9c8d7dd42f)

# Abstract
- (**Context-Aware Decoding**) LM 의 decoding 과정에서, context 를 사용할 때와 사용하지 않을때 (with and without) 의 차이점을 극대화시키는 contrastive output distribution 을 따르는 context-aware decoding (CAD) 방법론을 제안한다.
- (**Improving Faithfulness**) Context-aware decoding 방법으로 OPT, GPT, LLaMA, FLAN-T5 summarization task 에서 faithfulness 향상을 이뤄낸다.
- (<span style='color:green;font-weight:bold'> Resolving Knowledge Conflict </span>) 추가적으로, CAD 는 provided contxet 가 prior knowledge 와 충돌할 때, 그 conflict 를 해결하는데 효과적이라고 주장한다.

## 1. Introduction

<span style='color:green;font-weight:bold'> ▶ How LM deal prior knowledge and context knowledge </span>
<br>
Language model (LM) 이 coherent 하고 fluent 한 generation 을 잘 생성하는 것은 공공연한 사실이다.
<span style='background-color: #ffdce0'> 
그러나, 현재까지도 LM 이 param 속에 갖고 있는 prior knowledge 와 외부 지식으로 주어지는 context knowledge 두 가지 타입의 knowledge source 를 generation 과정에서 어떻게 처리하는지에 대한 연구가 더 필요하다. </span>

초창기 연구에서는 prior knowledge 에 집중하여, context knowledge 를 사용하지 않았을 때 생기는 hallucination 에 대해 집중한 연구들이 많다.
최근에는 이런 hallucination 극복으로 외부 지식을 context knowledge 로 주어 LM 이 generation 과정에 활용하게 하는 Retrieval augmented approach 가 많다.
하지만, 둘 사이에 conflict 가 있을 때 LM 이 어떻게 처리하는지는 문제가 된다.
한 가지 예시로, LLaMA 에게 "아르헨티나가 1978, 1986, 2022 년에 월드컵 우승을 했다" 라는 외부 지식을 전달한 상태에서, "아르헨티나가 월드컵을 몇 번 우승했어?" 라고 질문을 한다면 prior knowledge (2022년 전 지식) 에 따라 "Two" 라고 대답을 한다.

<span style='color:green;font-weight:bold'> ▶ CAD : Context-Aware Decoding </span>
<br>

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/3e9654c9-604d-4c07-864f-3347c48acf41)

이 연구에서는 simple context-aware decoding 방법론을 제안한다.
위의 figure 처럼 cAD 는 _with context_ <-> _without context_ 에 대한 output distribution difference 를 amplify 하는 새로운 output distribution 을 sample 한다.
<span style='background-color: #dcffe4'> 이것은 새로운 형태의 contrastive decoding 이다. </span>
기존의 연구([[1]](https://arxiv.org/abs/2210.15097)) 에서, more relevant contextual information 이 주어졌을 때, prior knowledge 를 down-weight 하는 contrastive decoding 방식이 존재한다. 
CAD 는 추가적인 additional training 없이 사용 가능한 off-the-shelf 방법이다.

<span style='color:green;font-weight:bold'> ▶ Experiments </span>
<br>
실험 결과, OPT, GPT-Neo, LLaMA 등의 vanilla LM 뿐 아니라, FLAN 등의 instruction-finetuend LM 에서 모두 faithfullness 향상을 확인한다.
CNN-DM 데이터셋에 대해 LLaMA-30B 에 CAD 를 적용했을 때, ROUGE-L 이 21%, summary factuality evaluation metrics 에서 14.3% 향상을 이룬다.
<span style='background-color: #dcffe4'> 특히, CAD 는 knowledge conflicting task 에서 beneficial 하다. </span> CAD 는 knowledge conflicts QA dataset([[2]](https://aclanthology.org/2021.emnlp-main.565/)) 에서 LLaMA-30B 에 기존방법보다 2.9 배 향상을 이뤄냈고, model size 가 커지면 커질 수록 이 효과의 증가를 확인한다.

## 2. Method
# 2.1. Background

Response 식은 아래와 같다.

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/6860f968-44e6-45b0-91e9-d9f540cdc93f)

여기서 context $c$ 는 prior knowledge 에 *conflict* 하거나 *unfamililar* 한 내용을 포함할 수 있다.
예를 들어, 첫 번째 Figure 처럼 "아르헨티나가 1978, 1986, 2022 년에 우승했다" 라는 외부 지식이 context $c$ 로써 prior knowledge 에 충돌하게 주어질 수 있다.

# 2.2. Context-aware decoding

이를 위해, 저자들은, context 없이 prior knowledge 만으로 output knowledge 를 추출한다.
따라서 prior knowledge 는 아래의 식이 되고,

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/0e07f50d-27c4-49e8-b6e4-5866c8d78b66)

이것과 context $c$ 를 포함한 $y_t$ 사이의 <span style='background-color: #dcffe4'> Point-Wise Mutual Information (PMI) </span> 을 활용하여 adjust 시킨다.

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/cc29321d-45bf-4875-891d-d2eb2460b22f)

이 식 자체만으로는 valid output distribution 이 아니므로, normalize 가 필요하다. 따라서 softmax 를 활용하여 최종적인 context-aware decoding 이 된다.

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/aa0c5efd-3fb1-42d1-ba3d-51cb4312f9cd)


## 3. Experimental Setup
# 3.1. Datasets and Metrics
- **Summarization** : Dataset | CNN-DM, XSUM   Metric | ROUGE-L, BERT-Precision, FactKB

- **Knowledge Conflicts** : Dataset | MemoTrap, NQ-Swap 

아래의 Tasble 에 context 를 활용한 두 task 의 example 을 볼 수 있다.

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/3c2331d1-e070-4db1-874f-e015870352e1)


# 3.2. Models and Baselines
모델은 OPT-13B, OPT-30B, GPT-Neo2.7B, GPT-Neo20B, LLaMA-13B, LLaMa-30B 를 활용하고, instruction-finetuned model로 FLAN-T5-3B, FLAN-T5-11B 를 활용한다.
$alpha$ 값은 Summarization 에는 0.5 로, Knowledge conflict task 에는 1.0 으로 설정하였다. 비교 baseline 은 knowledge conflict task 는 greedy decoding 와 비교하고, summarization task에는 top-p (p=0.9) sampling 과 비교한다.

## 4. Results
# 4.1. Main Results

<span style='color:green;font-weight:bold'> (1) Summarization </span>
<br>

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/29d7173e-cb79-49ff-b3fe-2ce1be5f33ce)

- <span style='background-color: #dcffe4'> We observe that CAD outperforms the standard decoding algorithm by a large margin in all eight models across both
datasets </span>
- <span style='background-color: #dcffe4'> Specifically, when applied to LLAMA30B in CNN-DM, CAD leads to 21% increase in ROUGE-L, 14.3% increase in factKB and 7.8% increase in BERT-P. </span>
- **이 결과는 CAD 가 quality 뿐 아니라 Factuality 측면에서도 효과적임을 보인다.**

<span style='color:green;font-weight:bold'> (2) Knowledge Conflicts </span>
<br>

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/9d10ecfb-d302-4879-8fff-53bf7b939bc8)

- <span style='background-color: #dcffe4'> CAD is significantly better than the regular decoding in all settings, with the exception of a minor decrease observed for FLAN-T5 on the non-conflict NQ dataset </span>
- <span style='background-color: #dcffe4'> Despite this, CAD achieves substantially better performance on the knowledge conflict datasets, e.g., CAD improve GPT-Neo 20B by 54.4% on Memotrap and by 128% on NQ-SWAP </span>
- CAD 가 LM 으로 하여금 prior knowledge 가 conflict 할 때의 scenario 에서 효과적임을 보인다.

# 4.2. Analysis
<span style='color:green;font-weight:bold'> (1) Quantitative Analysis </span>
<br>

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/281170e1-8dc3-4b45-8c8a-c052b5210ac2)

- XSUM 의 경우 Regular decoding 은 article 에 없는 말들을 생성하지만, CAD 는 오로지 article 에 의존하여 잘 생성한다.
- MemoTrap 에서 standard decoding 은 instruction 을 무시하지만, CAD 는 instruction 을 잘 따른다.

<span style='color:green;font-weight:bold'> (2) CAD brings consistent improvement to LMs with different sizes. </span>
<br>

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/ff83c01e-791e-4a3e-b7f7-b6ec2b8a628b)

- 다양한 모델 사이즈에서 모두 CAD 방법이 효과적이었다.
- Memotrap 과 NQSWAP 의 경우, 모델 사이즈가 커지면 커질 수록, CAD 를 써서 얻는 효과가 증가한다.

<span style='color:green;font-weight:bold'> (3) Effect of adjustment level </span>
<br>

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/9dd321da-7b56-4639-84d4-5f09e9dea9dd)

- $lambda$=0.5 일 때 가장 효과가 좋다.

## Conclusion
```
Off-the-shelf language models may suffer from an insufficient attention to the supplied context compared to its learned prior knowledge, leading toan unfaithful generation to the input context. We present context-aware decoding, a simple inferencetime method that downweights an output probability associated with the model’s prior knowledge to promote models’ attention to the contextual information. We experiment on two families of tasks that require a strong attention to the context, summarization and knowledge conflicts tasks. We show that CAD provides more reliable and factual outputs across different language models of various
sizes.
```
