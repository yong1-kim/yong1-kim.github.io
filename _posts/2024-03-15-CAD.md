 ---
layout: post
title:  "[Arxiv 2305] Trusting Your Evidence: Hallucinate Less with Context-aware Decoding"
date:   2024-03-15 16:00:00 +0900
use_math: true
categories: [Retrieval, LLM, PLM]
---

[[pdf]](https://arxiv.org/pdf/2305.14739.pdf) &emsp;
[[github]](https://github.com/hongshi97/CAD)

**Weijia Shi<sup>1*</sup>, Xiaochuang Han<sup>1*</sup>, Mike Lewis<sup>2</sup>, Yulia Tsvetkov<sup>1</sup>, Luke Zettlemoyer<sup>1</sup>, Scott Yih<sup>2</sup>**
<br><sup>1</sup> University of Washington, Seattle, WA, <sup>2</sup> Meta AI  &emsp;

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/e91e8ba5-39d6-40a4-b0e9-fe9c8d7dd42f)

# Abstract
- (**Context-Aware Decoding**) LM 의 decoding 과정에서, context 를 사용할 때와 사용하지 않을때 (with and without) 의 차이점을 극대화시키는 contrastive output distribution 을 따르는 context-aware decoding (CAD) 방법론을 제안한다.
- (**Improving Faithfulness**) Context-aware decoding 방법으로 OPT, GPT, LLaMA, FLAN-T5 summarization task 에서 faithfulness 향상을 이뤄낸다.
- (<span style='color:green;font-weight:bold'> Resolving Knowledge Conflict </span>) 추가적으로, CAD 는 provided contxet 가 prior knowledge 와 충돌할 때, 그 conflict 를 해결하는데 효과적이라고 주장한다.

## 1. Introduction

## 2. Method
# 2.1. Background

# 2.2. Context-aware decoding

## 3. Experimental Setup
# 3.1. Datasets and Metrics
- **Summarization**

- **Knowledge Conflicts**

# 3.2. Models and Baselines

## 4. Results
# 4.1. Main Results

<span style='color:green;font-weight:bold'> (1) Summarization </span>
<br>


<span style='color:green;font-weight:bold'> (2) Knowledge Conflicts </span>
<br>

# 4.2. Analysis
<span style='color:green;font-weight:bold'> (1) Quantitative Analysis </span>
<br>

<span style='color:green;font-weight:bold'> (2) CAD brings consistent improvement to LMs with different sizes. </span>
<br>

<span style='color:green;font-weight:bold'> (3) Effect of adjustment level </span>
<br>

<span style='color:green;font-weight:bold'> 초록색볼드체 </span>
<br>

## Conclusion
```
Off-the-shelf language models may suffer from an insufficient attention to the supplied context compared to its learned prior knowledge, leading toan unfaithful generation to the input context. We present context-aware decoding, a simple inferencetime method that downweights an output probability associated with the model’s prior knowledge to promote models’ attention to the contextual information. We experiment on two families of tasks that require a strong attention to the context, summarization and knowledge conflicts tasks. We show that CAD provides more reliable and factual outputs across different language models of various
sizes.
```
<span style='background-color: #dcffe4'> 초록색배경 </span>
<span style='background-color: #ffdce0'> 빨간색배경 </span>
