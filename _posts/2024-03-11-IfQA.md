---
layout: post
title:  "[EMNLP2023] IfQA: A Dataset for Open-domain Question Answeringunder Counterfactual Presuppositions"
date:   2024-03-11 18:00:00 +0900
use_math: true
categories: [Retrieval, LLM, PLM]
---

[[pdf]](https://aclanthology.org/2023.emnlp-main.515.pdf) &emsp;
[[github]](https://allenai.org/data/ifqa)

**Wenhao Yu<sup>♦</sup>, Meng Jiang<sup>♣</sup>, Peter Clark<sup>♠</sup>, Ashish Sabharwal<sup>♠</sup>**
<br><sup>♦</sup> Tecent AI Seattle Lab <sup>♣</sup> University of Notre Dame <sup>♠</sup> Allen Institute for AI &emsp;

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/dc04e304-b2b0-44e5-8293-aee477bbaa0e)

# Abstract
- (**lack of counterfactual QA dataset**) counterfactual reasoning 이 매우 중요하지만, large-scale counterfactual open-domain question answering (QA) dataset 이 부족하여, model 을 평가하기 힘들다.
- (<span style='color:green;font-weight:bold'> IfQA </span>) 모든 question 이 'if' 를 통한 **counterfactual presupposition** 에 기반한 IfQA 벤치마크를 introduce 한다. 이 Question 들은 parameter 속의 진실과 반대되는 imagined situation 에 대해서도 right information 을 identify 할 수 있어야한다.
- (**Experiment**) supervised retrieve-then-read pipeline 모델들에 대하여, 낮은 점수를 보이며, ChatGPT 를 활용한 Chain-of-Thought 을 활용해도 여전히 challenging 한 open-domain QA benchmark 이다.

## 1. Introduction

## 2. IfQA : Task and Dataset
# 2.1. Dataset Collection

# 2.2. Dataset Analysis

# 2.3. Dataset Splits

## 3. Experiments
# 3.1. Retrieval Corpus

# 3.2. Comparison Systems

# 3.3. Evaluation Metrics

# 3.4. Implementation Details

# 3.5. Results and Discussion

## Conclusion

## Limitations





<span style='color:green;font-weight:bold'> 초록색볼드체 </span>

<span style='background-color: #dcffe4'> 초록색배경 </span>
<span style='background-color: #ffdce0'> 빨간색배경 </span>
