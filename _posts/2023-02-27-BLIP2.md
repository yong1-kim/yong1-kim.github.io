---
layout: post
title:  "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen image Encoders and Large Language Models"
date:   2023-02-27 15:56:00 +0900
use_math: true
categories: [Vision-and-Language, Transformer]
---
[[pdf]](https://arxiv.org/pdf/2301.12597.pdf) &emsp;
[[github]](https://github.com/salesforce/LAVIS/tree/main/projects/blip2) &emsp;
[[huggingface]](https://huggingface.co/spaces/Salesforce/BLIP2)

**Junnan Li<sup>‡</sup>, Dongxu Li<sup>‡</sup>, Silvio Savarese<sup>‡</sup>, Steven Hoi<sup>‡</sup>**
<br><sup>‡</sup> Salesforce Research  &emsp;

![image](https://user-images.githubusercontent.com/42200027/221531078-bd8624b7-a4dc-4eaa-9856-4e6b3fcb13cb.png)

# Abstract
- (Motivation) Vision-and-Langauge Transformer 의 스케일이 커지면서 pre-training 이 너무 힘들어졌다.
- (Method) 이 논문에서는 off-the-shelf frozen pre-trained image encoder 와 frozne LLM 을 활용하여 **BLIP-2** 라는 efficient 한 pre-training strategy 를 소개한다.
- (Method) BLIP-2 는 lightweight **Querying Transformer** 를 활용하여 modality gap 을 bridge 한다.
- (Method) 두 가지 step 으로 이뤄져 있는데, 첫 번째 step 은 frozen image encoder 로 부터 vision-language representation learning 을 bootstrap 하고, 두 번째 step은 frozen LLM 을 통해 vision-to-language generative learning 을 bootstrap 한다.
- (Experiment) BLIP-2 는 여러 vision-and-language task 에서 State-of-the-Art 를 기록하였고, 특히 zero-shot VQAv2 에서는 flamingo80B 를 54배 적은 parameter로 8.7%의 성능을 추월하였다.

# Introduction
Vision-and-Language Pretraining(VLP) 가 최근 눈부신 성장을 보여주고 있지만, pre-training 과정에 large-scale model 과 dataset 을 필요로 한다.
Vision-and-Language model 은 각각 발전된 unimodal model 로부터 성능을 뽑아내는 것이 자연스럽다.
이 논문에서는 off-the-shelf pre-trained vision model 과 language model 을 bootstrapping 하는 *generic* 하고 *compute-efficient* VLP 방법을 소개한다.
Pre-trained vision model 은 high-quality visual representation 을 제공한다.
Pre-trained language model (LLM)은 strong language generation 과 zero-shot transfer ability 를 제공한다.

Pre-trained unimodal model 들을 VLP 에 활용하기 위해서는 cross-modal alignment 가 필수적이다.
그러나 LLM 의 경우, pre-training 과정에서 image 를 전혀 관측하지 않기 때문에, LLM 을 freezing 하는 것은 vision-language alingment 를 더욱 어렵게 만든다.
이러한 관점에서 기존의 [Frozen](https://arxiv.org/abs/2106.13884) 이나 [Flamingo](https://arxiv.org/pdf/2205.14204.pdf) 와 같은 iamge-to-text generation loss 는 modality gap 을 줄이는데 사용하지만, 본 논문에서 loss 만으로는 insufficient 하다는 것을 검증한다.

이 논문에서는 이 문제점을 해결하기 위하여 <span style='background-color: #dcffe4'> Effective Vision-language alignemnt 를 위해서, Qerying Transformer (Q-former) </span>를 제안한다. 
이 Q-Former 는 위의 그림에서와 같이, frozen image encoder 로부터 visual feature 를 추출하기 위하여 learnable query vector 를 추출한다. 
이 것은 Frozen Image encoder 와 frozen LLM 사이의 information bottleneck 역할을 하는데,<span style='color:green;font-weight:bold'> desired text 를 생성하기 위해, most useful visual feature 를 뽑아내는 역할을 한다.
 </span>
 첫 번째 pre-training stage 에서 Q-former 가 text 와 가장 관련된 visual representation 이 무엇인지 배우도록 학습한다.
 이후 두 번째 pre-training stage 에서, Q-former 와 LLM 을 결합하여 vision-to-language generative learning 을 수행하여, Q-former 의 output 이 LLM 에 의해 해석될 수 있도록 학습한다.
 
# Method
<span style='color:green;font-weight:bold'> Model Architecture </span>
<br>
![image](https://user-images.githubusercontent.com/42200027/221538312-0f265de9-1f34-4840-95c7-5c61357e811d.png)

Frozen Image Encoder 와 Frozen LLM 사이의 gap 을 bridge 하기 위하여, trainable module 인 Q-Former 를 도입한다.
Q-Former 는 input image resolution 과 상관없이, fixed number 의 output feature 를 뽑아낸다.
위의 그림처럼, Q-former 는 self-attention layer 를 share 하는 두 개의 transformer layer 로 구성된다: (1) frozen image encoder 로 부터 visual feature extraction 을 위해 사용되는 transformer, (2) text encoder 와 text decoder 의 역할을 하는 text transformer.
Q-former 의 self-attention layer 는 pre-trained BERT 를 활용하였고, layer 마다 inject 되는 cross-attention layer 는 randomly initialized 되었다.
Q-former 는 188M의 parameter 로 이루어져있고, Query vector 역시 model param 이다.
실험에서는 768 차원을 갖는 32 개의 query vector (*Z*)을 사용하였다.
Vit-L/14 에서 사용하는 frozen image feature 257x1024 에 비하면, 32x768 의 크기를 갖는 *Z* 는 크기가 매우 적은 편이다.
이 query vector 는 text 와 가장 relevant 한 visual information 을 extract 하는데 사용된다.

이후, [BLIP](https://arxiv.org/abs/2201.12086) 에서 영감을 받아, 세 개의 pre-training objective 를 jointly optimize 한다.
세 개의 pre-training objective 는 위의 그림과 같이 각기 다른 attention masking strategy 를 통해 이루어진다.

<span style='color:green;font-weight:bold'> Image-Text Contrastive Learning (ITC) </span>
<br>
ITC 는 image representation 과 text representation 사이의 mutual information 을 극대화한다.
Postivie pair 를 negative pair 들과 contrasting 하여 구현한다.
[CLS] token 의 output embedding 인 text representation *t* 와 *Z* 을 align 한다.
32 개의 query 중 가장 높은 iamge-text similarity 를 갖는 것을 고른다.
infromation leak 을 피하기 위해, unimodal self-attention 을 차용하는데, 위의 그림에서와 같이 attention masking 을 활용하여 서로가 서로를 allow 할 수 없다.


<span style='color:green;font-weight:bold'> Image-grounded Text Generation (ITG) </span>
<br>
ITG 는 Q-former 가 given input image 로 부터 text 를 generation 하도록 학습한다.
UniLM 과 비슷하게 causal self-attention mask 를 통하여 구현한다.

<span style='color:green;font-weight:bold'> Image-Text Matching (ITM) </span>
<br>
ITM 은 image and text preresentation 사이의 fine-grained alignment 을 위해 사용된다.
image-text pair 를 잘 찾아내는 지의 binary classification 으로 구성된다.
bi-directional self-attnetion mask 를 통해 모든 query 와 text 들이 서로를 attend 할 수 있다.


![image](https://user-images.githubusercontent.com/42200027/221542885-d2f0bcd5-df4e-4ac2-b005-7b58b931a75b.png)

<span style='color:green;font-weight:bold'> Bootstrap Vision-to-Language Generative Learning from a Frozen LLM </span>
<br>

Frozen LLM 과 Q-former 를 통해 generative pre-training stage 를 거친다.
이는 LLM 와 generative language capability 를 harvest 하기 위함이다.
위의 그림처럼, FC layer 를 통해 *Z*의 Output embedding 을 LLM 의 text embedding 으로 linearly proejction 한다.
이후 projected query embedding 이 input text embedding 에 prepend 되어 사용된다.
이 것은 마치 <span style='background-color: #dcffe4'> soft visual prompts </span> 로써의 역할을 한다.
그림과 같이 decoder-only model 과 encoder-decoder model 을 각각 LM loss 와 prefix LM loss 를 통해 학습한다.

<span style='color:green;font-weight:bold'> Pre-training data </span>
<br>
BLIP 과 같이 COCO, Visual Genome, CC3M, CC12M, SBU, 그리고 LAION400M 의 115M image 를 포함한 129M image 를 사용한다.
[CapFilt](https://arxiv.org/abs/2201.12086) 방법을 통해 web image 로 부터 synthetic cpation 을 생성한다.
정확히는, $BLIP_{large}$ captioning model 을 통해, 10 개의 caption 을 생성한 후, CLIP ViT-L/14 를 이용하여 original caption 과의 similarity 를 측정하여 reranking 한 후 사용한다. 

<span style='color:green;font-weight:bold'> Pre-trained image encoder and LLM. </span>
<br>
Pre-trained image model : (1) VIT-L/14 CLIP, (2) ViT-G/14 EVA_CLIP
<br>
Pre-trained LLM : (1) OPT for decoder-only, (2) FLanT5 for encoder-decoder based LLM

# Experiment

![image](https://user-images.githubusercontent.com/42200027/221545672-d1e3e584-0720-43db-b12b-c022dbf5bdd5.png)

위의 표와 같이 BLIP-2 는 적은 파라미터로도 zero-shot setting 에서 압도적으로 좋은 성능을 보여준다.

<span style='color:green;font-weight:bold'> Instructed Zero-shot Image-to-Text Generation </span>
<br>
![image](https://user-images.githubusercontent.com/42200027/221545064-88a8ce3d-3e32-45bf-a58f-96f680e4b8d6.png)

BLIP-2 는 LLM 으로 하여금 image 를 잘 이해할 수 있게 만든다.
위의 그림에서 예시를 볼 수 있다.
Zero-shot VQA 에서, OPT 를 활용할 경우 prompt 로 "Question: {} Answer",를 FlanT5 를 활용할 경우, "Question: {} Short Answer:" 를 활용한다.
![image](https://user-images.githubusercontent.com/42200027/221546686-70356c63-191c-4b8b-bda8-eb50dd915841.png)

위의 표에서 와같이 BLIP-2 는 VQAv2 와 GQA 에서 압도적인 성능을 보여준다.
특히 VQAv2 에서 Flamingo80B 를 8.7% 나 앞섰으며, 54 배 적은 param 수로 얻은 결과이다.

![image](https://user-images.githubusercontent.com/42200027/221547193-d1ab3be4-b3c3-47b3-b733-db534cfc41a4.png)

위의 그림은 Pre-training stage1 의 영향력을 보여준다. 두 방식의 LLM 에서 모두 pre-training stage1 으로 query 에 visual information 을 학습시킬 때 좋은 결과를 얻었다.

<span style='color:green;font-weight:bold'> Image Captioning </span>
<br>

![image](https://user-images.githubusercontent.com/42200027/221547369-7cbdab67-802e-47ca-afe3-66463ab2a110.png)

Image Captioning prompt 로는 "a photo of"를 사용하였고, COCO 로 finetuning 학습을 한 후, NoCAPs 로 zero-shot 실험을 한 결과와 COCO test set 으로 한 결과는 위의 표와 같다.
BLIP-2 는 out-of-domain image captioning 에서 매우 좋은 성능을 보여준다.

<span style='color:green;font-weight:bold'> Visual Question Answering </span>
<br>

![image](https://user-images.githubusercontent.com/42200027/221547772-806e2377-812e-4f98-8c1e-fea34cf07b74.png)

Annotate VQA data 가 주어졌을 때, LLM 은 frozen 하고 Q-former 만을 finetune 하여 VQA 를 학습한다.
위의 표에서와 같이 open-ended generation dmoel 에서 state-of-the-art 를 달성한다.
