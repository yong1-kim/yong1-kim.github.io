---
layout: post
title:  "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen image Encoders and Large Language Models"
date:   2022-02-27 15:56:00 +0900
use_math: true
categories: [Vision-and-Language, Transforemr]
---
[[pdf]](https://arxiv.org/pdf/2301.12597.pdf) &emsp;
[[github]](https://github.com/salesforce/LAVIS/tree/main/projects/blip2) &emsp;
[[huggingface]](https://huggingface.co/spaces/Salesforce/BLIP2)

**Junnan Li<sup>‡</sup>, Dongxu Li<sup>‡</sup>, Silvio Savarese<sup>‡</sup>, Steven Hoi<sup>‡</sup> **
<br><sup>‡</sup> Salesforce Research  &emsp;

![image](https://user-images.githubusercontent.com/42200027/221531078-bd8624b7-a4dc-4eaa-9856-4e6b3fcb13cb.png)

# Abstract
- (Motivation) Vision-and-Langauge Transformer 의 스케일이 커지면서 pre-training 이 너무 힘들어졌다.
- (Method) 이 논문에서는 off-the-shelf frozen pre-trained image encoder 와 frozne LLM 을 활용하여 **BLIP-2** 라는 efficient 한 pre-training strategy 를 소개한다.
- (Method) BLIP-2 는 lightweight **Querying Transformer** 를 활용하여 modality gap 을 bridge 한다.
- (Method) 두 가지 stepo 으로 이뤄져 있는데, 하나는 frozen image encoder 로 부터 vision-language representation learning 을 bootstrap 하고, 두 번쨰는 frozen LLM 을 통해 vision-to-language generative learning 을 bootstrap 한다.
- (Experiment) BLIP-2 는 여러 vision-and-language task 에서 SOTA 를 기록하였고, zero-shot VQAv2 에서는 flamingo80B 를 54배 적은 parameter로 8.7%의 성능을 추월하였다.

# Introduction
Vision-and-Language Pretraining(VLP) 가 최근 눈부신 성장을 보여주고 있지만, pre-training 과정에 large-scale model 과 dataset 을 필요로 한다.
Vision-and-Language model 은 각각 발전된 unimodal model 로부터 성능을 뽑아내는 것이 자연스럽다.
이 논문에서는 off-the-shelf pre-trained vision model 과 language model 을 bootstrapping 하는 *generic* 하고 *compute-efficient* VLP 방법을 소개한다.
Pre-trained vision model 은 high-quality visual representatino 을 제공한다.
Pre-trained language model (LLM)은 strong language generation 과 zero-shot transfer ability 를 제공한다.

pre-trained unimodal model 들을 VLP 에 활용하기 위해서는 cross-modal alignment 가 필수적이다.
그러나 LLM 의 경우, pre-training 과정에서 image 를 전혀 관측하지 않기 때문에, freezing 하는 것은 vision-language alingment 를 더욱 어렵게 만든다.
이러한 관점에서 기존의 [Frozen](https://arxiv.org/abs/2106.13884) 이나 [Flamingo](https://arxiv.org/pdf/2205.14204.pdf) 와 같은 iamge-to-text generation loss 는 modality gap 을 줄이는데 사용하지만, 본 논문에서 이 것은 insufficient 하다는 것을 검증한다.

<span style='background-color: #dcffe4'> Effective Vision-language alignemnt 를 위해서, Qerying Transformer (Q-former) 를 제안한다. </span>
이 Q-Former 는 위의 그림에서와 같이, frozen image encoder 로부터 visual feature 를 추출하기 위하여 learnable query vector 를 추출한다. 
이 것은 Frozen Image encoder 와 frozen LLM 사이의 information bottleneck 역할을 하는데,<span style='color:green;font-weight:bold'> desired text 를 생성하기 위해, most useful visual feature 를 뽑아내는 역할을 한다.
 </span>
 첫 번째 pre-training stage 에서 Q-former 가 text 와 가장 관련된 visual representation 이 무엇인지 배우도록 학습한다.
 이후 두 번째 pre-training stage 에서, Q-former 와 LLM 을 결합하여 vision-to-language generative learning 을 수행하여, Q-former 의 output 이 LLM 에 의해 해석될 수 있도록 학습한다.
 
 # Method
 <span style='color:green;font-weight:bold'> Model Architecture </span>
 <br>
 ![image](https://user-images.githubusercontent.com/42200027/221538312-0f265de9-1f34-4840-95c7-5c61357e811d.png)

 Frozen Image Encoder 와 Frozen LLM 사이의 gap 을 bridge 하기 위하여, trainable module 인 Q-Former 를 도입한다.
 Q-Former 는 input image resolution 과 상관없이, fixed number 의 output feature 를 뽑아낸다.
 위의 그림처럼, Q-former 는 self-attention layer 를 share 하는 두 개의 transformer layer 로 구성된다: (1) frozen image encoder 로 부터 visual feature extraction 을 위해 사용되는 transformer, (2) text encoder 와 text decoder 의 역할을 하는 text transformer.
 Q-former 의 self-attention layer 는 pre-trained BERT 를 활용하였고, layer 마다 inject 되는 cross-attention layer 는 randomly initialized 되었다.
 Q-former 는 188M의 parameter 로 이루어져있고, Query vector 역시 model param 이다.
 
 
