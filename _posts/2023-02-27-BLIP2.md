---
layout: post
title:  "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen image Encoders and Large Language Models"
date:   2023-02-27 15:56:00 +0900
use_math: true
categories: [Vision-and-Language, Transforemr]
---
[[pdf]](https://arxiv.org/pdf/2301.12597.pdf) &emsp;
[[github]](https://github.com/salesforce/LAVIS/tree/main/projects/blip2) &emsp;
[[huggingface]](https://huggingface.co/spaces/Salesforce/BLIP2)

**Junnan Li<sup>‡</sup>, Dongxu Li<sup>‡</sup>, Silvio Savarese<sup>‡</sup>, Steven Hoi<sup>‡</sup> **
<br><sup>‡</sup> Salesforce Research  &emsp;

![image](https://user-images.githubusercontent.com/42200027/221531078-bd8624b7-a4dc-4eaa-9856-4e6b3fcb13cb.png)

# Abstract
- (Motivation) Vision-and-Langauge Transformer 의 스케일이 커지면서 pre-training 이 너무 힘들어졌다.
- (Method) 이 논문에서는 off-the-shelf frozen pre-trained image encoder 와 frozne LLM 을 활용하여 **BLIP-2** 라는 efficient 한 pre-training strategy 를 소개한다.
- (Method) BLIP-2 는 lightweight **Querying Transformer** 를 활용하여 modality gap 을 bridge 한다.
- (Method) 두 가지 stepo 으로 이뤄져 있는데, 하나는 frozen image encoder 로 부터 vision-language representation learning 을 bootstrap 하고, 두 번쨰는 frozen LLM 을 통해 vision-to-language generative learning 을 bootstrap 한다.
- (Experiment) BLIP-2 는 여러 vision-and-language task 에서 SOTA 를 기록하였고, zero-shot VQAv2 에서는 flamingo80B 를 54배 적은 parameter로 8.7%의 성능을 추월하였다.

# Introduction
Vision-and-Language Pretraining(VLP) 가 최근 눈부신 성장을 보여주고 있지만, pre-training 과정에 large-scale model 과 dataset 을 필요로 한다.
Vision-and-Language model 은 각각 발전된 unimodal model 로부터 성능을 뽑아내는 것이 자연스럽다.
이 논문에서는 off-the-shelf pre-trained vision model 과 language model 을 bootstrapping 하는 *generic* 하고 *compute-efficient* VLP 방법을 소개한다.
Pre-trained vision model 은 high-quality visual representatino 을 제공한다.
Pre-trained language model (LLM)은 strong language generation 과 zero-shot transfer ability 를 제공한다.

pre-trained unimodal model 들을 VLP 에 활용하기 위해서는 cross-modal alignment 가 필수적이다.
그러나 LLM 의 경우, pre-training 과정에서 image 를 전혀 관측하지 않기 때문에, freezing 하는 것은 vision-language alingment 를 더욱 어렵게 만든다.
이러한 관점에서 기존의 [Frozen](https://arxiv.org/abs/2106.13884) 이나 [Flamingo](https://arxiv.org/pdf/2205.14204.pdf) 와 같은 iamge-to-text generation loss 는 modality gap 을 줄이는데 사용하지만, 본 논문에서 이 것은 insufficient 하다는 것을 검증한다.

<span style='background-color: #dcffe4'> Effective Vision-language alignemnt 를 위해서, Qerying Transformer (Q-former) 를 제안한다. </span>
이 Q-Former 는 위의 그림에서와 같이, frozen image encoder 로부터 visual feature 를 추출하기 위하여 learnable query vector 를 추출한다. 
이 것은 Frozen Image encoder 와 frozen LLM 사이의 information bottleneck 역할을 하는데,<span style='color:green;font-weight:bold'> desired text 를 생성하기 위해, most useful visual feature 를 뽑아내는 역할을 한다.
 </span>
 첫 번째 pre-training stage 에서 Q-former 가 text 와 가장 관련된 visual representation 이 무엇인지 배우도록 학습한다.
 이후 두 번째 pre-training stage 에서, Q-former 와 LLM 을 결합하여 vision-to-language generative learning 을 수행하여, Q-former 의 output 이 LLM 에 의해 해석될 수 있도록 학습한다.
 
# Method
<span style='color:green;font-weight:bold'> Model Architecture </span>
<br>
![image](https://user-images.githubusercontent.com/42200027/221538312-0f265de9-1f34-4840-95c7-5c61357e811d.png)

Frozen Image Encoder 와 Frozen LLM 사이의 gap 을 bridge 하기 위하여, trainable module 인 Q-Former 를 도입한다.
Q-Former 는 input image resolution 과 상관없이, fixed number 의 output feature 를 뽑아낸다.
위의 그림처럼, Q-former 는 self-attention layer 를 share 하는 두 개의 transformer layer 로 구성된다: (1) frozen image encoder 로 부터 visual feature extraction 을 위해 사용되는 transformer, (2) text encoder 와 text decoder 의 역할을 하는 text transformer.
Q-former 의 self-attention layer 는 pre-trained BERT 를 활용하였고, layer 마다 inject 되는 cross-attention layer 는 randomly initialized 되었다.
Q-former 는 188M의 parameter 로 이루어져있고, Query vector 역시 model param 이다.
실험에서는 768 차원을 갖는 32 개의 query vector 을 사용하였다. (notation *Z*)
Vit-L/14 에서 사용하는 frozen image feature 257x1024 에 비하면, 32x768 의 크기를 갖는 *Z* 는 매우 적은 편이다.
이 query vector 는 text 와 가장 relevant 한 visual information 을 extract 하는데 사용된다.

이후, [BLIP](https://arxiv.org/abs/2201.12086) 에서 영감을 받아, 세 개의 pre-training objective 를 jointly optimze 한다.
세 개의 pre-training objective 는 위의 그림과 같이 각기 다른 attention masking strategy 를 통해 이루어진다.

<span style='color:green;font-weight:bold'> Image-Text Contrastive Learning (ITC) </span>
<br>
ITC 는 image representation 과 text representation 사이의 mutual information 을 극대화한다.
Postivie pair 를 negative pair 들과 contrasting 하여 구현한다.
[CLS] token 의 output embedding 인 text representation *t* 와 *Z* 을 align 한다.
32 개의 query 중 가장 높은 iamge-text similarity 를 갖는 것을 고른다.
infromation leak 을 피하기 위해, unimodal self-attention 을 차용하는데, 위의 그림에서와 같이 attention masking 을 활용하여 서로가 서로를 allow 할 수 없다.


<span style='color:green;font-weight:bold'> Image-grounded Text Generation (ITG) </span>
<br>
ITG 는 Q-former 가 given input image 로 부터 text 를 generation 하도록 학습한다.
UniLM 과 비슷하게 causal self-attention mask 를 통하여 구현한다.

<span style='color:green;font-weight:bold'> Image-Text Matching (ITM) </span>
<br>
ITM 은 image and text preresentation 사이의 fine-grained alignment 을 위해 사용된다.
image-text pair 를 잘 찾아내는 지의 binary classification 으로 구성된다.
bi-directional self-attnetion mask 를 통해 모든 query 와 text 들이 서로를 attend 할 수 있다.


![image](https://user-images.githubusercontent.com/42200027/221542885-d2f0bcd5-df4e-4ac2-b005-7b58b931a75b.png)

<span style='color:green;font-weight:bold'> Bootstrap Vision-to-Language Generative Learning from a Frozen LLM </span>
<br>

