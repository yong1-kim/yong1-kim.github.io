---
layout: post
title:  "[ICML2023] Large Language Models Struggle to Leanr Long-Tail Knowledge"
date:   2023-12-17 20:08:00 +0900
use_math: true
categories: [Transformer, LLM]
---

[[pdf]](https://proceedings.mlr.press/v202/kandpal23a/kandpal23a.pdf)
[[github]](https://github.com/nkandpa2/long_tail_knowledge)

**Nikhil Kandpal <sup>1*</sup>, Haikang Deng <sup>1*</sup>, Adam Roberts <sup>2</sup>, Eric Wallace <sup>3</sup>, Colin Raffel <sup>1*</sup>**
<br><sup>1</sup> UNC Chapel Hill <sup>2</sup> Google Research <sup>3</sup> UC Berkeley. Correspondence to: Nikhil Kandpal <nkandpa2@cs.unc.edu>.
 &emsp;

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/7bc7bdb2-1d44-4e02-8b31-0f2a0978293e)

# Abstract
- (Motivation) LLM 은 인터넷 속의 많은 지식을 배우지만, 특정한 정보는 web 에 흔하지만, 어떠한 정보는 그렇지 않다.
- 이 논문에서는 LLM 에 의해 기억된 knowledge 와 web 으로부터의 pre-training dataset 속의 정보의 관계를 연구한다.
- 정확히는, fact 기반의 question 을 답하는 LM 의 능력은, pre-training 시에 그 question 에 연관된 documents 를 얼마나 많이 보았는지에 relate 되었다는 것을 보인다.
- (Long-tail knowledge 약점) 오늘날의 모델들은 long-tail knowledge 에 취약하며, retrieval-augmentation 이 그 개선에 큰 역할을 함을 보인다.

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/5253139e-b16e-4749-b7b9-5bb4c4528e16)

# Introduction

LLM 은 well-known factiid 로부터 asoteirc domain-specific information 에 이르기 까지 많은 정보를 인터넷으로부터 얻는다.
모델은 이를 parameter 에 implicit 하게 저장하므로, 현재 LLM 의 크기와 pre-training dataset 의 크기는 매우 크다.
이를 위해 현재 인터넷으로부터 학습을 많이 진행하는데, 인터넷 속의 정보들은 equal 하게 등장하지 않으며, 특히 long-tail information 의 경우, 거의 등장하지 않거나 적게 등장한다.

<span style='background-color: #dcffe4'> 이 연구에서는 LLM 의 답변 능력과, 해당 question 이 pre-training 단계에서 얼마나 많은 document 에 등장하는지의 연관성을 조사한다. </span>
이들은 factoid QA dataset 인 [TriviaQA](https://aclanthology.org/P17-1147/)  와 [Natural Questions](https://aclanthology.org/Q19-1026/) 에 대하여, ground qa pair 가 concrete 한 subject-object 로 연결되는 것에 집중한다.
예를 들어, _(In what ciy was the poet Dante born? Florence)_ 라는 QA pair 에 대하여, Datne-Florence 는 co-occur 할 확률이 높다.
이 co-occurrence 를 identify 하기 위해, C4, Pile, ROOTS, OpenWebText 와 Wikipedia 등의 trillions of token 에 <span style='color:green;font-weight:bold'> entity linking pipeline  </span> 을 적용한다.

이들은 LM 의 능력은 pre-training document 에 등장하는 question 수, pre-training datset 크기, 그리고 모델 사이즈와 연관이 있음을 보인다.
이들은 또한 counterfactual re-training 실험을 수행하여, 4.8B 파라미터 LM 을 특정 document 에 대해 with/with-out train 시킨다. 모델 정확도는 relevant document 가 제거된 question 에서 크게 감소하는데, 이는 entity linking pipieline 을 검증하고 관찰된 상관 관계 경향이 실제로 인과 관계가 있을 가능성이 있다는 것을 보여준다.

마지막으로, 모델 스케일링과 Retrieval-augmentation 을 통해, pre-training 에 거의 등장하지 않는 knowledge 를 더 잘 capture 하는 방법을 분석한다.
모델 스케일과 관련하여, parmeter 수와 QA accuracy 간의 log-linear relationship 이 있음을 보인다.
이는 Long-tail question 에 대해, 일반적인 QA accuracy 를 얻기 위해서는 quadrillion param 정도로 극적인 파라미터 수 증가를 필요로 함을 의미한다.
Retrieval-augmented system 은 더 promising 한데, 이들은 LLM 의 long-tail question 에 대한 성능을 크게 향상시킨다.

# Identifying Relevant Pre-training Data 

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/c1f88df7-eef3-4674-89a6-b1c0df17337d)

<span style='color:green;font-weight:bold'> Background and Research Question </span><br>
LLM 은 최근 매우 좋은 성능을 보이지만 한 가지 의문점을 남긴다 : 현재로서는 LM 이 실제로 어떤 종류의 지식을 포착하는지 여전히 명확하지 않다. 예를 들어, 그들은 단순히 Pre-training 데이터에서 빈번하게 나타나는 "easy" fact 들만을 학습하는 것인가?
저자들은 in-context learning (ICL) 을 통해 QA 몇 개 를 prompt exempler 로 주고, 문제를 풀게 한 다음, LM 의 능력과 pre-training data 속의 관련된 정보량 사이의 관계를 조사한다.

<span style='color:green;font-weight:bold'> Approach </span><br>
저자들은 우선 question 속에 포함되어 있는 salient entity 를 찾아내고, ground-truth answer 의 alias set 들을 찾아낸다.
이후, salinet question entity 와 answer entity 가 co-occur 하는 pre-training document 를 찾아낸다.
위의 Figure 2 에서, salient question entity 와 salient answer entity 로 Dante-Alighieri 와 Florence 를 추출한 뒤, both entity 를 담고 있는 document 를 count 한다.

이들의 approach 는 [T-rex et al](https://aclanthology.org/L18-1544/)https://aclanthology.org/L18-1544/. 을 기반으로 하는데, 이 연구는 subject-relation-object triplet 중 subject 와 object 두 개만이 text 에 co-occur 하면, 그 triplet 역시 존재한다는 연구를 바탕으로 한다. 
추가적으로, human study 를 통해 이 counting pipeline 이 relevant document 를 잘 추출함을 보인다.
이러한 발견들을 기반으로, 저자들은 salinet question - answer entity 를 통해 찾은 document 를  
<span style='background-color: #dcffe4'> relevant document</span>라고 정한다. 

이 방법을 적용하기위해, massive pre-training corpora 를 entity-link  해야하고, downstream QA dataset 역시 적용해야 한다.

## 1. Entity Linking Pre-training Data

저자들은 [DBpedia Spotlight Entity Linker](https://www.dbpedia-spotlight.org/docs/spotlight.pdf) 를 활용한다. Pre-training data 는 (1) The Pile, (2) ROOTS, (3) C4, (4) OpenWebText, (5) Wikipedia 이다.
이 과정은 128-CPU-core machine 으로 3 주 정도 걸려서, 2.1 TB 의 entity link 를 생성한다.

## 2. Finding Entity Paris in QA Data

저자들은 TriviaQA 와 Natural Question 두 개의 open-domain QA dataset 을 entity link 한다.
Few-shot prompt example 로 쓰이는 일부 data 를 제외하고, training data 와 validation data 를 모두 사용한다.
먼저 각 예제에 대해 DBPedia Entity Linker 를 실행한다. 
하나의 질문에 대해 여러 답변이 있을 수 있으므로 질문과 모든 유효한 답변을 연결한다. 
이렇게 함으로써 더 정확한 Entity linking 이 가능해진다. 
이후, ground truth 집합에서 발견된 most common entity 를 salient answer entity 로 사용한다. 
그런 다음 질문에서 발견된 모든 entity 를 반복하면서, pre-training dataset 에서 salient answer entity 와 가장 많이 co-occur 하는 entity 를 선택한다. 
질문, 답변 또는 둘 다에서 entity 를 찾을 수 없는 경우 예제를 삭제한다. 
결과적으로 관련 문서의 수가 0 이면 역시 예제를 삭제한다. 
이는 엔터티 링킹 오류 때문일 가능성이 있기 때문이다.

## 3. Human Evaluation of Document Counting Pipeline

인간 평가 결과, TriviaQA 의 60% 정도에 해당하는 것들이 relevant 하다는 결과를 얻었다.
Pipeline 이 완벽하지 않은 이유는 entity linker 가 잘못되었거나, saline question entity 와 salient answer entity 를 모두 가졌다고 해서 모든 document 가 relevant 하지 않기 때문이다. 그러나, 추후 실험에서 이 pipeline 이 충분히 efficient 하다는 것을 검증한다.

# LM Accruacy Depends on Relevant Doucment Count

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/a09460ec-3f1e-4deb-932d-1c4c14768278)

우선, LLM 의 답변 능력과 pre-training corpus 에서의 relevant document 의 수의 관계성을 측정한다. 
일들은 GPT-Neo family, BLOOM family , GPT-3 family 에 대해서 실험한다.
GPT-Neo 는 Pile pre-training dataset 에 BLOOM 은 ROOTS 에, GPT-3 는 알려지지 않았지만 OpenWebText 를 통해 시뮬레이션 하였다.
모델이 new line(\n) 을 생성할 때까지 greedy decoding 시켰고, Exact Match (EM) 을 메트릭으로 사용한다.

BLOOM 의 실험 결과는 Figure 1 (맨위)에, GPT-Neo 의 실험결과는 Figure 3 에서 볼 수 있다. 
두 실험 모두 같은 경향성을 보이며, NQ 에 대한 GPT-Neo 의 실험결과는 Figure 4 에서 볼 수 있으며, 역시 같은 경향성을 보인다. 

<span style='color:green;font-weight:bold'> Simpler Methods for Identifying Relevant Documents
Are Less Effective  </span><br>

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/960708c5-36c5-4155-9bed-751651323704)

현재의 방법은 salient Q entity 와 salient A entity 가 co-occur 하는 것을 relevant document 로 정의했는데, salient Q entity 혹은 salient A entity 만 등장하는 것을 relevant document 로 해서 count 한 것과의 연관성을 본다. 
왼쪽에서, Q 와 A 만으로도 증가하는 것처럼 보이지만, Q 와 A 가 co-occur 하는 것이 5 번 보다 적은 document 들로 한정하여, Q entity 혹은 A entity 만을 relevant 의 기준으로 삼으면 성능이 좋아지지 않는 것을 보인다.

<span style='color:green;font-weight:bold'> Humans Show Different Trends Than LMs  </span><br>

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/c01966fc-3367-450c-b81c-ee304020e2b3)

이러한 결과에 대한 설명은 relevant document 수가 적은 질문이 단순히 "harder"하는 것이며, 이로 인해 모델 성능이 하락하는 것일 수 있다는 것이다. 
그러나 이것이 사실이 아님을 Natural Questions에서 인간 정확도를 측정함으로써 보여준다.
저자들은 5명의 다른 인간 평가자에 의해 레이블이 지정된 질문을 사용하여 하나의 평가자를 제외하고 나머지 네 명을 참 값 답변 집합으로 사용하는 "leave-one-annotator-out" 지표를 사용한다.
Figure 7 에 인간 정확도 대 관련 문서 수를 보면, **인간 정확도는 실제로 관련 문서가 적은 질문에 대해 가장 높은데**, 이는 LM 과 반대 경향성이다. 
저자들은 가설을 세우는데, 관련 문서가 적은 질문에 대해 인간이 더 뛰어난 이유는 (1) 더 rarer 한 사실에 관한 질문이 일반적인 entity 와 비교하여 **간단한 문제일 가능성**이 있기 때문이며, (2) 평가자에게 제공되는 위키피디아 문서가 rare 한 entity에 대해서는 더 짧기 때문에 독해가 더 쉽고 평가자 간의 일치도가 높아지기 때문이다.


<span style='color:green;font-weight:bold'> Causal Analysis via Re-training  </span><br>

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/b83e27f3-e864-4fe4-a430-8434851911ae)

지금까지의 결과는 상관적인 성격을 가지고 있다. 이러한 결과를 설명할 수 있는 알려지지 않은 변수가 있을 수 있으며, 다시 말해 더 rarer 한 질문이 다른 이유로 LM 에게 더 어려울 수 있다. 
여기서는 훈련 데이터에서 특정 문서를 제거하고 LM을 다시 훈련함으로써 인과 관계를 확립해본다.

우선은 기존 연구 ([Wang et al. 2022](https://arxiv.org/abs/2204.05832))에서의 설정을 따라 C4 에서 4.8B LM d을 훈련시킨다. 
그런 다음 Training dataset 에서 특정 문서를 삭제하는 효과를 측정한다. 
각 관련 문서 수의 로그 스케일 bin에 대해(예: $10^0$에서 $10^1$ 관련 문서, $10^1$에서 $10^2$, ...) Trivia QA에서 100개의 질문을 샘플링하고 C4에서 해당 질문의 모든 관련 문서를 제거한다. 이로써 전체 C4의 약 30%가 제거된다. 
마지막으로 이 수정된 Pre-training dataset 에서 "counterfactual" LM을 훈련시키고 이를 기준 모델과 비교한다. 
기준 모델과 대조 모델 모두 1회 에폭 동안 훈련하는데, 대조 모델은 30% 더 적은 단계 동안 훈련되며, 이로 인해 전반적으로 성능이 약간 떨어진다. 
이를 감안하여 관련 문서가 제거된 질문에 대한 성능만을 고려한다.

결과는 위의 그림과 같다.
원래 C4 데이터셋에서 관련 문서가 적은 질문의 경우, 기준 및 대조 LM 모두 성능이 나쁘며, 즉 그들의 성능 차이가 작다. 
그러나 많은 관련 문서가 있는 질문의 경우, 대조 LM의 성능이 크게 나빠진다. 
이는 관련 문서 수와 QA 성능 간에 인과적인 연결이 있다는 것을 시사한다.

# Methods to Improve Rare Fact Learning
지금까지 LLM 이 Relevant Document count 에 강한 dependency 를 가지고 있음을 확인하였다. 여기서는 이 dependene 을 완화하는 방법을 연구한다: 데이터 규모 증가, 모델 규모 증가, Retrieval augmented system 추가 

<span style='color:green;font-weight:bold'> Can We Scale Up Datasets? </span><br>

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/7bf994dd-e5e5-44f9-b27b-478152fb7d85)

최근 LLM 들은 몇 백 Billion parameter 로 학습된다. 실험 결과는 relevant document 가 log scale 로 커져야하기 때문에, 5 배 정도의 pre-training dataset size 증가는 큰 도움이 안될 것이다.
그러면 pre-training dataset 의 diversity 를 늘리는 것은 가능할까? 위의 표를 보면 놀랍게도, 모든 pre-training dataset 이 독립적으로 crawling 된 것임에도 불구하고, TiriviaQA 와의 correlation 은 거의 유사하여, 그렇게 까지 '다른' pre-training dataset 이라고 하기 어렵다.

<span style='color:green;font-weight:bold'> Can We Scale Up Models? </span><br>

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/272c1d38-617d-42e1-9bb4-2f4a2257e690)

Long-tail question (relevant document 수가 적은 question) 의 경우, 성능 향상을 보장하기 위해서는 기하급수적으로 크기를 늘려야한다. 위의 figure 에서, NQ dataset 중 100 개 이하의 revlant docs 을 갖는 long-tail 질문에 대하여, BLOOM 모델이 strong supervised model 이나 Human accuracy 에 비슷해지기 위해서는 무려 $10^18$ 까지 증가시켜야 한다. (증가시키진 않고 extrapolating 으로 $R^2=0.98$ linear fit 했을 때)

<span style='color:green;font-weight:bold'> Can We Use Retrieval Augmentation? </span><br>

**Orcale Retrieval**

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/544b5334-759a-42da-ac86-c93493a506cd)

위의 그림에서 보듯이, Wikipedia 의 gold paragrph 를 oracle 로 주어주면, GPT 가 적은 relevant document 질문에 대해서도 struggle 하지 않고 잘 하는 것을 볼 수 있다. 
직전의 human 의 경험과 비슷하게, relevant document 크기가 큰 질문일 수록 성능이 작아진다. 
(because rare questions are easier on average when relevant context information.)

**BM25 Retrieval**

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/c0d62bd9-e833-4c21-834d-7a5c232bdc88)

다음으로 common retrieval augmented baseline 을 따랐을 때, BM25 검색기 (Robertson & Zaragoza, 2009)를 사용하여 위키피디아에서 단락을 선택한다. 
상황별 훈련 예제와 테스트 질문 모두에 대해 상위 3개의 가장 높은 점수를 받은 단락을 추가한다. 
각 상황별 훈련 예제에 대해 검색된 단락 중 적어도 하나가 답변을 포함하도록 확인하여 LM 이 문서를 활용하는 방법을 학습하도록 하는 것이다.

먼저 BM25 검색기의 상위 k 리콜을 관련 문서 수의 함수로 평가하고 결과를 Figure 8 에서 볼 수 있다.
BM25가 특히 k의 큰 값에 대해 상당히 높은 리콜을 달성한다는 것을 발견할 수 있다. 
그러나 BM25 검색기는 여전히 **관련 문서 수에 대한 약한 의존성**을 보여준다. 

다음으로 Natural Questions에서 BM25-보강 GPT-Neo 모델의 정확도를 평가하고 결과를 Figure 9에서 볼 수 있다.
전반적으로 Retrieval-augmented model 은 모든 관련 문서 수 범위에서 closed-book 모델을 능가하며, 특히 rare example 에서 더 뛰어납니다. 
**이러한 결과는 검색 보강이 사전 훈련 데이터셋에서 관련 문서가 적은 질문에 대한 성능 향상을 위한 유망한 방법을 제공한다는 것을 시사한다.**

# Conclusion and Future Work
```
Large language models, trained on extensive internet text, exhibit notable few-shot learning capabilities. The release of LLMs and their pre-training datasets to the open source allows researchers to explore the origins of these capabilities. Our study, one of the first to connect LLM behavior to pre-training data, reveals that while LLMs perform moderately on open-domain QA benchmarks, their success is mainly limited to questions reflecting widely available pre-training knowledge. This prompts further investigation into enhancing long-tail knowledge retention, beyond simple scaling of model and dataset size. We are particularly enthusiastic about refining retrieval-augmented LMs, emphasizing efficiency and accuracy. Additionally, our focus on factoid question answering knowledge learning raises questions about similar relationships in other tasks. While our analysis centers on memorization's impact on question answering, it may extend to tasks involving (or avoiding) memorized knowledge, such as analyzing private text, common-sense reasoning, or predicting source code. Lastly, we encourage ongoing few-shot learning evaluations to uncover model behavior by tracing accuracy back to pre-training data properties, providing insights into existing models' strengths, weaknesses, and potential avenues for improvement.
```

