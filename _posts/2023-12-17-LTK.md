---
layout: post
title:  "[ICML2023] Large Language Models Struggle to Leanr Long-Tail Knowledge"
date:   2023-12-17 20:08:00 +0900
use_math: true
categories: [Transformer, LLM]
---

[[pdf]](https://proceedings.mlr.press/v202/kandpal23a/kandpal23a.pdf)
[[github]](https://github.com/nkandpa2/long_tail_knowledge)

**Nikhil Kandpal <sup>1*</sup>, Haikang Deng <sup>1*</sup>, Adam Roberts <sup>2</sup>, Eric Wallace <sup>3</sup>, Colin Raffel <sup>1*</sup>**
<br><sup>1</sup> UNC Chapel Hill <sup>2</sup> Google Research <sup>3</sup> UC Berkeley. Correspondence to: Nikhil Kandpal <nkandpa2@cs.unc.edu>.
 &emsp;

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/7bc7bdb2-1d44-4e02-8b31-0f2a0978293e)

# Abstract
- (Motivation) LLM 은 인터넷 속의 많은 지식을 배우지만, 특정한 정보는 web 에 흔하지만, 어떠한 정보는 그렇지 않다.
- 이 논문에서는 LLM 에 의해 기억된 knowledge 와 web 으로부터의 pre-training dataset 속의 정보의 관계를 연구한다.
- 정확히는, fact 기반의 question 을 답하는 LM 의 능력은, pre-training 시에 그 question 에 연관된 documents 를 얼마나 많이 보았는지에 relate 되었다는 것을 보인다.
- (Long-tail knowledge 약점) 오늘날의 모델들은 long-tail knowledge 에 취약하며, retrieval-augmentation 이 그 개선에 큰 역할을 함을 보인다.

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/5253139e-b16e-4749-b7b9-5bb4c4528e16)

# Introduction

LLM 은 well-known factiid 로부터 asoteirc domain-specific information 에 이르기 까지 많은 정보를 인터넷으로부터 얻는다.
모델은 이를 parameter 에 implicit 하게 저장하므로, 현재 LLM 의 크기와 pre-training dataset 의 크기는 매우 크다.
이를 위해 현재 인터넷으로부터 학습을 많이 진행하는데, 인터넷 속의 정보들은 equal 하게 등장하지 않으며, 특히 long-tail information 의 경우, 거의 등장하지 않거나 적게 등장한다.

<span style='background-color: #dcffe4'> 이 연구에서는 LLM 의 답변 능력과, 해당 question 이 pre-training 단계에서 얼마나 많은 document 에 등장하는지의 연관성을 조사한다. </span>
이들은 factoid QA dataset 인 [TriviaQA](https://aclanthology.org/P17-1147/)  와 [Natural Questions](https://aclanthology.org/Q19-1026/) 에 대하여, ground qa pair 가 concrete 한 subject-object 로 연결되는 것에 집중한다.
예를 들어, _(In what ciy was the poet Dante born? Florence)_ 라는 QA pair 에 대하여, Datne-Florence 는 co-occur 할 확률이 높다.
이 co-occurrence 를 identify 하기 위해, C4, Pile, ROOTS, OpenWebText 와 Wikipedia 등의 trillions of token 에 <span style='color:green;font-weight:bold'> entity linking pipeline  </span> 을 적용한다.

이들은 LM 의 능력은 pre-training document 에 등장하는 question 수, pre-training datset 크기, 그리고 모델 사이즈와 연관이 있음을 보인다.
이들은 또한 counterfactual re-training 실험을 수행하여, 4.8B 파라미터 LM 을 특정 document 에 대해 with/with-out train 시킨다. 모델 정확도는 relevant document 가 제거된 question 에서 크게 감소하는데, 이는 entity linking pipieline 을 검증하고 관찰된 상관 관계 경향이 실제로 인과 관계가 있을 가능성이 있다는 것을 보여준다.
