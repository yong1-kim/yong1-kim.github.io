---
layout: post
title:  "[EMNLP2023] Retrieval-Generation Alignment for End-to-End Task-Oriented Diaogue System"
date:   2024-01-24 17:00:00 +0900
use_math: true
categories: [Retrieval, Dialogue]
---

[[pdf]](https://arxiv.org/pdf/2305.06983.pdf) &emsp;
[[github]](https://github.com/jzbjyb/FLARE)

**Weizhou Shen<sup>1</sup>, Yingqi Gao<sup>1</sup>, Canbin Huang<sup>1</sup>, Fanqi Wan<sup>1</sup>, Xiaojun Quan<sup>1*</sup>, Wei Bi<sup>2*</sup>**
<br><sup>1</sup> School of Computer Science and Engineering, Sun Yat-sen University, China <sup>2</sup> Tencent AI Lab &emsp;

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/00c7778c-3dc1-415c-a07a-6e6b0539cab6)

## Abstract
- (**Retrieval for TOD**) Localized and specialized task 를 효과적으로 처리하기 위해, Task-Oriented Dialogue(TOD)은 Knowledge Base (KB) 에서 정보를 Retrieval 해온다.
- (**Retrieval Error**) 그러나, ChatGPT 나 T5 등의 generative model 이 KB record 에서 retrieved 된 정보를 처리할 때, 사소한 차이점으로 인한 잘못된 결과를 생성해낸다.
- (**Proposed Method**) 이 논문에서는 maximal marginal likelihood 를 사용하여, response generation 으로부터의 signal 을 통해 perceptive retriever 를 학습시킨다.
- (**Experiment**) 이 방법으로 학습된 retriever 를 T5 와 ChatGPT 를 backbone 으로 하여 싫머을 진행하였을 떄, high-quality 의 knowledge record 로 부터, 좋은 response 를 generate 하는 것을 검증한다.

## 1.Introduction

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/713638ed-3f2c-4c7d-9d36-5f69bb4bc8a7)

<span style='color:green;font-weight:bold'> TOD : Task-Oriented Dialogue System </span>
<br>
Task-Oriented Dialogue System (TOD)은 기차 예약, 스케쥴 조정 등 특정한 목표를 수행을 돕는 시스템이다.
보통 TOD 는 pipeline 과 End2End 형식으로 나뉘는데, DST, Policy 등의 모듈이 나눠져 파이프라인 형식으로 진행이 되거나, 중간 개입 없이 한 번에 response 를 generate 하는 방식이 각각 그것들이다.
Pipeline 모델과 그 각 모듈들에 대한 연구가 성행하다가, <span style='background-color: #dcffe4'>  최근 Large Language Model (LLM) 의 출현 덕분에, End2End 모델에 대한 관심도가 급증 </span> 하고 있다.

<span style='color:green;font-weight:bold'> RAG : Retrieval-augmented generation </span>
<br>
Retrieval-augmented generation (RAG) 은 result 를 generate 하기 위해 외부 지식을 retrieval 하여 활용하는 것을 말한다.
[Q-TOD](https://arxiv.org/pdf/2210.07564.pdf) 에서는 E2E-TOD 에 RAG 방법을 적용하여 기존의 방법들을 훨씬 뛰어넘는 성능을 보였다.
하지만 저자들의 preliminary study 에 따르면, <span style='background-color: #dcffe4'> knowledge retriver 의 perofrmance 와 reponse generator 의 performance 사이의 correlation 은 상당히 약하며, </span> 이 것은 **retriever 을 imporve 한다고 해서 전체적인 generation 성능이 좋아지는 것은 아니라는 것**을 의미한다.
저자들은 이 현상을 <span style='color:green;font-weight:bold'> misalignment </span> between retireval and generation 이라고 명명한다. 이 현상이 최근 E2E-TOD 의 발목을 잡는 bottleneck 이다.

<span style='color:green;font-weight:bold'> Qualitative analysis </span>
<br>
Qualitative analysis 로, 저자들은 이 misalignment 는 <span style='background-color: #dcffe4'> homegeneity of retreived knowledge entity </span>  때문이라고 가정한다.
위의 Figure 1 과 같이, retrieved 되어 온 entity 들은 약간의 차이점을 제외하고, 높은 수준의 유사성을 보인다.
결과적으로, reponse generator 는 knowledge-related token 보다, 학습된 language token 에 predominant 하게 반응하여 결과를 생성해낸다고 본다.

<span style='color:green;font-weight:bold'> MK-TOD : Meta Knowledge for TOD </span>
<br>
이에 저자들은 **M**eta **K**nowledge for end-to-end **T**ask-**O**riented **D**ialogue system (**MK-TOD**) 을 제안한다.
MK-TOD 는 언급된 misalignment 를 해결하는 것을 목표로 한다.
우선, [maximal marginal likelihood](https://proceedings.neurips.cc/paper_files/paper/2021/file/da3fde159d754a2555eaa198d2d105b2-Paper.pdf) 방법을 통해 retriever 가 학습 과정 내내 progressive 하게 학습되도록 하였고, response generator 가 entity 들을 잘 구분하게 하기 위하여, **meta knowledge** 를 사용할 수 있는 능력을 갖추게 한다.
여기서, meta knowledge 는 retrieval 에 관련된 추가 정보로, retrieval order, retrieval confidence, co-occurrence rate 으로 이뤄진다.
Meta knowledge 는 세 가지 접근법을 통해 학습을 진행해 보는데, (1) special prefix token 을 추가, (2) prompt 활용, (3) contrastive learning 적용이다. 
또 추가적으로 generator 가 discriminative ability 를 갖게 하기 위하여, negative knowledge 를 사용하는 방법도 실험한다.

<span style='color:green;font-weight:bold'> Experiments </span>
<br>
MK-TOD 를 T5 와 ChatGPT 모델을 backbone 으로 하여 적용한뒤, 다른 E2E-TOD system 과 비교실험을 진행한다.
SMD, CamRest, Woz 같은 benchmark dataset 에 대하여 제안된 system 이 기존의 SOTA 를 뛰어넘는 성능을 보인다.
또한, MK-TOD 를 통해 ChatGPT의 in-context learning 을 효과적으로 향상시킬 수 있음을 보인다.
추가적인 분석으로, meta-knowledge 의 학습이 misalingment 개선에 큰 도움이 되는 것을 보인다.

## 2. Methodology : MK-TOD

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/1d752f27-b477-4ac5-80e0-24bb479154ff)

MK-TOD 방법에 대한 개요는 Figure 2 에 나와있다. 각 turn 을 생성하는데 있어, retirever 가 relevant entity 를 추출해 온 후, meta knowledge 와 결합하여 reponse generator 에 주어진다. reponse generator 는 이 정보들을 활용하여 한 문장씩 생성하며, normal text generation likelihood 과 marginal likelihood 를 모두 높인다.

# 2.1. Notations

- Dialog $D = (u_1,r_1, ..., u_T,r_T)$ with $T$ turns where $u_t$ and $r_t$ are the $t$-th turn user and system utterances.
- Context $c_t = (u_1,r_1, ..., u_t)$
- External KB $K = (e_1, e_2, ..., e_B)$ with knowledge base size $B$
- $r_t$를 생성하기 위해, $c_t$ 와 $K$ 를 입력으로 받는다.

# 2.2. System Overview

Retrieve module 속의 context encoder 가 context 를 embedding 한 후, 그 embedded vector 를 eneity encoder 가 encode 한 KB 속의 knowledge vector 들과 score 를 매겨, Top-K 를 뽑아온다.
이후, 이 Top-K entity 들은 Meta Knowledge 과 concate 하여 response generator 에 주어진다.

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/be0ce773-5f17-4c7e-b942-33a6784e6a00)

Response generator 는 위의 식을 통해 각각 entity $\epsilon_t$ 에 따른 response 의 prob 을 생성한다.
이후 negative log-likelihood (NLL) 를 통해 text generation 을 학습한다.

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/1bf65500-edf5-4666-b09f-64af2a3bf03e)

# 2.3. Maximum Marginal Likelihood

<span style='background-color: #dcffe4'> 학습과정에서 Retrieval label 이 없기 때문에, generator 로부터의 supervision signal 을 활용하여 retriever 를 학습한다. </span>
그러나, 위의 NLL loss 를 backpropagate 하는 것은 불가능하기 때문에, **Maximum Maringal Likelihood (MML)** 를 활용한다.

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/8ac392d2-4960-4872-a7e1-f9376d6ef608)

Knowledge base 속의 entity 의 likelihood 를 모두 통합하여 knowledge 전체의 Bayesian perspective 를 제공한다.
$\pi$는 retriever 의 parameter 고, $q$는 entity 하나 에 대한 retrieval prob 이다.
이 때, 모든 entity 에 대한 $q$ 계산이 cost 가 들기 때문에, [EMDR](https://proceedings.neurips.cc/paper_files/paper/2021/file/da3fde159d754a2555eaa198d2d105b2-Paper.pdf) 의 방법과 마찬가지로, 
retrieved 된 Top-K $\epsilon$ entity 로 대체한다.
따라서 아래처럼 $K$ 대신 $\epsilon$ 으로 식이 변경된다.

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/035a91f3-7718-489c-bf91-1b4c9d913702)

이에 따라, MML loss 는 아래와 같으며,

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/92dd9f6e-4ea5-49d2-8017-e654038950e6)

최종적으로 hyperparam $\alpha$ 와 $\beta$ 를 통해 아래의 최종 loss 로 합쳐진다.

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/1ef0f453-0bbf-40cb-8ff6-f1f4e531ea68)

# 2.4. Meta Knowledge

Generator 가 올바른 entity 를 선택하도록 guide 하여, misalignment 를 해결하기 위해 meta knowledge 를 도입한다.
Meta knowledge 는 다음 세 가지로 이뤄진다 : **(1) Retrieval order, (2)Retreival confidence, (3)Co-occurrence** 
Retrieval confidence 는 hyper param 을 기준으로 세 가지 (high, middle, low-confidence) 로 나눠 구성되고, Co-occurence 는 이전의 context 에서 나왔던 entity 에 대해 그 정보를 기록한다.

앞 서 말했듯 이 meta knowledge 를 적용하기 위해 세 가지 approach 를 design 한다.

<span style='color:green;font-weight:bold'> (1) Prefix </span>
<br>
Special token 을 활용한 mapping function 이다. 
예를 들어, second ranking - middle confidence - not yet mentioned in the context entity 라면, <2nd-entity>, <mid-confidence>, <new-entity> 이렇게 mapping 한다.

<span style='color:green;font-weight:bold'> (2) Prompt </span>
<br>
_“This is the top-1 recalled entity with low confidence”_ 와 같은 형식으로 prompt 를 추가하는 방법이다.

<span style='color:green;font-weight:bold'> (3) Contrastive Learning </span>
<br>
Retriever 로 부터 추출된 $\epsilon$ entity 들을 활용한 contrastive learning 형식으로 generator 를 학습할 수 있다.
※ Contrastive learning 에 대한 자세한 내용은 논문 참고.

# 2.5. Negative Entity

Information Retrieval 에서의 [negative sampling 기법](https://aclanthology.org/2020.emnlp-main.550/)과 같이,  $K$ 에서 가장 낮은 score 를 보이는 entity 하나를 nagative neity 로 하여, special negative meta knowledge 와 함께 학습에 활용한다. 

## 3. Experimental Settings 
# 3.1. Datasets and Metrics
<span style='color:green;font-weight:bold'> Datasets </span>
<br>
- MultiWOZ 2.1 (MWOZ)
- CamRest
- Stanford Multi-Domain (SMD)

KB 는 session-level 일 수도 있고, dataset 전반에 걸쳐 공통으로 사용되는 것일 수도 있다.

<span style='color:green;font-weight:bold'> Metrics </span>
<br>
- BLEU
- Entity F1
- Recall@K

# 3.2. Implementation Details
- Retriever : BERT
- Generator : T5, ChatGPT (in-context learning)
※ ChatGPT : Retriever 는 T5 를 이용하여 학습된 retriever 를 활용한 inference 만 진행

# 3.3. Basline Methods
<span style='color:green;font-weight:bold'> Implicit Retrieval </span>
<br>
- DF-NET
- EER
- FG2Seq
- CDNET
- GPT-kE

<span style='color:green;font-weight:bold'> Explicit Retrieval </span>
<br>
- Q-TOD
- DialoKG
- MAKER

<span style='color:green;font-weight:bold'> LLM </span>
<br>
- ChatGPT (gpt-3.5-turbo)

## 4. Results and Anlaysis
# 4.1. Overall Results with Large-Scale KBs

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/9c87d7c7-b203-422c-ab9b-9014c66928c5)


 
<span style='color:green;font-weight:bold'> 초록색볼드체 </span>

<span style='background-color: #dcffe4'> 초록색배경 </span>
