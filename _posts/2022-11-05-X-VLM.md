---
layout: post
title:  "[ICML2022] Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts"
date:   2022-11-05 19:12:00 +0900
use_math: true
categories: [Vision-and-Language, Transformer]
---
[[pdf]](https://arxiv.org/pdf/2111.08276.pdf)  &emsp;
[[github]](https://github.com/zengyan-97/X-VLM) <br>

**Yan Zeng<sup>1</sup>, Xinsong Zhang Chaganty<sup>1</sup>, Hang Li<sup>1</sup>
<br><sup>1</sup>ByteDance AI Lab. Correspondence to: Yan Zeng. &emsp; 

![image](https://user-images.githubusercontent.com/42200027/200114819-21558181-1aa6-4491-955d-ee9a59887dae.png)

# Abstract
- 기존의 Vision-and-Language 방법들은 object detection 을 이용한 object-cnetric feature 에 의존하여 학습됨.
- 이러한 방법으로는 <span style='color:green;font-weight:bold'>  multiple object 들의 relation </span>  을 배우기 어렵다는 단점이 있음.
- 본 논문에서는 <span style='color:green;font-weight:bold'> multi-granularity </span> 를 통해 이 문제를 해결하여, state-of-the-art 를 달성함.

#Introduction
