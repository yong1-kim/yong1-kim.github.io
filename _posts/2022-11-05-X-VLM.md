---
layout: post
title:  "[ICML2022] Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts"
date:   2022-11-05 19:12:00 +0900
use_math: true
categories: [Vision-and-Language, Transformer]
---
[[pdf]](https://arxiv.org/pdf/2111.08276.pdf)  &emsp;
[[github]](https://github.com/zengyan-97/X-VLM) <br>

**Yan Zeng<sup>1</sup>, Xinsong Zhang Chaganty<sup>1</sup>, Hang Li<sup>1</sup>**
<br><sup>1</sup>ByteDance AI Lab. Correspondence to: Yan Zeng. &emsp; 

![image](https://user-images.githubusercontent.com/42200027/200114819-21558181-1aa6-4491-955d-ee9a59887dae.png)

# Abstract
- 기존의 Vision-and-Language 방법들은 object detection 을 이용한 object-cnetric feature 에 의존하여 학습됨.
- 이러한 방법으로는 <span style='color:green;font-weight:bold'>  multiple object 들의 relation </span>  을 배우기 어렵다는 단점이 있음.
- 본 논문에서는 <span style='color:green;font-weight:bold'> multi-granularity </span> 를 통해 이 문제를 해결하여, state-of-the-art 를 달성함.

#Introduction
현재 Vision-Language task 들은 대부분 Pre-trained Vision-Language Model (VLM) 의 fine-tuning 을 통해 좋은 성능을 보이고 있다. 현재 방법들은 대부분 위의 그림의 (a), (b) 의 두 approach 를 활용한다. 첫 번째 (a) 의 경우, object detction 모델을 미리 활용하여 object 를 뽑아놓거나 ([[1]](https://aclanthology.org/D19-1000.pdf), [[2]](https://arxiv.org/pdf/1908.02265.pdf), [[3]](https://arxiv.org/pdf/2006.06195.pdf)) on-the-fly 로 object detection 을 활용 ([[4]](https://arxiv.org/pdf/1908.08530.pdf), [[5]](https://aclanthology.org/2021.acl-long.42.pdf))하여 fine-grained (object-centric) feature 와 text 를 align 한다. 그리고 두 번째 (b) 의 경우, object detection 을 활용하지 않고, coarse-grained (overall) feature 와 text 를 align 하여 학습([[6]](https://arxiv.org/pdf/2004.00849.pdf), [[7]](https://arxiv.org/pdf/2102.03334.pdf), [[8]](https://proceedings.neurips.cc/paper/2021/file/505259756244493872b7709a8a01b536-Paper.pdf))을 한다. 두 방법 모두 단점이 존재하는데, fine-grained 의 경우, 몇몇의 dtected object 들이 text 와 관련이 없고, multiple object 들의 서로간의 relation 을 잡아내기 어렵다. 그리고, 모든 downstream task 에 맞춰 카테고리를 pre-define 하기 어렵다. 그리고, coarse-grained 의 경우, object-level feature 를 배울 수 없기 때문에, visual reasoning, visual grounding, 그리고 image captioning 과 같은 downstream task 에 대하여 좋지 못한 성능을 보인다. 이 논문에서는 <span style='color:green;font-weight:bold'> 두 방법의 장점을 취하기 위해 obejct-level 과 image-level 에 국한되지 않은 multi-grained alignment 방법을 제안 </span>한다.

본 논문에서는 위의 그림과 같이 multi-grained vision-language pre-trianing 방식을 위해, 세 가지 방향으로 trainign data 를 구성한다: <span style='background-color: #dcffe4'> 1) 전체 이미지를 설명하는 image caption, 2) "man wearing backpack" 과 같이 region 을 설명하는 region description, 그리고 3) "backpack" 과 같은 detector 가 찾아낸 object label.</span>  따라서 training data reformulation 을 통해, 하나의 image-text pair 가 여러 image (region) 과 여러 text label 을 갖게 변형되고, "visual concept" 이라는 개념은 object, region, 그리고 image 를 모두 설명하는 개념이 된다. 

제안되는 *** X-VLM ***  모델은, two-stream 구조로, image encoder, text encoder 그리고 fusion cross-modal encoder 로 구성된다. <span style='background-color: #dcffe4'> X-VLM 의 학습은 두 가지로 구성된다: 1) visual concept 을 associated text 에 따라 location 시키는 box-regression, IOU loss 2) text 를 visual concept 에 align 시키는 contrastive loss, matching loss, Masked Language Modeling(MLM) loss</span>. 

X-VLM 은 여러 가지 downstream task 에서 강력한 성능을 보인다. **image-text retrieval** 에서 [VinVL](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_VinVL_Revisiting_Visual_Representations_in_Vision-Language_Models_CVPR_2021_paper.pdf) 모델보다 4.65% 높은 성능 (R@1 on MSCOCO) 을 보였고, [ALIGN](https://arxiv.org/pdf/2102.05918.pdf), [ALBEF](https://proceedings.neurips.cc/paper/2021/file/505259756244493872b7709a8a01b536-Paper.pdf), 그리고 [METER](https://arxiv.org/abs/2111.02387) 와 같은 최신 transformer 기반 모델보다 좋은 성능을 보인다. **Visual Reasoning task** 에서는 [VinVL](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_VinVL_Revisiting_Visual_Representations_in_Vision-Language_Models_CVPR_2021_paper.pdf) 보다 VQA 에서 0.79%, NLVR2 에서 1.06% 높은 성능을 보인다. 심지어 1.8B 의 거대한 in-house data 로 학습한 [$SimVLM_{base}$](https://arxiv.org/pdf/2108.10904.pdf) 보다도 높은 성능을 보였다. ** Visual Grounding ** 에서는 [UNITER](https://arxiv.org/abs/1909.11740) 보다 4.5% 향상, [MDETR](https://openaccess.thecvf.com/content/ICCV2021/papers/Kamath_MDETR_-_Modulated_Detection_for_End-to-End_Multi-Modal_Understanding_ICCV_2021_paper.pdf) 보다 1.1% 향상을 이루어 냈다. Captioning 에서는 [$SimVLM_{base}$](https://arxiv.org/pdf/2108.10904.pdf) 과 유사한 성능을 보여주었다.

# Method
![image](https://user-images.githubusercontent.com/42200027/200116707-65c7f24b-d2be-47d8-896c-bb4cdc4b607b.png)
X-VLM 은 two-stream framework 로, image encoder ($I_{trans}$), text encoder ($T_{trans}$),  그리고 cross-modal encoder ($X_{trans}$) 세 가지로 이뤄져 있다.  세 encoder 모두 [transformer](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) 를 기반으로 한다. 저자들은 pre-training dataset 을 bounding box 와 associated text 가 있는 region, object 로 나누었고, $(I, T, \{(V^j, T^j)\}^N ) $ 으로 표기한다. 어떠한 경우 associated text 가 없어 T 가 NaN 일 때도 있고, bounding box 가 없어 N=0 일 때도 있다. 해당하는 boudning box $b_j$ 는 (cx, cy, w, h) 로 normalize 된다. 전체 image itself 의 bounding box $b$ = (0.5, 0.5, 1, 1) 이 된다.

<span style='color:green;font-weight:bold'> Vision Encoding </span>

# Conclusion
quoted from paper
- We propose performing multi-grained vision language pre-training to handle the alignments between texts and visual concepts.
- We propose to optimize the model (X-VLM) by locating visual concepts in the image given the associated texts and in the meantime aligning the texts with the visual concepts, where the alignments are in multigranularity.
- We empirically verify that our approach effectively leverages the learned multi-grained alignments in finetuning. X-VLM consistently outperforms existing state-of-the-art methods on many downstream V+L tasks.
