---
layout: post
title:  "[NeurIPS2023] Meta-in-context learning in large language models"
date:   2024-01-10 12:42:00 +0900
use_math: true
categories: [LLM]
---

[[pdf]](https://openreview.net/pdf?id=sx0xpaO0za)
[[github]](https://github.com/yandachen/In-context-Tuning)

**Julian Coda-Forno <sup>1,2,∗</sup>, Marcel Binz <sup>1</sup>, Zeynep Akata <sup>2</sup>, Matthew Botvinick <sup>3</sup>, Jane X. Wang <sup>3</sup>, Eric Schulz <sup>1</sup>**
<br><sup>1</sup> Max Planck Institute for Biological Cybernetics, <sup>2</sup> University of Tübingen - Tübingen, Germany <sup>3</sup> Google DeepMind - London, United-Kingdom <sup>1</sup> Gaoling School of Artificial Intelligence, Renmin University of China <sup>2</sup> School of Information, Renmin University of China <sup>3</sup> DIRO, Université de Montréal &emsp;

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/72321483-0834-465e-959d-1f2da731f4c4)

# Abstract
- 

<span style='color:green;font-weight:bold'> 초록색볼드체 </span>

<span style='background-color: #dcffe4'> 초록색배경 </span>
