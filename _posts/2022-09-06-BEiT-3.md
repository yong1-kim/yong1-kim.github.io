---
layout: post
title:  "[BEIT-3] Image as a Foreign Language: BEIT Pretraining for All Vision and Vision-Language Tasks"
date:   2022-09-06 10:27:00 +0900
categories: [Vision-and-Language, Transformer]
---
[[pdf]](https://arxiv.org/pdf/2208.10442.pdf)  &emsp;
[[github]](https://github.com/microsoft/unilm/tree/master/beit) <br>

**Wenhui Wang, Hangbo Bao, Li Dong∗, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, Furu Wei**
<br>Mcirosoft Corporation

![image](https://user-images.githubusercontent.com/42200027/188531427-783fbf18-35b0-41f7-b9c1-2fc00162347e.png)
![image](https://user-images.githubusercontent.com/42200027/188532460-761fded0-75fe-464c-9d29-45d17ced21de.png)

# Abstract 

- Vision task 와 Vision-and-Language task 에서 State-of-the-Art 를 달성한 general-purpose model **BEIT-3** 을 소개한다.
- 논문에서 소개되는 General-purpose 를 위한 Multi-way Transformer 속의 modular arcitecture 가 deep fusion 과 modality-specific encoding 을 가능케 한다.
- Masked "language" modeling 을 image 에 적용한 **Imglish** 방법과 text, image-text pair 에 적용한 unified 방법으로 pretraining 함으로써, object detection(COCO), semantic segmentation(ADE20K), image classification(ImageNet), visual reasoning (NVLR2), visual question answering(VQAv2), Image captioning(COCO), 그리고 cross-modal retrieval(Flickr30K, COCO) 에서 모두 state-of-the-art 를 달성하였다.

# Introduction : The Big Convergence

최근 Language([BERT](https://arxiv.org/pdf/1810.04805.pdf)), Vision([BEIT](https://arxiv.org/pdf/2106.08254.pdf), [BEITv2](https://arxiv.org/pdf/2208.06366.pdf)), 그리고 Multimodal([VLMO](https://arxiv.org/pdf/2111.02358.pdf), [CLIP](https://arxiv.org/pdf/2103.00020.pdf), [Coca](https://arxiv.org/pdf/2205.01917.pdf)) 등의 강력한 Transformer 모델이 각 연구의 trend 를 이룬다.

그 중 Vision-and-Language task 에서는 세 가지 ***pretraining convergence trend*** 가 있다.

첫째로, Transformer 모델의 성공이 language 로 부터 vision, 그리고 multimodal 로 퍼지고 있다는 점이다. 그러나 Vision-and-Language 의 경우, downstream task 에 맞춰 Transformer 모델이 다른데, 직접 end-task format 을 Transformer의 구조에 맞춰줘야 한다는 단점이 있고, 또 paramtere 들이 modality 들을 잘 공유하지 못한다는 점이 있다. 이에 본 논문에서는 **[Multiway Transformers(BEIT)](https://arxiv.org/pdf/2106.08254.pdf)** 를 차용하여 
<span style='background-color: #dcffe4'> 하나의 통합된 모델이 다양한 donwstream task 를 푸는 </span> general-purpose 모델을 제안한다.

둘째로, Masked modeling 방법이 여러 모달리티에서도 성공을 거둔다는 점이다. 그러나 Pretraining task 를 위시한 masked modeling 방법에 대하여, 기존의 vision-and-language transformer 들은 image-text matching 같은 multitask 를 배우는데, 이러한 multitask pretraining 방법은 scaling-up 에 적합하지 않다. 따라서, 본 논문에서는 
<span style='color:green;font-weight:bold'> mask-then-predict </span> 의 간단한 방법을 통해 통합하였는데, 이는 image 를 ***Imglish*** 라는 하나의 foreign language 로 생각하여 [BERT](https://arxiv.org/pdf/1810.04805.pdf) 의 MLM(Masked Language Modeling) 과 같은 방식만 사용한다.

셋째로, model size 와 data size 를 키우는 것이 generalization quality 에 도움이 된다는 점이다. 본 논문에서는 이를 따라 수십억개(Billions)의 parameter 로 scaling-up 하였고, private data 없이 in-house data 만으로 큰 margin 으로 state-of-the-art 를 달성하였다.

![image](https://user-images.githubusercontent.com/42200027/188538934-5e22cb2f-d45f-41bd-9b8e-661ffa8b83f5.png)
본 논문에서는 위와 같이 Multiway Transformer 모델을 차용하는데, 앞서 언급한 것과 같이 text token 과 image patch 를 mask 한 후, predict 하는 self-supervised learning 방법만 이용한다. 첫 번째 그림과 표와 같이 본 논문에서 제시하는 <span style='color:green;font-weight:bold'> BEIT-3 </span> 모델이 많은 vision task 와 vision-and-language task 에서 state-of-the-art 를 달성하였다. 

# BEIT-3 : A General-Purpose Multimodal Foundation Model

<span style='color:green;font-weight:bold'> Backbone Network : Multiway Transformers </span> 


Backbone Architecture 로는 Multiway Transformers([VLMO](https://arxiv.org/pdf/2111.02358.pdf))를 활용하였다. 그림에서 보듯이 shared self-attention 이 modality 들의 alignment 와 deep fusion 을 한 이후, 각 모달리티 별 expert network 가 학습된다. 본 연구에서는 vision, text, vision-and-text 의 3-way transformers 가 활용된다. 이 기본 backbone architecture 를 바탕으로 아래의 그림처럼 각 downstream task 에 맞게 BEIT-3 모델이 구성된다.

![image](https://user-images.githubusercontent.com/42200027/189814164-41c2e0fd-2232-48bb-952d-b3a62ff1a101.png)

<span style='color:green;font-weight:bold'> Pretraining Task : Masked Data Modeling </span> 

Prtining task 로는 masked data modeling([VL-BEIT](https://arxiv.org/pdf/2206.01127.pdf)) 를 활용한다. 이는 BERT 와 마찬가지 방법으로, word token 과 image patch 를 masking 한 후, predict 하는 방법으로 이 unifed mask-then-predict 방법이 modality 간의 alignment 의 학습에 도움이 된다. 
또, pretraining task 로 오로지 이 방법 하나만을 사용함으로써 scaling-up 에 친화적이다. 기존의 vision-and-language model 들은 multiple pretraining task 를 활용하여, training process scaling-up 에 좋지 않으며, mask-then-predict 만 사용했을 때, 적은 배치 사이즈로도 학습이 잘 되는 것을 확인하였다.
Model Spec 과 Pretraining Data는 아래와 같다. 

![image](https://user-images.githubusercontent.com/42200027/189816317-8cfbec27-b9da-4b89-a45b-934101d6f978.png)

# Experiments on Vision and Vision-and-Language Tasks

앞서 언급했듯, BEIT-3 는 여러 Vision task 와 Vision-and-Language task 에 State-of-the-Art 를 달성하였다.

<span style='color:green;font-weight:bold'> (1) Vision-and-Language Downstream Tasks </span> <br>
Visual Question Answering(VQA) / Visual Reasoning / Image Captioning <br>
![image](https://user-images.githubusercontent.com/42200027/189816702-cca5320d-c1cb-419e-bc24-3bf63973515c.png)

Image-Text Retrieval / Zero-shot Image-Text Retrieval <br>
![image](https://user-images.githubusercontent.com/42200027/189817305-0a78a22d-d22c-45b0-914b-9ab3ae81fb4c.png)

<span style='color:green;font-weight:bold'> (2) Vision Downstream Tasks </span> <br>
Object Detection and Instance Segmentation
![image](https://user-images.githubusercontent.com/42200027/189817577-3840f151-7ca8-471d-9c77-cc1b38889386.png)

Semantic Segmentation<br>
![image](https://user-images.githubusercontent.com/42200027/189817609-ee6ca56b-5807-41fb-8d23-5020e4c2b68f.png)

Image Classification<br>
![image](https://user-images.githubusercontent.com/42200027/189817675-6430543a-ddfb-41e6-8767-b5125f17f8e5.png)

# Discussion & Comments
결론의 문구에서, MultiLingual 로 확장하고 Audio 로 모달리티를 확장한 BEIT-3 에 대한 Future work 을 준비 중인 것 같다. (<span style='background-color: #dcffe4'> "For future work, we are working on pretraining multilingual BEIT-3 and including more modalities (e.g., audio) in BEIT-3 to facilitate the cross-lingual and cross-modality transfer, and advance the big convergence of large-scale pretraining across tasks, languages, and modalities"</span>) 
간단하지만 강력한 성능을 보이는 Transformers 모델이 Vision-and-Language 에서도 확장이 되고, 초거대 Language Model 들의 방법론들이 차례로 Vision-and-Language task 에 적용이 되고 있다는 느낌이 든다. 
