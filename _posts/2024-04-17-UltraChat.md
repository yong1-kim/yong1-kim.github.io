---
layout: post
title: "[EMNLP2023] Enhancing Chat Language Models by Scaling High-quality Instructional Conversations"
date: 2024-04-17 13:20:00 +0900
use_math: true
categories: [LLM, PLM]
---

[[pdf]](https://aclanthology.org/2023.emnlp-main.183.pdf) &emsp;
[[github]](https://github.com/thunlp/UltraChat)

**Ning Ding<sup>1∗</sup>, Yulin Chen<sup>2,3∗</sup>, Bokai Xu<sup>4</sup>, Yujia Qin<sup>2,3</sup>, Shengding Hu<sup>2,3</sup>, Zhiyuan Liu<sup>2,3†</sup>, Maosong Sun<sup>2,3†</sup>, Bowen Zhou<sup>1†</sup>**
<br> <sup>1</sup> Department of Electronic Engineering, Tsinghua University, <sup>2</sup> Department of Computer Science and Technology, Tsinghua University, <sup>3</sup> BNRIST, IAI, Tsinghua University, <sup>4</sup> The Chinese University of Hong Kong, Shenzhen &emsp;

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/02a7fabe-3a4c-4303-8fdb-17b6ee11d6a3)

## Abstract
- (**Finetuning chat Language model**) ChatGPT 와 같은 chat language model 을 instruction data 를 통한 fine-tuning 하는 것은, diversity 와 quality 가 받춰줄 때 좋은 성능을 끌어올릴 수 있는 방법이다.
- (<span style='color:green;font-weight:bold'> UltraChat </span>) Human query 를 포함하지 않은 large-scale instructional conversation 을 담고 있는 UltraChat 을 제안한다. 이 데이터셋은 scale, average length, diversity, coherence 등에서 우수성을 보인다.
- (**UltraLM**) UltraChat 을 활용하여 LLaMA 를 finetuning 하여, UltraLM 을 만들었고, WizardLM 과 Vicuna 를 포함한 open-source model 보다 좋은 성능을 보인다.

## 1. Introduction

## 2. Related Work

## 3. Data Construction

## 4. Data Analysis

## 5. Experiments

## 6. Conclusion


<span style='color:green;font-weight:bold'> 초록색볼드체 </span>
<br>
<span style='background-color: #dcffe4'> 초록색배경 </span>
<span style='background-color: #ffdce0'> 빨간색배경 </span>

<span style='color:green;font-weight:bold'> ▶ </span>
<br>
