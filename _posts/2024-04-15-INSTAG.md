---
layout: post
title: "[ICLR2024] #INSTAG: INSTRUCTION TAGGING FOR ANALYZING SUPERVISED FINE-TUNING OF LARGE LANGUAGE MODELS"
date: 2024-04-15 13:20:00 +0900
use_math: true
categories: [LLM, PLM]
---

[[pdf]](https://openreview.net/attachment?id=pszewhybU9&name=pdf) &emsp;
[[github]](https://github.com/OFA-Sys/InsTag)

**Keming Lu∗& Hongyi Yuan∗& Zheng Yuan & Runji Lin & Junyang Lin & Chuanqi Tan & Chang Zhou & Jingren Zhou**
<br> Alibaba DAMO Academy &emsp;

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/822daeff-5c6b-4eff-8889-308c7a2ecc17)

## Abstract
- (<span style='background-color: #dcffe4'> Lack of diversity in instruction-following data </span>) LLM 을 supervised fine-tuning (SFT) 을 통해 instruction 을 학습시킬 수 있다. 이를 위해 좋은(good) instruction-following dataset 이 필요한데, 현재 diversity 와 complexity 의 측면에서 데이터가 희박하고 분석이 부족하다.
- (<span style='color:green;font-weight:bold'> INSTAG </span>) 이제 저자들은 INSTAG 라는 open-set instruction tagging method 를 제안한다. 이는 tag 를 통해 human instruction 의 semantic 과 intention 을 부여하여, instruction diversity 와 complexity 를 정량적으로 분석할 수 있게한다.
- (**Data sampling procedure**) INSTAG 의 diverse and complex instruction 을 통해 LLM 학습에 효과를 본 것을 토대로, data sampling procedure 를 통해 6K 개의 sample 을 선별한다.
- (**TAGLM**) INSTAG 를 학습한 모델인 TAGLM 이 MT-bench 에서 다른 open-source model 을 압도한다.

## 1. Introduction

<span style='color:green;font-weight:bold'> ▶ </span>
<br>

## 2. Related Works

## 3. INSTAG
# 3.1. OPEN-SET FINE-GRAINED TAGGING

# 3.2. TAG NORMALIZATION

# 3.3. QUALITY EVALUATION

# 3.4. PRELIMINARY ANALYSIS

## 4. INSTAG FOR DATA SELECTION
# 4.1. EXPERIMENTAL SETUP

# 4.2. RESULTS

# 4.3. DECOUPLED ANALYSIS

## 5. INSTAGGER: LOCAL TAGGER BY DISTILLATION

## 6. CONCLUSION
```
In this paper, we introduced INSTAG, an open-set tagging method leveraging the instructionfollowing ability of ChatGPT for SFT data analysis. We apply INSTAG on open-source SFT datasets, showing diverse and complex data leads to better alignment performance. We designed a complexity-first diverse sampling method to select 6K samples, and TAGLM fine-tuned on this selected dataset outperforms other open-source models aligned with considerably more data. Moreover, further decoupled analyses revealed that model performance increases with fine-tuning on more diverse and complex SFT data, respectively. In summary, our proposed INSTAG provides a novel aspect for a deeper understanding of query distribution in the alignment of LLMs. It has robust potential to be extended to more applications beyond the data selection shown in this work, such as creating comprehensive evaluations and tag-based self-instruct.
```





<span style='color:green;font-weight:bold'> 초록색볼드체 </span>
<br>
<span style='background-color: #dcffe4'> 초록색배경 </span>
<span style='background-color: #ffdce0'> 빨간색배경 </span>
