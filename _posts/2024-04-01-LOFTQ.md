---
layout: post
title: "[ICLR2024] LOFTQ: LORA-FINE-TUNING-AWARE QUANTIZATION FOR LARGE LANGUAGE MODELS"
date: 2024-04-01 10:00:00 +0900
use_math: true
categories: [LLM, PLM]
---

[[pdf]](https://openreview.net/pdf?id=LzPWWPAdY4) &emsp;
[[github]](https://github.com/yxli2123/LoftQ)

**Yixiao Li<sup>1∗</sup>, Yifan Yu<sup>1∗</sup>, Chen Liang<sup>1</sup>, Pengcheng He<sup>2</sup>, Nikos Karampatziakis<sup>2</sup>, Weizhu Chen<sup>2</sup>, Tuo Zhao<sup>1</sup>**
<br> <sup>∗</sup> Equal contribution, <sup>1</sup> Li, Yu, Liang, and Zhao are affiliated with Georgia Institute of Technology. Correspondence to yixiaoli@gatech.edu, yyu429@gatech.edu, and tourzhao@gatech.edu., <sup>2</sup> He, Karampatziakis, and Chen are affiliated with Microsoft Azure. &emsp;

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/ca298884-6f7e-4688-99c9-0c393beed8cc)

## Abstract




<span style='color:green;font-weight:bold'> 초록색볼드체 </span>
<br>
<span style='background-color: #dcffe4'> 초록색배경 </span>
<span style='background-color: #ffdce0'> 빨간색배경 </span>
<span style='color:green;font-weight:bold'> ▶ </span>
<br>
