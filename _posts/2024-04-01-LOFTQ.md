---
layout: post
title: "[ICLR2024] LOFTQ: LORA-FINE-TUNING-AWARE QUANTIZATION FOR LARGE LANGUAGE MODELS"
date: 2024-04-01 10:00:00 +0900
use_math: true
categories: [LLM, PLM]
---

[[pdf]](https://openreview.net/pdf?id=LzPWWPAdY4) &emsp;
[[github]](https://github.com/yxli2123/LoftQ)

**Yixiao Li<sup>1∗</sup>, Yifan Yu<sup>1∗</sup>, Chen Liang<sup>1</sup>, Pengcheng He<sup>2</sup>, Nikos Karampatziakis<sup>2</sup>, Weizhu Chen<sup>2</sup>, Tuo Zhao<sup>1</sup>**
<br> <sup>∗</sup> Equal contribution, <sup>1</sup> Li, Yu, Liang, and Zhao are affiliated with Georgia Institute of Technology. Correspondence to yixiaoli@gatech.edu, yyu429@gatech.edu, and tourzhao@gatech.edu., <sup>2</sup> He, Karampatziakis, and Chen are affiliated with Microsoft Azure. &emsp;

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/ca298884-6f7e-4688-99c9-0c393beed8cc)

## Abstract
- (**Quantization and LoRA**) LLM 을 Finetuning 하기 위해 필수불가결한 요소가 Quantization 이고, 최근 LoRA fine-tuning 기법을 통한 quantization 연구도 활발하다. 기존의 연구들은 quantization 과 LoRA 를 같이 적용하였을 때, full fine-tuning 과 비교하여 consistent gap 이 있음을 한계점으로 지적한다.
- (<span style='color:green;font-weight:bold'> LoftQ </span>) 이에 저자들은 LLM 을 quantize 하면서 동시에, LoRA fine-tuning 을 위한 proper low-rank initialization 을 찾는 LoRA-Fine-Tuning-aware Quatization, LoftQ 를 제안한다. 이 방법론은 full-precision model 과 quantized model 사이의 discrepancy 를 경감시켜 downstream task 에서의 generalization 성능을 향상ㅅ이킨다.
- (**Experiment**) NLU, QA, Summarization, NLG task 등에 적용하였을 때, 기존의 quantization method 보다 우수한 성능을 보이고, 특히 어려운 2-bit 이나 2/4-bit mixed precision regime 에서 강력한 성능을 보임을 확인한다.






<span style='color:green;font-weight:bold'> 초록색볼드체 </span>
<br>
<span style='background-color: #dcffe4'> 초록색배경 </span>
<span style='background-color: #ffdce0'> 빨간색배경 </span>
<span style='color:green;font-weight:bold'> ▶ </span>
<br>
