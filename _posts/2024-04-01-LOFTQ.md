---
layout: post
title: "[ICLR2024] LOFTQ: LORA-FINE-TUNING-AWARE QUANTIZATION FOR LARGE LANGUAGE MODELS"
date: 2024-04-01 10:00:00 +0900
use_math: true
categories: [LLM, PLM]
---

[[pdf]](https://openreview.net/pdf?id=LzPWWPAdY4) &emsp;
[[github]](https://github.com/yxli2123/LoftQ)

**Yixiao Li<sup>1∗</sup>, Yifan Yu<sup>1∗</sup>, Chen Liang<sup>1</sup>, Pengcheng He<sup>2</sup>, Nikos Karampatziakis<sup>2</sup>, Weizhu Chen<sup>2</sup>, Tuo Zhao<sup>1</sup>**
<br> <sup>∗</sup> Equal contribution, <sup>1</sup> Li, Yu, Liang, and Zhao are affiliated with Georgia Institute of Technology. Correspondence to yixiaoli@gatech.edu, yyu429@gatech.edu, and tourzhao@gatech.edu., <sup>2</sup> He, Karampatziakis, and Chen are affiliated with Microsoft Azure. &emsp;

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/ca298884-6f7e-4688-99c9-0c393beed8cc)

## Abstract
- (**Quantization and LoRA**) LLM 을 Finetuning 하기 위해 필수불가결한 요소가 Quantization 이고, 최근 LoRA fine-tuning 기법을 통한 quantization 연구도 활발하다. 기존의 연구들은 quantization 과 LoRA 를 같이 적용하였을 때, full fine-tuning 과 비교하여 consistent gap 이 있음을 한계점으로 지적한다.
- (<span style='color:green;font-weight:bold'> LoftQ </span>) 이에 저자들은 LLM 을 quantize 하면서 동시에, LoRA fine-tuning 을 위한 proper low-rank initialization 을 찾는 LoRA-Fine-Tuning-aware Quatization, LoftQ 를 제안한다. 이 방법론은 full-precision model 과 quantized model 사이의 discrepancy 를 경감시켜 downstream task 에서의 generalization 성능을 향상ㅅ이킨다.
- (**Experiment**) NLU, QA, Summarization, NLG task 등에 적용하였을 때, 기존의 quantization method 보다 우수한 성능을 보이고, 특히 어려운 2-bit 이나 2/4-bit mixed precision regime 에서 강력한 성능을 보임을 확인한다.

## 1. Introduction

## 2. Background
# 2.1. Transformer Models

# 2.2. Qunatization

## 3. METHOD
# 3.1. LoRA-Aware Quantization

# 3.2. Alternating optimization

# 3.3. Applying to LORA Fine-tuning

## 4. Experiments


# 4.1. Encoder-only Model : DEBERTa-v3

# 4.2. Encoder-Decdoer Model : BART

# 4.3. Decoder-only Model : LLaMA-2

# 4.4. Analysis

## 5. Discussion

## Conclusion
```
We propose LoftQ, a quantization framework for LLMs, which alternatively applies quantization and low-rank approximation to the original high-precision pre-trained weights, to obtain an initialization for the subsequent LoRA fine-tuning. Experiments on natural language understanding, question answering, summarization, and natural language generation show that our framework remarkably surpasses existing methods, e.g., QLoRA, for quantizing encoder-only, encoder-decoder, and decoder-only models. We have not observed our method exhibiting worse performance over QLoRA. Moreover, our quantization framework demonstrates effectiveness and robustness particularly in low-bit quantization regimes, e.g., the 2-bit level.
```


<span style='color:green;font-weight:bold'> 초록색볼드체 </span>
<br>
<span style='background-color: #dcffe4'> 초록색배경 </span>
<span style='background-color: #ffdce0'> 빨간색배경 </span>
<span style='color:green;font-weight:bold'> ▶ </span>
<br>
