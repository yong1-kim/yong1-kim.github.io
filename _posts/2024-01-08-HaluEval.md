---
layout: post
title:  "[EMNLP2023] HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models"
date:   2024-01-08 11:00:00 +0900
use_math: true
categories: [PLM, LLM]
---

[[pdf]](https://proceedings.mlr.press/v202/lee23n/lee23n.pdf)
[[github]](https://github.com/lgresearch/QASA)

**Junyi Li <sup>1,3,4*</sup>, Xiaoxue Cheng <sup>1*</sup>, Wayne Xin Zhao <sup>1,4†</sup>, Jian-Yun Nie <sup>3</sup>, and Ji-Rong Wen <sup>1,2,4</sup>**
<br><sup>1</sup> Gaoling School of Artificial Intelligence, Renmin University of China <sup>2</sup> School of Information, Renmin University of China <sup>3</sup> DIRO, Université de Montréal
 &emsp;

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/a4d70239-9fa8-446c-90f3-f0d62c16eb81)

# Abstract
- (**Hallucination**) ChatGPT 와 같은 Large Language Model (LLM) 은 soruce 와 대치되거나, factual knowledge 를 확인할 수 없는 <span style='color:green;font-weight:bold'> hallucination </span> 이 발생한다.
- (**HaluEval**) Hallucination 의 *what types of content* 와 *to which extent* 을 측정하기 위해, hallucination recognize 하는 LLM 의 능력을 평가하는 large hallucinated sample 인 <span style='color:green;font-weight:bold'>  HaleuEval benchmark </span>  를 만들었다.
- (**Challenges**) ChatGPT 와 LLM 들이 hallucination recognizing 에 great challenge 가 있음을 보이며, external knowledge 를 제공하거나 addtional reasoning step 을 추가하는 것이 hallucination 을 줄일 수 있음을 보인다.

## 1. Introduction 

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/e3463584-61a8-404a-82a2-0ee31186dabe)


Large Language Model 의 prominent capability 이면에 hallucination 문제가 존재함은 공공연한 사실이다.
Hallucination 은 soruce 와 대치되거나, factual knowledge 를 확인할 수 없는 content 를 생성하는 것을 의미한다.
몇몇의 연구([[1]](https://aclanthology.org/2022.acl-long.236/),[[2]](https://arxiv.org/abs/2304.10513),[[3]](https://arxiv.org/abs/2301.04449)) 에서 small LM 에 대한 hallucination 원인을 조사하기 위한 연구가 있었지만, <span style='color:green;font-weight:bold'> what types of content and to which extent LLMs tend to hallucinate </span> 에 대한 연구는 미흡하다.

이를 위해 이 논문에서는 **Hal**l**u**cination **Eval**uation (**HaluEval**) benchmark 를 소개한다.
HaluEval 은 35,000 개의 hallucinated/normal sample 로 이뤄져있고, 이 중 5,000 개는 *general* user query 에 대한 chatGPT 의 response, 그리고 30,000 개는 (1) question answering, (2) knowledge-grounded dialogue, (3) text summarization 에 걸친  *task-specific* sample 이다.

위의 Figure 에 construction pipeline 을 볼 수 있다.
<span style='background-color: #dcffe4'> 우선, general user query 에 대하여(Figure bottom), </span> Alpca 의 instruction tuning dataset 에서, 5,000개의 query 를 추출한다. 
LLM 이 hallucination 을 더 잘 생성하게 하기 위해, chatGPT 에 query 에 대한 3 개의 response 를 생성하게 한 후, 이 3개의 response 의 simiarilty 가 가장 낮은 5,000개의 query 만을 사용한다. 
이러한 것은 최근 [SelfcheckGPT](https://arxiv.org/abs/2303.08896) 에서 LLM 의 conflicting and diverged response 에서 hallucination 이 나타날 확률이 높다는 발견에 기반한다.
이후 Human annotator 로 하여금, hallucinated info 가 있는지, 그리고 있다면 corresponding span 을 mark 하도록 한다.

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/ce589f75-08db-44f7-b03c-1afd8fe5f797)

위의 Table 의 예시에서, human annotator 초록색으로 hallucinated span 을 marking 한 것을 볼 수 있다. 
이 human-annotated query-response 를 ㅗㅌㅇ해 LLM 이 어떠한 type 의 content 를 hallucinate 하는지 분석할 수 있다.

<span style='background-color: #dcffe4'> 그 다음, task-specific sample 에 대하여(Figure top), </span> two-stage approach 가 사용된다.
첫 step 으로 existing task (e.g. HotpotQA) 에 대하여, ChatGPT 로 하여금, one-pass syle 과 conversational style 로 hallucinated sample 을 생성하게 한다.
두 style 로 나누는 것은 hallucinated sample 의 다양성을 위해서다.
두번째 step 으로, 가장 plausible 하고 difficult 한 hallucinated sample 을 고르기 위하여, ground-trtuth example 을 통해 filtering instruction 을 elaborate 하여, ChatGPT 로 하여금 sample 을 고르게한다.
이 **Sample-then-Filtering** 기법을 통해, specific task example 의 hallucinated counterpart 를 생성할 수 있다.

<span style='background-color: #dcffe4'> HaluEval benchmark 를 활용한 실험</span> 을 통해 아래 세 가지 특징을 발견한다. 
- ChatGPT 는 unverifable information 를 날조하는 경향이 강하다.
- LLM 들은 hallucination 을 알아차리는 것이 매우 어려우며, 특히 sample generation 에 사용된 ChatGPT 역시 그러하다.
- LLM 의 부족한 hallucination recognizing 능력은 explicit knowledge 의 제공과, intermediate reasoning step 의 추가로 발전시킬 수 있다. Hallucinated sample 에 대한 contrastive learning 은 오히려 LLM 으로 하여금 더 confuse 하게 만들어, worse performance 를 보이게 한다.

## 2. The HaluEval Benchmark





<span style='color:green;font-weight:bold'> 초록색볼드체 </span>
<span style='background-color: #dcffe4'> 초록색배경 </span>
