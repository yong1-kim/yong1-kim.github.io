---
layout: post
title:  "[ACL2023] ReAugKD: Retrieval-Augmented Knowledge Distillation For Pre-trained Language Models"
date:   2024-03-18 17:00:00 +0900
use_math: true
categories: [Retrieval, LLM, PLM]
---

[[pdf]](https://aclanthology.org/2023.acl-short.97.pdf)  &emsp;
[[github]](https://github.com/gabriben/awesome-generative-information-retrieval)

**Jianyi Zhang<sup>1</sup>, Aashiq Muhamed<sup>2</sup>, Aditya Anantharaman<sup>2</sup>, Guoyin Wang<sup>2</sup>, Changyou Chen<sup>3</sup>, Kai Zhong<sup>2</sup>, Qingjun Cui<sup>2</sup>, Yi Xu<sup>2</sup>, Belinda Zeng<sup>2</sup>, Trishul Chilimbi<sup>2</sup>, Yiran Chen<sup>1</sup>**
<br><sup>1</sup> Duke University, <sup>2</sup> Amazon <sup>3</sup> University at Buffalo, SUNY

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/6751ad19-b493-4ca7-8724-77364c1ad54e)

# Abstract
- (**Knowledge Distillation**) Large-scale pre-trained LM 을 작은 모델로 distillation 하는 연구가 성행하고 있다. 기존의 KD 접근 방식은 teacher model 의 soft label 과 intermediate activation 을 trasnfer learning 하여 student model 을 학습시킨다.
- (<span style='color:green;font-weight:bold'> ReAugKD </span>) 이 논문에서는 teacher 의 soft model 에 더불어 **kowledge base** 형태의 non-parametric memory 를 같이 활용할 경우 더 좋은 generalization 성능을 보이는 distillation 방법을 제안한다. Student 모델로 하여금 knowledge base 를 효과적으로 retrieve 하는 이 ReAugKD framework 은 teacher 와 student embedding space 에서 relational knowledge 를 align 하는 loss 로 학습한다.
- (**Experiment**) 실험 결과, GLUE benchmark 에 대해 State-of-the-Art 성능을 보인다.

## 1. Introduction

## 2. Methodology
# 2.1. Training Phase

# 2.2. Loss Function

# 2.3. Inference Phase

## 3. Experimental Results

## Conclusion
```
In this paper, we present ReAugKD, a knowledge distillation framework with a retrieval mechanism that shows state-of-the-art performance on the GLUE benchmark. In the future, we plan to expand the knowledge base with more information from the teacher and extend it to additional tasks.
```

## Limitations
```
Our method relies on having access to teacher embeddings and prediction which may not always be possible in a black-box distillation setting. Retrieval augmentation also requires maintaining a knowledge base that is memory intensive.
The cost of the retrieval process is dependent on the size of the training corpus, which can be a limitation when dealing with very large training datasets.
Conducting dataset distillation (Wang et al., 2018b) on the training corpus to further reduce memory cost and retrieval time is an important future step for our framework.
```




<span style='color:green;font-weight:bold'> 초록색볼드체 </span>
<br>

<span style='background-color: #dcffe4'> 초록색배경 </span>
<span style='background-color: #ffdce0'> 빨간색배경 </span>
