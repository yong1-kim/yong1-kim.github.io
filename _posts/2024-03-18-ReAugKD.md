---
layout: post
title:  "[ACL2023] ReAugKD: Retrieval-Augmented Knowledge Distillation For Pre-trained Language Models"
date:   2024-03-18 17:00:00 +0900
use_math: true
categories: [Retrieval, LLM, PLM]
---

[[pdf]](https://aclanthology.org/2023.acl-short.97.pdf)  &emsp;
[[github]](https://github.com/gabriben/awesome-generative-information-retrieval)

**Jianyi Zhang<sup>1</sup>, Aashiq Muhamed<sup>2</sup>, Aditya Anantharaman<sup>2</sup>, Guoyin Wang<sup>2</sup>, Changyou Chen<sup>3</sup>, Kai Zhong<sup>2</sup>, Qingjun Cui<sup>2</sup>, Yi Xu<sup>2</sup>, Belinda Zeng<sup>2</sup>, Trishul Chilimbi<sup>2</sup>, Yiran Chen<sup>1</sup>**
<br><sup>1</sup> Duke University, <sup>2</sup> Amazon <sup>3</sup> University at Buffalo, SUNY

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/6751ad19-b493-4ca7-8724-77364c1ad54e)

# Abstract
- (**Knowledge Distillation**) Large-scale pre-trained LM 을 작은 모델로 distillation 하는 연구가 성행하고 있다. 기존의 KD 접근 방식은 teacher model 의 soft label 과 intermediate activation 을 trasnfer learning 하여 student model 을 학습시킨다.
- (<span style='color:green;font-weight:bold'> ReAugKD </span>) 이 논문에서는 teacher 의 soft model 에 더불어 **kowledge base** 형태의 non-parametric memory 를 같이 활용할 경우 더 좋은 generalization 성능을 보이는 distillation 방법을 제안한다. Student 모델로 하여금 knowledge base 를 효과적으로 retrieve 하는 이 ReAugKD framework 은 teacher 와 student embedding space 에서 relational knowledge 를 align 하는 loss 로 학습한다.
- (**Experiment**) 실험 결과, GLUE benchmark 에 대해 State-of-the-Art 성능을 보인다.

## 1. Introduction

<span style='color:green;font-weight:bold'> ▶ Knowledge Distillation (KD) </span>
<br>
BERT, RoBERTa, Electra 등의 LM 이 좋은 성능을 보이지만, 이 것들은 M ~ B 단위의 param 을 가지고 있어 제한된 환경에서 가동이 힘들다.
이에 성능 좋은 위의 모델들은 teacher model 로 하고, param 수가 더 적은 student model 로 지식을 전달하는 knowledge distillation (KD) 연구가 활발하다.
기존의 KD 모델들은 typically student param 속의 지식과 teacher 의 output prediction 의 divergence 를 최소화 하는 방식으로 학습한다.
<span style='background-color: #ffdce0'> 이러한 단순한 KD 방법은 student 모델의 작은 param 떄문에 어느정도 한계점이 있다. 특히 LLM 에서 많이 나타나는 task-specific knowledge 를 distill 하여 학습하기는 힘들다. </span>

<span style='color:green;font-weight:bold'> ▶ Retrieval-Augmented Knowledge Distillation (ReAugKD) </span>
<br>
저자들은 이 문제를 해결하기 위해 *Retrieval-Augmented Knowledge Distillation (ReAugKD)* 방법론을 제안한다.
<span style='background-color: #dcffe4'> ReAugKD 방법은 implicit parametric memory 에 더하여 non-parametric external memory 를 가져와서, kNN retrieval 을 통해 retrieve 를 한다. </span>
Key intuition 은 teacher 의 task-specific knowledge 로부터 가져올 수 있는 external memory 를 studnet 모델이 활용할 수 있는 능력을 갖추게 하는 것이다.

<span style='color:green;font-weight:bold'> ▶ Experiment </span>
<br>
실험 결과, GLUE benchmark 에서 State-of-the-Art 를 달성했으며, retrieval 을 하지 않은 방법보다 단 3% 의 latency overhead 만 존재함을 보인다.
또한, ReAugKD 방식을 통한 학습이 student model 의 generalization 성능 향상을 이끄는 것을 확인한다.

## 2. Methodology
# 2.1. Training Phase

# 2.2. Loss Function

# 2.3. Inference Phase

## 3. Experimental Results

## Conclusion
```
In this paper, we present ReAugKD, a knowledge distillation framework with a retrieval mechanism that shows state-of-the-art performance on the GLUE benchmark. In the future, we plan to expand the knowledge base with more information from the teacher and extend it to additional tasks.
```

## Limitations
```
Our method relies on having access to teacher embeddings and prediction which may not always be possible in a black-box distillation setting. Retrieval augmentation also requires maintaining a knowledge base that is memory intensive.
The cost of the retrieval process is dependent on the size of the training corpus, which can be a limitation when dealing with very large training datasets.
Conducting dataset distillation (Wang et al., 2018b) on the training corpus to further reduce memory cost and retrieval time is an important future step for our framework.
```




<span style='color:green;font-weight:bold'> 초록색볼드체 </span>
<br>

<span style='background-color: #dcffe4'> 초록색배경 </span>
<span style='background-color: #ffdce0'> 빨간색배경 </span>
