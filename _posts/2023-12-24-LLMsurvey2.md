---
layout: post
title:  "A Survey of Large Language Models (2)"
date:   2023-12-24 11:45:00 +0900
use_math: true
categories: [Transformer, PLM, LLM]
---

[[pdf]](https://arxiv.org/pdf/2303.18223.pdf)
[[github]](https://github.com/RUCAIBox/LLMSurvey)

**Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie and Ji-Rong Wen**

<span style='background-color: #dcffe4'> [A Survey of Large Language Models (1)](https://yong1-kim.github.io/transformer/plm/llm/2023/12/24/LLMsurvey1.html) 에 이어서... </span>

## 4. Pre-training

LLM 을 pretrain 하는데는 효율적인 알고리즘, model architecture, optimization technique 등이 모두 중요하다.
이번 섹션에서는 LLM 을 pretrain 하기 위한 세 가지 요소인 **_(1) data collection, (2) model architecture, (3) training technique_** 를 각각 살펴본다.

# 4.1. Data Collection and Preparation

LLM 을 학습하기 위해 높은 퀄리티의 dataset 을 확보하는 것은 매우 중요하다. 이번 section 에서는 _data source, preprocessing methods, 그리고 pre-training data 가 LLM 에 미치는 영향_ 의 세 가지 측면을 살펴본다.

<span style='background-color: #dcffe4'> (1) Data Source </span>

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/032b874d-64ac-4b15-97be-5da2c45a6e15)

대부분의 LLM 은 위에 보이는 그림처럼 여러 data source 의 mixture 를 pretraining dataset 으로 활용한다.


<span style='background-color: #dcffe4'> 초록색 배경 </span>

<span style='color:green;font-weight:bold'> 초록색 볼드체 </span>

<span style='background-color: #dcffe4'> [A Survey of Large Language Models (3)](https://yong1-kim.github.io/transformer/plm/llm/2023/12/24/LLMsurvey3.html) 에서 계속... </span>
