---
layout: post
title:  "A Survey of Large Language Models (2)"
date:   2023-12-24 11:45:00 +0900
use_math: true
categories: [Transformer, PLM, LLM]
---

[[pdf]](https://arxiv.org/pdf/2303.18223.pdf)
[[github]](https://github.com/RUCAIBox/LLMSurvey)

**Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie and Ji-Rong Wen**

<span style='background-color: #dcffe4'> [A Survey of Large Language Models (1)](https://yong1-kim.github.io/transformer/plm/llm/2023/12/24/LLMsurvey1.html) 에 이어서... </span>

## 4. Pre-training

LLM 을 pretrain 하는데는 효율적인 알고리즘, model architecture, optimization technique 등이 모두 중요하다.
이번 섹션에서는 LLM 을 pretrain 하기 위한 세 가지 요소인 **_(1) data collection, (2) model architecture, (3) training technique_** 를 각각 살펴본다.

# 4.1. Data Collection and Preparation

LLM 을 학습하기 위해 높은 퀄리티의 dataset 을 확보하는 것은 매우 중요하다. 이번 section 에서는 _data source, preprocessing methods, 그리고 pre-training data 가 LLM 에 미치는 영향_ 의 세 가지 측면을 살펴본다.

<span style='color:green;font-weight:bold'> (1) Data Source </span>

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/032b874d-64ac-4b15-97be-5da2c45a6e15)

대부분의 LLM 은 위에 보이는 그림처럼 여러 data source 의 mixture 를 pretraining dataset 으로 활용한다.
Dataset 들은 크게 두 가지로 나눌 수 있는데, 하나는 **general text data**, 다른 하나는  **specialized text data** 이다.
**General data** 는 대부분의 LLM 에서 활용하는 dataset 으로 webpage, books, converational text 등이 속하며, 크기가 크고 (large) 다양하며 (diverse), 접근이 용이하기 때문에, generalization ability 를 높이기 위해 필요하다.
**Specialized data** 에는 multilingual data, scientific data, code 와 같은 특정한 task-solving capability 를 부여하기 위해 사용된다.

※ 각 dataset 에 대한 자세한 설명은 논문 참조.

<span style='color:green;font-weight:bold'> (2) Data Preprocessing </span>

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/c03bcc64-4cac-481f-b140-1f0a44968174)

Pre-training corpus 를 준비한 이후에는 noisy, redundant, irrelevant, toxic data 를 제거하는 전처리가 필수적이다. 
최근 [Data-Juicer](https://arxiv.org/pdf/2309.02033.pdf) 라는 여러 전처리 방법을 담고 있는전처리 tool 이 release 되었다. 
일반적인 전처리 pipeline 은 위의 그림과 같다.

- **Quality Filtering**

Quality filtering 에는 일반적으로, classifier-based 방법과 heuristic-based 방법이 있는데, 기존에 많이 사용하던 classifier-based 방법은 방언이나 구어체 등을 제거할 확률이 높아 bias 를 높이는 경향이 있다. 따라서 최근 BLOOM 이나 GOPHER 등에서는 heuristic 방법을 쓰는데, 그 종류에는 *Language based filtering, Metric based filtering, Statistic based filtering, Kyeowrd based Filtering* 등이 있다.

- **De-duplication**

[최근 한 연구](https://arxiv.org/abs/2205.10487)에서, 문장이 반복되는 duplication 문제가 training 을 unstable 하게 만들고 성능을 떨어뜨린다는 주장을 하였다. 이에 repeated word 를 가지는 low quality 문장을 제거하고 (sentence-level), n-gram 등을 기반으로 너무 많이 겹치는 documnet 를 제거하며 (document-level), dataset contimination 문제 해결을 위해 training set 과 eval set 의 overlap 을 해결한다 (set-level).

- **Privacy Reduction**

흔히, PII 라고 부르는 _personally identifiable information_ 를 pretraining corpus 에서 제거해야 한다. 한 가지 방법으로는 rule-based 로 name, address, phone number 등을 지우는 것이다.

- **Tokenization**

이제 Tokenization 을 진행하면 된다. 최근에는 subword-level 기반의 tokenization 이 주로 사용되고, byte pair encoding (BPE), Wordpiece tokenization, unigram tokenization 등이 사용된다. 
BPE 는 multilingual setting 에서 장점을 보이며, GPT-2, BART, LLaMA 등에서 사용한다.
Wordpiece 는 Google 의 subword tokenization 알고리즘으로, 처음에는 voice search system 을 위해 고안되었으나, 이후 MT 모델, 그리고 BERT 에서 사용되었다. Wordpiece 는 BPE 와 기본적으로 유사한 방법이지만, merge 하는 방법에서 조금의 차이점을 보인다. 
마지막으로 Unigram tokenization 은 EM 알고리즘의 일종으로, old LM 을 활용하여 큰 vocab 에서 하나씩 제거해 나가며 dictionary 를 완성한 후, 다시 re-estimate 하여 vocab 을 만들고를 반복한다. T5, mBART 등에서 사용되었다.

OPT 와 GPT-3 가 GPT-2 tokenizer 를 사용한 것처럼, 기존에 있는 tokenizer 를 사용하는 것도 좋은 방법 중에 하나이지만, <span style='background-color: #dcffe4'> 모델이 학습하는 pre-training corpus 에 맞춰 specially designed tokenization 기법을 적용하는 것은 큰 도움이 된다. </span>
따라서, 최근에는 BPE 와 unigram 기법을 합친 Sentence Piece library 를 활용하는 등 **customized tokenizer** 를 활용하는 경향성이 높다. 단, transfer learning 을 할 때 이러한 customized tokenizer 는 조심해야한다. LLaMA 의 경우, pretraining 시에 BPE 를 활용하기 때문에, non-english dataset 에 대해서는 fine-tuning 에 어려움이 있을 수 있다.

<span style='color:green;font-weight:bold'> (3) Data Scheduling </span>

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/a4bba40f-b52c-47e3-b470-d565ce12a1aa)

Data scheduling 에는 두 가지가 중요하다 : **data mixture, data curriculum**.
- **Data mixutre**

Data 를 섞을 때는 proportion 이 중요하다. 보통 upsampling, downampling 기법등을 이용한다. 
최근 여러 연구에서 하나의 domain 의 data 를 너무 많이 배우는 것은 좋지 못한 성능을 낸다는 것을 검증하였다.
또, 몇몇의 연구에서는 heuristic 하게 proportion 을 결정하지 않고, model 을 활용하여 optimize 하는 방법을 제안하였다. 간단한 예로, downstream task 에 맞춰 그 task 에 맞는 pretraining corpus 의 비율을 증가시키는 것들이 있으나, 실용적이지는 못하다. 

- **Data curriculum**

Basic skill 을 배운 이후 traget skill 을 배우는 것이 효과적이라는 것이 몇몇 연구([[1]](https://arxiv.org/abs/2307.14430),[[2]](https://arxiv.org/abs/2308.12950))에서 검증되었다. 이에 따라 dataset 을 pretraining 할 때, 어떠한 것을 먼저 배울지 그 curriculum 을 정하는 것도 중요하다.
보통 target skill 은 <span style='background-color: #dcffe4'> coding, Mathematics, Long context modeling </span> 세 가지에 대해 curriculum 을 많이 적용한다.


# 4.2. Architecture

이 섹션에서는 LLM 의 아키텍쳐 디자인 : mainstream architecture, pre-training objective, detail configuration 등을 살펴본다.

<span style='color:green;font-weight:bold'> (1) Typical Architectures </span>
<br>


![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/1e13623a-d8e8-4f83-937d-3aacefc143b5)

LLM 의 backbone 은 Transformer 가 de-facto architecture 이다.
보통 크게 세 가지 major type 으로 나눈다 : encoder-decoder 구조, causal decoder 구조, prefix decoder 구조.

- **Encdoer-decoder Architecture** : T5, BART 등
- **Causal Decoder Architecture** : GPT-Series, OPT, BLOOM, Gopher 등 대부분의 LLM 들
- **Prefix Decoder Architecture** : U-PaLM, GLM-130B 등

<span style='color:green;font-weight:bold'> (2) Detailed Configuration </span>
<br>

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/51ccd0ed-ad88-4322-a3b9-1a2dc18acdf9)


대부분 LLM 의 기반인 Transformer 의 네 가지 configuration 인 (1)**Normalization method**, (2) **Normalization position**, (3)**Activation Functions**, (4) **Position embeddings** 를 다룬다.

추가적으로, Attention mechanism 에 대해서는 (1) Full attention, (2) Sparse attention, (3) Multi-query/grouped-query attention, (4) FlahsAttention, (5) PagedAttention 등을 다룬다. 

※ 각 configuration 및 method 에 대한 자세한 설명은 논문 참조.

<span style='color:green;font-weight:bold'> (3) Pre-training Tasks </span>
<br>
LLM 은 대부분 **Langague Modeling** 과 **Denoising Autoencoding** 을 학습한다.
※ 관련 내용은 너무 유명하므로 생략, 논문 참조.

<span style='color:green;font-weight:bold'> (4) Long Context Modeling </span>
<br>
최근 PDF proecssing 이나 story writing 과 같은 <span style='background-color: #dcffe4'> long context modeling capacity </span> 를 increasing 하기 위한 요구가 많다.
GPT-4 는 128K context window 를 지원하고, Claude 2.1 (Anthropic 社) 은 200K context window 를 활용한다.
Long context modeling 능력을 위해서는 대표적으로 두 가지 기법이 활용된다.

- **Scaling Position Embeddings**

T5 Bias, ALiBi, xPos, NoPE 같은 position embedding 기법들 대부분이 maximum training length 안에서의 학습만으로 충분한 generalization 효과를 본다. 이를 <span style='background-color: #dcffe4'> extrapolation capability </span> 라고 하는데, mainstream position embedding 중 하나인 Rotary Position Embedding (RoPE) 의 경우, 이 extrapolation capa 가 없다. 이에 아래 방법들을 통해 RoPE 를 longer text 에 scale 할 수 있다.

1) Direct model fine-tuning : LLM 을 단순하게 더 긴 text 에 fine-tuning 하는 방법이다. 보통 multi-stage approach (e.g. 2K->8K-> 32K) 를 활용한다. 매우 느리다는 단점이 있다.
   
2) Position interpolation : Long context 의 position index 들을 downcale 하여, original context window 크기로 맞추는 방법이다. 단순히 position index 들에 L/L' (original context length L, target context lenth L') 을 곱해주는데, 실험 결과 효과적으로, 그리고 효율적으로 Long context 로 extend 할 수 있지만, 짧은 텍스트에 오히려 adverse impact 가 있다.
   
3) Position truncation : out-of-distribution rotation angle 문제를 해결하기 위해, long context 의 longer relative position 을 truncate 해버리는 방법이다. ReRoPE 나 LeakyReRoPe 에서는 pre-difeined window length 를 정의한 후, window 안은 유지한채 그 바깥은 truncate 하거나 maximum context length 로 interpolate 하는 방법을 소개한다. 이 방법으로 local position relationships 을 유지하면서 extrapolation capacity 를 얻는 것을 확인한다. 단점은, attention matrix 를 두 번 계산하기 때문에, 추가적인 cost 가 든다.
   
4) Base modification : 이미 고정된 maximum training length (e.g. 4096 in LLaMA2) 에서, basis angle 인 $\theta$ 를 줄이면 longer text 처리가 가능하다.  


- **Adapting Context Windows**

LLM 이 학습 과정에서 고정된 context window 를 갖고 있기 때문에, long sequence 처리가 힘들다. 이 한계점을 극복하기 위해, 아래의 방법들이 고안되었다.

1) Parallel context window : Fusion-in-Decoder (FID) 와 같이, divdied-and-conquer 기술을 활용하여 input text 를 처리한다. 그러나 이러한 방법은 different segment 들을 구별할 수 없기 때문에, 성능에 제한이 있다.

2) Λ-shaped context window : 최근 연구들에서 LLM 은 attention weight 을 시작과 끝에 더 크게 allocate 하는 *"lost in the middle"* 현상을 보인다. 이 발견에 따라, LM-Infinite, StreamingLLM 등은 "Λ-shaped" attention mask 방법을 적용하여, scope 를 정한 후 그 바깥의 token 은 버린다. 이 방법은 long context 에의 확장성은 좋지만, long-range dependency 를 모델링 하는데 어려움이 있고 성능이 좋지 못하다. 

3) External memory : Transformer 의 attention pattern 의 대부분이 small subset of token 에서 capture 된다는 발견을 바탕으로, past key 들을 external memory 에 넣은 후, k-NN search 를 통 k 개의 most relevant token 을 찾아 generation 에 활용한다. 

<span style='color:green;font-weight:bold'> (5) Decoding Strategy </span>
<br>
LLM 이 학습된 이후에는 효과적인 generation 을 위한 decoding strategy 을 잘 선택해야할 필요가 있다.
Greedy search, Beam search (+Length Penalty), Random Sampling, Top-k sampling, Top-p sampling (neclues sampling) 기법 등이 존재한다. 또한, LLM 의 decoding 방식은 ***memory wall*** 등의 문제로 효율적이지 못한데, 이를 해결하기 위해, Fast-Decoding 등이 고안되었다.

# 4.3 Model Training

LLM 을 학습하기 위한 중요한 setting 과 trick 들을 알아본다.

<span style='color:green;font-weight:bold'> (1) Optimization Setting </span>
<br>

- **Batch Training**

Training stability 와 throughput 을 위해 batch size 는 어느 정도 크게 가져간다. (2,048 examples or 4M tokens)
GPT-3 와 PaLM 에서는 dynamic 하게 batch size 를 키우는 새로운 기법을 소개한다.
GPT-3 의 경우 32K token 부터 시작하여 3.2M 까지 증가한다. 이러한 dynamic schedule 이 LLM 학습에 안정성을 부여한다는 Empirical result 가 존재한다.

- **Optimizer**

Adam 과 AdamW 가 LLM 학습에 많이 사용된다. Hyper-parameter 로는 $\beta_1=0.9, \beta_2=0.95, \epsilon=10^{-8}$ 를 사용한다.
T5 와 PaLM 에서는 Adafactor 가 사용되었다.

- **Stabilizing the Trainig**

LLM 학습시에 mode collapse 와 같은 training instability issue 가 발생하기 쉽다.
기존에 이러한 학습 안정성을 위해 gradient clipping 이나 weight decay 등이 제안되었지만, <span style='background-color: #dcffe4'> LLM 에서는 여전히 training loss spike 가 튀는 경우가 빈번하다. </span>
이러한 학습을 위해서 PaLM 과 OPT 의 경우, spike 가 튀기 직전의 checkpoint 에서 restart 하는 나이브한 방법을 택하며, 문제가 되는 data 는 skip 한다.
GLM 의 경우, spike 를 발생시키는 abnormal gradient 를 shirnk 한다.


<span style='color:green;font-weight:bold'> (2) Scalable Training Techniques </span>
<br>

LLM 학습 시에 두 가지 큰 issue 가 있다: 하나는 training throughput 이 너무 크다는 것이고, 다른 하나는 GPU memory 에 loading 할 때 모델이 크다는 것이다.
이를 해결하기 위한 기법들을 소개한다.

- **3D parallelism**

3D parallelism 은 흔히 사용되는 세 가지 병렬 처리 방식인 data parallelism, pipeline parallelism, tensor parallelism 을 모두 사용하는 것이다.
Data parallelism 은 흔히 쓰이는 방식이므로 생략하고, pipeline 의 경우 consecutive layer 를 GPU 에 분산 배치하여 학습시키는 것이다. 이 때, GPU 가 다른 GPU 의 연산을 기다려야 하는 bubbles overhead 문제가 발생하는데, 이를 해결 하기 위해, GPipe 나 PipeDream 등의 기법이 개발되었다. Tensor parallelism 의 경우, matrix tensor 를 submatrix 로 split 하여 다른 GPU 에 올리는 것이다. Megatron-LM 등의 오픈 소스에서도 쓸 수 있다. 

위의 기법들을 practice 에 적용할 때는 jointly 적용이 된다.
예를 들어, BLOOM 의 경우 384 개의 A100 이 사용되었으며, 8-way data parallelism, 4-way tensor parallelism, 12-way pipeline parallelism 이 사용되었다.

- **ZeRO**

DeepSpeed library 에 존재하는 ZeRO 기법은 data parallelism 시 모든 data 를 모든 GPU 가 다 갖고 있지 않고, 일부만 가지고 있다가 필요시에 retrieve 하는 방식이다. Pytorch 에서는 ZeRO 와 유사한 기법으로 FSDP 가 구현되어있다.

- **Mixed Precision Training**

32-bit float 연산을 16-bit (FP16), 더 나아가 8-bit (FP8) 로 줄인다. 그러나 일반적인 방법은 성능 저하를 불러 올 수 있기 때문에, 최근에는 Brain Floating Point (BF16) 이라는 것이 개발되었고, FP16 에 비해 더 많은 exponent bits 를 할당하여 FP16 보다 좋은 성능을 보였다.


<span style='color:green;font-weight:bold'> 초록색 볼드체 </span>
<span style='background-color: #dcffe4'> 초록색 배경 </span>

<span style='background-color: #dcffe4'> [A Survey of Large Language Models (3)](https://yong1-kim.github.io/transformer/plm/llm/2023/12/24/LLMsurvey3.html) 에서 계속... </span>
