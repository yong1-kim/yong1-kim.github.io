---
layout: post
title:  "A Survey of Large Language Models (2)"
date:   2023-12-24 11:45:00 +0900
use_math: true
categories: [Transformer, PLM, LLM]
---

[[pdf]](https://arxiv.org/pdf/2303.18223.pdf)
[[github]](https://github.com/RUCAIBox/LLMSurvey)

**Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie and Ji-Rong Wen**

<span style='background-color: #dcffe4'> [A Survey of Large Language Models (1)](https://yong1-kim.github.io/transformer/plm/llm/2023/12/24/LLMsurvey1.html) 에 이어서... </span>

## 4. Pre-training

LLM 을 pretrain 하는데는 효율적인 알고리즘, model architecture, optimization technique 등이 모두 중요하다.
이번 섹션에서는 LLM 을 pretrain 하기 위한 세 가지 요소인 **_(1) data collection, (2) model architecture, (3) training technique_** 를 각각 살펴본다.

# 4.1. Data Collection and Preparation

LLM 을 학습하기 위해 높은 퀄리티의 dataset 을 확보하는 것은 매우 중요하다. 이번 section 에서는 _data source, preprocessing methods, 그리고 pre-training data 가 LLM 에 미치는 영향_ 의 세 가지 측면을 살펴본다.

<span style='background-color: #dcffe4'> (1) Data Source </span>

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/032b874d-64ac-4b15-97be-5da2c45a6e15)

대부분의 LLM 은 위에 보이는 그림처럼 여러 data source 의 mixture 를 pretraining dataset 으로 활용한다.
Dataset 들은 크게 두 가지로 나눌 수 있는데, 하나는 **general text data**, 다른 하나는  **specialized text data** 이다.
**General data** 는 대부분의 LLM 에서 활용하는 dataset 으로 webpage, books, converational text 등이 속하며, 크기가 크고 (large) 다양하며 (diverse), 접근이 용이하기 때문에, generalization ability 를 높이기 위해 필요하다.
**Specialized data** 에는 multilingual data, scientific data, code 와 같은 특정한 task-solving capability 를 부여하기 위해 사용된다.

※ 각 dataset 에 대한 자세한 설명은 논문 참조.

<span style='background-color: #dcffe4'> (2) Data Preprocessing </span>

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/c03bcc64-4cac-481f-b140-1f0a44968174)

Pre-training corpus 를 준비한 이후에는 noisy, redundant, irrelevant, toxic data 를 제거하는 전처리가 필수적이다. 
최근 [Data-Juicer](https://arxiv.org/pdf/2309.02033.pdf) 라는 여러 전처리 방법을 담고 있는전처리 tool 이 release 되었다. 
일반적인 전처리 pipeline 은 위의 그림과 같다.

- **Quality Filtering**

Quality filtering 에는 일반적으로, classifier-based 방법과 heuristic-based 방법이 있는데, 기존에 많이 사용하던 classifier-based 방법은 방언이나 구어체 등을 제거할 확률이 높아 bias 를 높이는 경향이 있다. 따라서 최근 BLOOM 이나 GOPHER 등에서는 heuristic 방법을 쓰는데, 그 종류에는 *Language based filtering, Metric based filtering, Statistic based filtering, Kyeowrd based Filtering* 등이 있다.

- **De-duplication**

[최근 한 연구](https://arxiv.org/abs/2205.10487)에서, 문장이 반복되는 duplication 문제가 training 을 unstable 하게 만들고 성능을 떨어뜨린다는 주장을 하였다. 이에 repeated word 를 가지는 low quality 문장을 제거하고 (sentence-level), n-gram 등을 기반으로 너무 많이 겹치는 documnet 를 제거하며 (document-level), dataset contimination 문제 해결을 위해 training set 과 eval set 의 overlap 을 해결한다 (set-level).

- **Privacy Reduction**

흔히, PII 라고 부르는 _personally identifiable information_ 를 pretraining corpus 에서 제거해야 한다. 한 가지 방법으로는 rule-based 로 name, address, phone number 등을 지우는 것이다.

- **Tokenization**

이제 Tokenization 을 진행하면 된다. 최근에는 subword-level 기반의 tokenization 이 주로 사용되고, byte pair encoding (BPE), Wordpiece tokenization, unigram tokenization 등이 사용된다. 
BPE 는 multilingual setting 에서 장점을 보이며, GPT-2, BART, LLaMA 등에서 사용한다.
Wordpiece 는 Google 의 subword tokenization 알고리즘으로, 처음에는 voice search system 을 위해 고안되었으나, 이후 MT 모델, 그리고 BERT 에서 사용되었다. Wordpiece 는 BPE 와 기본적으로 유사한 방법이지만, merge 하는 방법에서 조금의 차이점을 보인다. 
마지막으로 Unigram tokenization 은 EM 알고리즘의 일종으로, old LM 을 활용하여 큰 vocab 에서 하나씩 제거해 나가며 dictionary 를 완성한 후, 다시 re-estimate 하여 vocab 을 만들고를 반복한다. T5, mBART 등에서 사용되었다.

OPT 와 GPT-3 가 GPT-2 tokenizer 를 사용한 것처럼, 기존에 있는 tokenizer 를 사용하는 것도 좋은 방법 중에 하나이지만, <span style='background-color: #dcffe4'> 모델이 학습하는 pre-training corpus 에 맞춰 specially designed tokenization 기법을 적용하는 것은 큰 도움이 된다. </span>
따라서, 최근에는 BPE 와 unigram 기법을 합친 Sentence Piece library 를 활용하는 등 **customized tokenizer** 를 활용하는 경향성이 높다. 단, transfer learning 을 할 때 이러한 customized tokenizer 는 조심해야한다. LLaMA 의 경우, pretraining 시에 BPE 를 활용하기 때문에, non-english dataset 에 대해서는 fine-tuning 에 어려움이 있을 수 있다.

<span style='background-color: #dcffe4'> (3) Data Scheduling </span>

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/a4bba40f-b52c-47e3-b470-d565ce12a1aa)

Data scheduling 에는 두 가지가 중요하다 : **data mixture, data curriculum**.
- **Data mixutre**

Data 를 섞을 때는 proportion 이 중요하다. 보통 upsampling, downampling 기법등을 이용한다. 
최근 여러 연구에서 하나의 domain 의 data 를 너무 많이 배우는 것은 좋지 못한 성능을 낸다는 것을 검증하였다.
또, 몇몇의 연구에서는 heuristic 하게 proportion 을 결정하지 않고, model 을 활용하여 optimize 하는 방법을 제안하였다. 간단한 예로, downstream task 에 맞춰 그 task 에 맞는 pretraining corpus 의 비율을 증가시키는 것들이 있으나, 실용적이지는 못하다. 

- **Data curriculum**

Basic skill 을 배운 이후 traget skill 을 배우는 것이 효과적이라는 것이 몇몇 연구([[1]](https://arxiv.org/abs/2307.14430),[[2]](https://arxiv.org/abs/2308.12950))에서 검증되었다. 이에 따라 dataset 을 pretraining 할 때, 어떠한 것을 먼저 배울지 그 curriculum 을 정하는 것도 중요하다.
보통 target skill 은 <span style='background-color: #dcffe4'> coding, Mathematics, Long context modeling </span> 세 가지에 대해 curriculum 을 많이 적용한다.


# 4.2. Architecture

이 섹션에서는 LLM 의 아키텍쳐 디자인 : mainstream architecture, pre-training objective, detail configuration 등을 살펴본다.

<span style='color:green;font-weight:bold'> (1) Typical Architectures </span>
<br>


![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/1e13623a-d8e8-4f83-937d-3aacefc143b5)

LLM 의 backbone 은 Transformer 가 de-facto architecture 이다.
보통 크게 세 가지 major type 으로 나눈다 : encoder-decoder 구조, causal decoder 구조, prefix decoder 구조.

- **Encdoer-decoder Architecture** : T5, BART 등
- **Causal Decoder Architecture** : GPT-Series, OPT, BLOOM, Gopher 등 대부분의 LLM 들
- **Prefix Decoder Architecture** : U-PaLM, GLM-130B 등

<span style='color:green;font-weight:bold'> (2) Detailed Configuration </span>
<br>

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/51ccd0ed-ad88-4322-a3b9-1a2dc18acdf9)


대부분 LLM 의 기반인 Transformer 의 네 가지 configuration 인 (1)**Normalization method**, (2) **Normalization position**, (3)**Activation Functions**, (4) **Position embeddings** 를 다룬다.

추가적으로, Attention mechanism 에 대해서는 (1) Full attention, (2) Sparse attention, (3) Multi-query/grouped-query attention, (4) FlahsAttention, (5) PagedAttention 등을 다룬다. 

※ 각 configuration 및 method 에 대한 자세한 설명은 논문 참조.

<span style='color:green;font-weight:bold'> (3) Pre-training Tasks </span>
<br>
LLM 은 대부분 **Langague Modeling** 과 **Denoising Autoencoding** 을 학습한다.
※ 관련 내용은 너무 유명하므로 생략, 논문 참조.

<span style='color:green;font-weight:bold'> (4) Long Context Modeling </span>
<br>
최근 PDF proecssing 이나 story writing 과 같은 <span style='background-color: #dcffe4'> long context modeling capacity </span> 를 increasing 하기 위한 요구가 많다.
GPT-4 는 128K context window 를 지원하고, Claude 2.1 (Anthropic 社) 은 200K context window 를 활용한다.
Long context modeling 능력을 위해서는 대표적으로 두 가지 기법이 활용된다.

- **Scaling Position Embeddings**

T5 Bias, ALiBi, xPos, NoPE 같은 position embedding 기법들 대부분이 maximum training length 안에서의 학습만으로 충분한 generalization 효과를 본다. 이를 <span style='background-color: #dcffe4'> extrapolation capability </span> 라고 하는데, mainstream position embedding 중 하나인 Rotary Position Embedding (RoPE) 의 경우, 이 extrapolation capa 가 없다. 이에 아래 방법들을 통해 RoPE 를 longer text 에 scale 할 수 있다.

1) Direct model fine-tuning : LLM 을 단순하게 더 긴 text 에 fine-tuning 하는 방법이다. 보통 multi-stage approach (e.g. 2K->8K-> 32K) 를 활용한다. 매우 느리다는 단점이 있다.
   
2) Position interpolation : Long context 의 position index 들을 downcale 하여, original context window 크기로 맞추는 방법이다. 단순히 position index 들에 L/L' (original context length L, target context lenth L') 을 곱해주는데, 실험 결과 효과적으로, 그리고 효율적으로 Long context 로 extend 할 수 있지만, 짧은 텍스트에 오히려 adverse impact 가 있다.
   
3) Position truncation : out-of-distribution rotation angle 문제를 해결하기 위해, long context 의 longer relative position 을 truncate 해버리는 방법이다. ReRoPE 나 LeakyReRoPe 에서는 pre-difeined window length 를 정의한 후, window 안은 유지한채 그 바깥은 truncate 하거나 maximum context length 로 interpolate 하는 방법을 소개한다. 이 방법으로 local position relationships 을 유지하면서 extrapolation capacity 를 얻는 것을 확인한다. 단점은, attention matrix 를 두 번 계산하기 때문에, 추가적인 cost 가 든다.
   
4) Base modification : 이미 고정된 maximum training length (e.g. 4096 in LLaMA2) 에서, basis angle 인 $\theta$ 를 줄이면 longer text 처리가 가능하다.  


- **Adapting Context Windows**

LLM 이 학습 과정에서 고정된 context window 를 갖고 있기 때문에, long sequence 처리가 힘들다. 이 한계점을 극복하기 위해, 아래의 방법들이 고안되었다.

(1) Parallel context window : 

<span style='color:green;font-weight:bold'> 초록색 볼드체 </span>
<span style='background-color: #dcffe4'> 초록색 배경 </span>

<span style='background-color: #dcffe4'> [A Survey of Large Language Models (3)](https://yong1-kim.github.io/transformer/plm/llm/2023/12/24/LLMsurvey3.html) 에서 계속... </span>
