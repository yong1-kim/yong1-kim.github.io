---
layout: post
title:  "[ICML2022] NLP From Scratch Without Large-Scale Pretraining: A Simple and Efficient Framework"
date:   2022-11-12 17:38:00 +0900
use_math: true
categories: [Transformer, PLM]
---
[[pdf]](https://proceedings.mlr.press/v162/yao22c/yao22c.pdf)  &emsp;
[[github]](https://github.com/yaoxingcheng/TLM) <br>

**Xingcheng Yao<sup>* 1</sup>, Yanan Zheng<sup>* 2</sup>, Xiaocong Yang<sup>3 4</sup>, Zhilin Yang<sup>1 5 4</sup>**
<br><sup>*</sup>Equal Contribution, <sup>1</sup> Institute for Interdisciplinary Information Sciences, Tsinghua University, <sup>2</sup>Department of Computer Science and Technology, Tsinghua University, <sup>3</sup> School of Economics and Management, Tsinghua University, <sup>4</sup> Recuurent AI, Inc, <sup>5</sup> Shanghai Qi Zhi Institute.      &emsp; 

![image](https://user-images.githubusercontent.com/42200027/201530852-d42832d4-ee65-47d1-92bd-e127c1648c0a.png)

# Abstract
- (Motivation) Pre-trained Language Model (PLM) 이 NLP task 를 푸는 굉장히 강력한 standard 가 되었지만, train 하기에는 computation cost 가 너무 비싸다. 
- (Solution) 이 연구에서는 simple and efficient learning framework **TLM** 을 제안하여, large-scale pretraining 에 rely 하지 않는 학습 방법을 제안한다. 
- (Method) Labeled task data 와 large general corpus 에 대하여, TLM 은 task data 를 Query 로 하여 general corpus 로부터 tiny subset 을 retrieval 한 후, task objective 를 jointly optimize 한다.  
- (Result) 4개 domain 의 8개 데이터셋에 대한 실험 결과, TLM 은 PLM 과 비교하여 FLOP 은 두 자리수나 적으면서 성능은 더 좋거나 유사한 성능을 보인다. 

# Introduction
Pre-trained Language Models (PLMs) 들이 NLP 에서 큰 성공을 거두고 있다. Large general corpora 에 Masked Language Modeling (MLM; [BERT](https://aclanthology.org/N19-1423.pdf), [RoBERTa](https://arxiv.org/pdf/1907.11692.pdf), [T5](https://arxiv.org/pdf/1910.10683.pdf)), autoregressive language modeling([GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf), [GPT-3](https://arxiv.org/pdf/2005.14165.pdf)), permutation language modeling([XLNet](https://papers.nips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf)) 등의 self-supervised language modeling task 을 활용하여 pre-train 하고, 적은 양의 downstream task 에 대하여 fine-tuning 하는 PLM 은 많은 NLP task 에서 압도적인 성능을 보이고 있다. 

그러나, 이러한 PLM 들은 computationally expensive 하다. 예를 들어 [RoBERTa-Large](https://arxiv.org/abs/1907.11692) 의 경우, 4.36 x $10^21$ 이라는 엄청난 FLOPs 을 요구하며, 이는 무려 1,000 대의 32GB V100 GPU 로 하루를 계산해야하는 양이다. 더 큰 Large Language Model (LLM) 으로 가게 되면, [GPT-3](https://arxiv.org/pdf/2005.14165.pdf) 의 경우, 이 RoBERTa-Large 보다도 50배나 더 많은 계산량이 학습에 요구된다. 이러한 엄청난 계산량은 연구계, 특히 학교단위의 연구계에서 새로운 architecture 탐구나, customized LM 탐구, 개선된 pre-training loss 탐구 등의 연구를 limited budget 문제로 불가능하게 만든다. 현재 대부분의 NLP 연구자들은 fine-tuning alogrithm 을 발전시키는데 기대고 있지만, 이는 pre-training procedure 에 대개 upper-bound 될 수 밖에 없다. 

기존의 몇몇 연구들([ELECTRA](https://arxiv.org/pdf/2003.10555.pdf), [Primer](https://arxiv.org/pdf/2109.08668.pdf), [[1]](https://arxiv.org/pdf/2109.10686.pdf), [EarlyBERT](https://aclanthology.org/2021.acl-long.171.pdf)) 에서 language model pre-training 의 효율성을 개선하려는 시도가 있었지만, 대부분은 sample-efficient self-supervised task 를 제안하거나, pre-training 에 알맞는 efficient Transformer architecture 를 제안하는데 그친다. 이러한 연구들은 매우 효율적이고 도움이 되지만, FLOP 측면에서 한 자리수 정도를 줄이는데 그친다. Distillation 으로 PLM 의 size 를 줄이려는 시도들([DistilBERT](https://arxiv.org/pdf/1910.01108.pdf), [TinyBERT](https://aclanthology.org/2020.findings-emnlp.372.pdf)) 도 있었지만, 이러한 시도는 학습을 위해, 거대한 PLM training 이 필요하다는 단점이 있다. 그리고 아직까지 distilled version 의 PLM 은 RoBERTa-Large 같은 기존 PLM에 비해 성능이 많이 떨어진다.

<span style='background-color: #dcffe4'> 이 연구에서는 performance drop 없이  drastic efficiency improvement 를 갖는 완전히 새로운 pretraining-finetuning framework 를 제안한다.</span> 연구자들은 간단하고(simple), 효율적이고(efficient), **pre-training-free** framework 인 **T**ask-driven **L**anguage **M**odeling (**TLM**) 기법을 제안한다. Large general corpus 와 some labeled task data 가 주어졌을 때, TLM 은 PLM 에 의존하지 않고 model 을 from scratch 로 학습을 시작한다. TLM 은 두 가지 key idea 에서 motivate 되었다. 첫 번째로, 인간은 시험공부 벼락치기를 위해, 모든 책을 다 보지 않고 단지 몇 개의 chapter 만을 본다. 저자들은 specific 한 task 를 푸는데 있어서 large corpus 를 다 보는 것은 큰 redunduncy 가 있다고 가정한다. 두 번째로, supervised labeled data 를 직접 학습하는 것이, unlabeled data 로 부터 language modeling objective 를 최적화하는 것보다, downstream performance 에 더 효과적이다. 이러한 점들로부터, TLM 은 task data 를 query 로 하여, general corpus 의 tiny subset 을 retrieve 한다. 이후, retrieved data 와 task data를  supervised task objective 와 languge modeling objective 를 jointly optimizing 한다. 

4 개 domain - news, review, computer science, biomedical science - 의 8 개 데이터셋 (실험 세팅 : [[2]](https://aclanthology.org/2020.acl-main.740.pdf))에서, TLM 은 BERT 와 RoBERTa 보다 좋거나 유사한 성능을 보이면서, 무려 **2 개 자리수(two orders of magnitude)나 적은 FLOPs** 를 사용한다. 

# Related Works

