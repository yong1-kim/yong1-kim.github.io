---
layout: post
title:  "[ICML2022] NLP From Scratch Without Large-Scale Pretraining: A Simple and Efficient Framework"
date:   2022-11-12 17:38:00 +0900
use_math: true
categories: [Transformer, PLM]
---
[[pdf]](https://proceedings.mlr.press/v162/yao22c/yao22c.pdf)  &emsp;
[[github]](https://github.com/yaoxingcheng/TLM) <br>

**Xingcheng Yao<sup>* 1</sup>, Yanan Zheng<sup>* 2</sup>, Xiaocong Yang<sup>3 4</sup>, Zhilin Yang<sup>1 5 4</sup>**
<br><sup>*</sup>Equal Contribution, <sup>1</sup> Institute for Interdisciplinary Information Sciences, Tsinghua University, <sup>2</sup>Department of Computer Science and Technology, Tsinghua University, <sup>3</sup> School of Economics and Management, Tsinghua University, <sup>4</sup> Recuurent AI, Inc, <sup>5</sup> Shanghai Qi Zhi Institute.      &emsp; 

![image](https://user-images.githubusercontent.com/42200027/201530852-d42832d4-ee65-47d1-92bd-e127c1648c0a.png)

# Abstract
- (Motivation) Pre-trained Language Model (PLM) 이 NLP task 를 푸는 굉장히 강력한 standard 가 되었지만, train 하기에는 computation cost 가 너무 비싸다. 
- (Solution) 이 연구에서는 simple and efficient learning framework **TLM** 을 제안하여, large-scale pretraining 에 rely 하지 않는 학습 방법을 제안한다. 
- (Method) Labeled task data 와 large general corpus 에 대하여, TLM 은 task data 를 Query 로 하여 general corpus 로부터 tiny subset 을 retrieval 한 후, task objective 를 jointly optimize 한다.  
- (Result) 4개 domain 의 8개 데이터셋에 대한 실험 결과, TLM 은 PLM 과 비교하여 FLOP 은 두 자리수나 적으면서 성능은 더 좋거나 유사한 성능을 보인다. 

# Introduction
Pre-trained Language Models (PLMs) 들이 NLP 에서 큰 성공을 거두고 있다. Large general corpora 에 Masked Language Modeling (MLM; [BERT](https://aclanthology.org/N19-1423.pdf), [RoBERTa](https://arxiv.org/pdf/1907.11692.pdf), [T5](https://arxiv.org/pdf/1910.10683.pdf)), autoregressive language modeling([GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf), [GPT-3](https://arxiv.org/pdf/2005.14165.pdf)), permutation language modeling([XLNet](https://papers.nips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf)) 등의 self-supervised language modeling task 을 활용하여 pre-train 하고, 적은 양의 downstream task 에 대하여 fine-tuning 하는 PLM 은 많은 NLP task 에서 압도적인 성능을 보이고 있다. 

그러나, 이러한 PLM 들은 computationally expensive 하다. 예를 들어 [RoBERTa-Large](https://arxiv.org/abs/1907.11692) 의 경우, 4.36 x $10^21$ 이라는 엄청난 FLOPs 을 요구하며, 이는 무려 1,000 대의 32GB V100 GPU 로 하루를 계산해야하는 양이다. 더 큰 Large Language Model (LLM) 으로 가게 되면, [GPT-3](https://arxiv.org/pdf/2005.14165.pdf) 의 경우, 이 RoBERTa-Large 보다도 50배나 더 많은 계산량이 학습에 요구된다. 이러한 엄청난 계산량은 연구계, 특히 학교단위의 연구계에서 새로운 architecture 탐구나, customized LM 탐구, 개선된 pre-training loss 탐구 등의 연구를 limited budget 문제로 불가능하게 만든다. 현재 대부분의 NLP 연구자들은 fine-tuning alogrithm 을 발전시키는데 기대고 있지만, 이는 pre-training procedure 에 대개 upper-bound 될 수 밖에 없다. 

기존의 몇몇 연구들([ELECTRA](https://arxiv.org/pdf/2003.10555.pdf), [Primer](https://arxiv.org/pdf/2109.08668.pdf), [[1]](https://arxiv.org/pdf/2109.10686.pdf), [EarlyBERT](https://aclanthology.org/2021.acl-long.171.pdf)) 에서 language model pre-training 의 효율성을 개선하려는 시도가 있었지만, 대부분은 sample-efficient self-supervised task 를 제안하거나, pre-training 에 알맞는 efficient Transformer architecture 를 제안하는데 그친다. 이러한 연구들은 매우 효율적이고 도움이 되지만, FLOP 측면에서 한 자리수 정도를 줄이는데 그친다. Distillation 으로 PLM 의 size 를 줄이려는 시도들([DistilBERT](https://arxiv.org/pdf/1910.01108.pdf), [TinyBERT](https://aclanthology.org/2020.findings-emnlp.372.pdf)) 도 있었지만, 이러한 시도는 학습을 위해, 거대한 PLM training 이 필요하다는 단점이 있다. 그리고 아직까지 distilled version 의 PLM 은 RoBERTa-Large 같은 기존 PLM에 비해 성능이 많이 떨어진다.

<span style='background-color: #dcffe4'> 이 연구에서는 performance drop 없이  drastic efficiency improvement 를 갖는 완전히 새로운 pretraining-finetuning framework 를 제안한다.</span> 연구자들은 간단하고(simple), 효율적이고(efficient), **pre-training-free** framework 인 **T**ask-driven **L**anguage **M**odeling (**TLM**) 기법을 제안한다. Large general corpus 와 some labeled task data 가 주어졌을 때, TLM 은 PLM 에 의존하지 않고 model 을 from scratch 로 학습을 시작한다. TLM 은 두 가지 key idea 에서 motivate 되었다. 첫 번째로, 인간은 시험공부 벼락치기를 위해, 모든 책을 다 보지 않고 단지 몇 개의 chapter 만을 본다. 저자들은 specific 한 task 를 푸는데 있어서 large corpus 를 다 보는 것은 큰 redunduncy 가 있다고 가정한다. 두 번째로, supervised labeled data 를 직접 학습하는 것이, unlabeled data 로 부터 language modeling objective 를 최적화하는 것보다, downstream performance 에 더 효과적이다. 이러한 점들로부터, TLM 은 task data 를 query 로 하여, general corpus 의 tiny subset 을 retrieve 한다. 이후, retrieved data 와 task data를  supervised task objective 와 languge modeling objective 를 jointly optimizing 한다. 

4 개 domain - news, review, computer science, biomedical science - 의 8 개 데이터셋 (실험 세팅 : [[2]](https://aclanthology.org/2020.acl-main.740.pdf))에서, TLM 은 [BERT](https://aclanthology.org/N19-1423.pdf) 와 [RoBERTa](https://arxiv.org/pdf/1907.11692.pdf) 보다 좋거나 유사한 성능을 보이면서, 무려 **2 개 자리수(two orders of magnitude)나 적은 FLOPs** 를 사용한다. 

# Related Works
<span style='color:green;font-weight:bold'> Pre-trained Language Models </span>
<br>
BERT 이후로 많은 PLM 모델들이 등장하였고, 이들은 많은 NLP 문제들의 de-facto solution 이 되었다.
이들은 거의 대부분 pre-training 으로 large corpus 에서 contextualized token representation 을 배우고, specific task 에 labeled data 를 fine-tuning 해서 학습한다. 
BERT 는 16 G English corpora 를 MLM 을 이용해 학습하고, RoBERTa 는 BERT 와 구조가 같지만, 160G 의 English text 를 large batch size 와 dynamic token masking 등을 이용해 학습한다.
이 연구에서는 BERT 와 RoBERTa 를 baseline 으로 사용한다.

<span style='color:green;font-weight:bold'> Efficient Pretraining for NLP </span>
<br>
Languge model 의 pre-training 의 efficiency 를 향상시키기 위한 연구가 많이 있었다. [You et al.](https://arxiv.org/pdf/1904.00962.pdf) 과 [Megatron-LM](https://arxiv.org/pdf/1909.08053.pdf) 에서는 pre-training process를 가속화 하기위해, 데이터 병렬과 모델 병렬처리를 활용한다. 하지만, 병렬처리를 활용한 가속화는 FLOP 측면에서 전혀 줄어들지 않는다. EarlyBERT 와 Prier 에서는 lottery ticket hypothesis 와 Neural Architecture Search 를 이용한 efficient neural network 를 찾았다. 이는 FLOP 측면에서 50% ~ 70% 의 computational cost 를 줄였다. ELCTRA 와 [DeBERTa](https://arxiv.org/pdf/2006.03654.pdf) 는 adversarial training 과 disentagled representation of content and position 이라는 새로운 LM pre-training mechanism 을 직접 design 하여 50% ~ 75% 의 computation cost 개선을 가져왔다. [Train-no-evil](https://aclanthology.org/2020.emnlp-main.566.pdf) 에서는 selective masking 을 활용한 task-guided pre-training 으로 50% 의 computational cost reduction 을 얻었다. 
이 연구에서는 이러한 연구들과는 독립적으로(orthogonal), **training data redundancy 를 줄이는 방법** 을 통해, efficiency  를 향상시킨다. 
이 연구가 훨씬 더 drastic improvement 를 가져온다.

<span style='color:green;font-weight:bold'> Efficient Inference of Pretrained Models </span>
<br>
PLM 연구의 다른 한 줄기는 inference efficiency 를 향상시키는 방향의 연구들이다. 
[DistilBERT](https://arxiv.org/pdf/1910.01108.pdf), [TinyBERT](https://aclanthology.org/2020.findings-emnlp.372.pdf), [MobileBERT](https://arxiv.org/abs/2004.02984), [FastBERT](https://arxiv.org/abs/2004.02178), [BORT](https://arxiv.org/pdf/2010.10499.pdf), 그리고 [BERT-of-Theseus](https://aclanthology.org/2020.emnlp-main.633/) 같은 연구들에서는 **small-sized model** 을 통해 inference efficiency 를 추구한다.
[Q8-BERT](https://ieeexplore.ieee.org/document/9463531), [Q-BERT](https://ojs.aaai.org//index.php/AAAI/article/view/6409), [I-BERT](https://proceedings.mlr.press/v139/kim21d.html) 등에서는 **quantizing 기법**을 이용하여 low-precision representation 을 통해 inference 를 향상시킨다.
**Pruning 기법** 을 활용하여 small size PLM 을 inference 를 위해 사용하는 연구들([[3]](https://papers.nips.cc/paper/2019/hash/2c601ad9d2ff9bc8b282670cdd54f69f-Abstract.html), [[4]](https://aclanthology.org/2020.emnlp-main.496/), [[5]](https://aclanthology.org/2020.repl4nlp-1.18.pdf)) 도 있다.
<span style='background-color: 	#F4C2C2'> 그러나 이러한 model compression 기법을 이용한 방법들은 large PLM 에 의존할 뿐 아니라, 성능도 꽤 큰 차이로 떨어지게 된다.</span> 
이 연구에서 제시하는 방법은 PLM 에 의존하지 않을 뿐더러, 성능 역시 비슷하거나 좋아진다.

<span style='color:green;font-weight:bold'> Domain and Task Adaptation for Pretrained Models </span>
<br>
Domain-adaptive fine-tuning 은 pre-trained model 을 in-domain data 에 language modeling obejctive 로 fine-tune 하는 것이다.
이 방법은 domain/task adaptation 에서 좋은 성능이 있음이 밝혀졌다. ([[6]](https://aclanthology.org/N19-1189/), [[7]](https://aclanthology.org/2020.acl-main.740/), [[8]](https://arxiv.org/abs/2009.04984), [[9]](https://academic.oup.com/bioinformatics/article/36/4/1234/5566506)) 
TLM 과의 차이점은, TLM 은 additional domain data 를 필요로 하지 않고, 단지 BERT 와 RoBERTa 의 corpora 만 활용한다.
그리고 기존의 domain-adaptive fine-tuning 방식은 pre-trained model 을 필요로 하지만, TLM 은 그렇지 않다는 차이점이 있다. 

<span style='color:green;font-weight:bold'> Co-training for Semi-supervised Learning and DataDensity-Based Active Learning </span>
<br>
TLM 과의 유사성을 갖는 연구로 두 가지가 있다.
첫 번째는 **Co-Training (CT)**  ([[10]](https://link.springer.com/chapter/10.1007/978-3-030-01267-0_9), [[11]](https://ieeexplore.ieee.org/document/9710117)) 이고, 두 번째는 **Data-Density-Based Active Learning (DAL)** ([[12]](https://dl.acm.org/doi/10.1109/TASL.2009.2033421),[[13]](https://www.sciencedirect.com/science/article/abs/pii/S095741741730369X))이다.
CT 와 TLM 모두 unlabeled data 를 certain task 학습을 위해 활용하는 것은 같지만, 2 가지 측면에서 차이점이 있다.
첫 번째는 CT 는 unlabeled data 를 다양한 view 에서 보기 위한 여러가지 distinct model 들이 필요하지만, TLM 은 single model 을 train 한다.
두 번째로 TLM은 unlabeled data 의 selection process 가 있지만, CT 에서는 이 process 가 고려되지 않는다.

TLM 과 DAL 은 unlabeled data 에서 representative instance 를 찾는 flavor 는 동일하다.
그러나, DAL 의 경우 모든 unlabled data 가 task 의 definition 으로 label 될 수 있다는 가정이 있어야 하지만, TLM 은 그것이 필요하지 않다.
그리고, DAL 은 전체 unlabeled data 로 부터 iteratively critical instance 를 찾기 위해 노력하지만, TLM 은 labeld data 와 관련이 있는 relevant instance  를 one-shot 으로 한 번만 찾기 때문에 훨씬 효율적이다. 
따라서 TLM 이 classic DAL 알고리즘 보다 훨씬 효율적이다.

# Method
# TLM : Task-Driven Language Modeling
인간은 제한된 시간과 노력으로 빠르게 특정한 task 를 master 할 수 있는 능력을 지니고 있다. 
예를 들어, 시험 벼락치기를 할 때, 전세계의 모든 책을 보는 것이 아니라 단지 몇 개의 chapter 만을 공부하지만, 시험을 잘 볼 수 있다.
이 관찰로부터, 저자들은 **빠르고 정확하기 task-relevant information 을 locate 하는 것이 key aspect** 라고 가정한다.
결국, TLM 은 <span style='background-color: #dcffe4'> (1) general corpora 로부터 relevant training data 를 automatically retreive 하고, (2) retrieved data 와 task data 를 결합하여 학습한다.</span>

수식적으로 보면, general corpus $D = \lbrace d_i \rbrace_i$ where $d_i$ is document, labled task data, $T = {(x_i,y_i)}_i$ where $x_i$ is text and $y_i \in Y$ is a label 에 대해, 목표는 coniditional probability for classification $f(x)=\hat{p}(y \vert x)$ 를 추정하는 model $f$ 를 학습하는 것이다. 

![image](https://user-images.githubusercontent.com/42200027/201535488-a9f75e21-aaf2-46e9-a6e3-4a70756f7873.png)

TLM 은 위의 그림과 같이 두 가지 step 으로 이뤄져 있다.
(1) General corpora 로부터 task data 를 query 로 하여 data 를 retrieve 하는 step
(2) Retrieved data 와 Task data 를 language modeling objective 와 task objective 를 이용하여 jointly optimizing 하는 step

<span style='color:green;font-weight:bold'> Retrieval From General Corpus </span>
<br>
Task data 로 부터, top-K document 를 추출한 뒤, combine 한 뒤 subset $S$를 만든다.
Subset $S$ 는 general corpus $D$의 tiny subset 이다.

저자들은 효율적인 retrieval 을 [BM25](https://www.staff.city.ac.uk/~sbrp622/papers/foundations_bm25_review.pdf) 를 활용한다.
Embedding-based dense retriever ([[13]](https://aclanthology.org/2020.emnlp-main.550/)) 을 활용하면 좋은 retrieval 결과를 얻을 수 있지만, 저자들은 최대한 simple 한 방법을 구사하기 위해 사용하지 않았다. 
Embedding-based dense retriever 은 additional computational cost 도 필요로 한다.
Retrieval performance 와 computational cost 사이의 trade-off 에 대한 연구는 future work 로 남긴다.
그리고, extremely long text 에 대한 retrieval 에서 [RAKE](https://onlinelibrary.wiley.com/doi/10.1002/9780470689646.ch1) 알고리즘같이 keyward 을 query 로 하여 retreival 하는 것이 전체 input sequence 를 query 로 하는 것보다 더 성능이 좋음을 확인한다. 
앞으로, retrieved data 인 $S$ 를 external data, text data $T$ 를 internal data 로 여긴다.

[Note] 이 방법은 task-agnostic 하다.
이 방법은 오로지 input text $x$ 에만 의존하고, label $y$ 에는 의존하지 않기 때문이다. 
그리고 retrieval procedure 역시 domain-specific data 접근을 가정하지 않는다.

<span style='color:green;font-weight:bold'> Joint Training </span>
<br> 
![image](https://user-images.githubusercontent.com/42200027/201536205-eea0de9d-626e-4fe5-a9f3-5c8bfc15aca3.png)

$L_mlm(x)$ 는 BERT 와 같은 masked language modeling loss 이고, $L_task(f(x),y)$ 는 task-specific loss function 이다.
$\rho_1$ 과 $\rho_2$ 는 hyperparameter 이다. 
Network architecture 는 BERT 와 같으며, CLS head 를 classification 으로, LM head 를 MLM 으로 사용한다.
TLM 은 BERT 외의 다른 구조로도 extend 될 수 있다.

학습은 두 stage 로 이뤄진다.
첫 번째 stage 에서, Loss 의 첫 번째줄인 $\rho_1$ batch 의 external data 학습에 두 번째줄인 1개의 batch size 의 internal data 를 끼운다. 
두 번째 stage 에서는 $\rho_1$ 과 $\rho_2$ 를 모두 0 으로하여, task-objective 를 이용하여 internal data 만을 finetuning 한다.

# Comparison between TLM and PLMs
TLM 과 PLM 의 pretraining-finetuning 모두 두 stage 를 갖는다.
사실, TLM 의 두 번째 stage 는 PLM 의 finetuning stage 와 완전히 동일하다.
두 framework 의 차이는 아래 표에서 볼 수 있다.

![image](https://user-images.githubusercontent.com/42200027/201536655-c3e55c7e-ce0a-4867-b4d0-0274981e3b5e.png)

PLM 은 task-agnostic knowledge 를 최대한 extremely high cost 를 활용해 배우지만, TLM 은 매우 적은 cost 로 task-related data 만을 학습한다.
앞으로는 TLM 의 pros and cons 를 살펴본다.

<span style='color:green;font-weight:bold'> Democratizing NLP  </span>
<br> 



<span style='color:green;font-weight:bold'> Efficiency </span>
<br> 


<span style='color:green;font-weight:bold'> Flexibility </span>
<br> 

<span style='color:green;font-weight:bold'> Generality </span>
<br> 



