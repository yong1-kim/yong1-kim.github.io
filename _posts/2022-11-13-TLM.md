---
layout: post
title:  "[ICML2022] NLP From Scratch Without Large-Scale Pretraining: A Simple and Efficient Framework"
date:   2022-11-12 17:38:00 +0900
use_math: true
categories: [Transformer, PLM]
---
[[pdf]](https://proceedings.mlr.press/v162/yao22c/yao22c.pdf)  &emsp;
[[github]](https://github.com/yaoxingcheng/TLM) <br>

**Xingcheng Yao<sup>* 1</sup>, Yanan Zheng<sup>* 2</sup>, Xiaocong Yang<sup>3 4</sup>, Zhilin Yang<sup>1 5 4</sup>**
<br><sup>*</sup>Equal Contribution, <sup>1</sup> Institute for Interdisciplinary Information Sciences, Tsinghua University, <sup>2</sup>Department of Computer Science and Technology, Tsinghua University, <sup>3</sup> School of Economics and Management, Tsinghua University, <sup>4</sup> Recuurent AI, Inc, <sup>5</sup> Shanghai Qi Zhi Institute.      &emsp; 

![image](https://user-images.githubusercontent.com/42200027/201530852-d42832d4-ee65-47d1-92bd-e127c1648c0a.png)

# Abstract
- (Motivation) Pre-trained Language Model (PLM) 이 NLP task 를 푸는 굉장히 강력한 standard 가 되었지만, train 하기에는 computation cost 가 너무 비싸다. 
- (Solution) 이 연구에서는 simple and efficient learning framework **TLM** 을 제안하여, large-scale pretraining 에 rely 하지 않는 학습 방법을 제안한다. 
- (Method) Labeled task data 와 large general corpus 에 대하여, TLM 은 task data 를 Query 로 하여 general corpus 로부터 tiny subset 을 retrieval 한 후, task objective 를 jointly optimize 한다.  
- (Result) 4개 domain 의 8개 데이터셋에 대한 실험 결과, TLM 은 PLM 과 비교하여 FLOP 은 두 자리수나 적으면서 성능은 더 좋거나 유사한 성능을 보인다. 

# Introduction
Pre-trained Language Models (PLMs) 들이 NLP 에서 큰 성공을 거두고 있다. Large general corpora 에 Masked Language Modeling (MLM; [BERT](https://aclanthology.org/N19-1423.pdf), [RoBERTa](https://arxiv.org/pdf/1907.11692.pdf), [T5](https://arxiv.org/pdf/1910.10683.pdf)), autoregressive language modeling([GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf), [GPT-3](https://arxiv.org/pdf/2005.14165.pdf)), permutation language modeling([XLNet](https://papers.nips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf)) 등의 self-supervised language modeling task 을 활용하여 pre-train 하고, 적은 양의 downstream task 에 대하여 fine-tuning 하는 PLM 은 많은 NLP task 에서 압도적인 성능을 보이고 있다. 

그러나, 이러한 PLM 들은 computationally expensive 하다. 예를 들어 [RoBERTa-Large](https://arxiv.org/abs/1907.11692) 의 경우, 4.36 x $10^21$ 이라는 엄청난 FLOPs 을 요구하며, 이는 무려 1,000 대의 32GB V100 GPU 로 하루를 계산해야하는 양이다. 더 큰 Large Language Model (LLM) 으로 가게 되면, [GPT-3](https://arxiv.org/pdf/2005.14165.pdf) 의 경우, 이 RoBERTa-Large 보다도 50배나 더 많은 계산량이 학습에 요구된다. 이러한 엄청난 계산량은 연구계, 특히 학교단위의 연구계에서 새로운 architecture 탐구나, customized LM 탐구, 개선된 pre-training loss 탐구 등의 연구를 limited budget 문제로 불가능하게 만든다. 현재 대부분의 NLP 연구자들은 fine-tuning alogrithm 을 발전시키는데 기대고 있지만, 이는 pre-training procedure 에 대개 upper-bound 될 수 밖에 없다. 

기존의 몇몇 연구들([ELECTRA](https://arxiv.org/pdf/2003.10555.pdf), [Primer](https://arxiv.org/pdf/2109.08668.pdf), [[1]](https://arxiv.org/pdf/2109.10686.pdf), [EarlyBERT](https://aclanthology.org/2021.acl-long.171.pdf)) 에서 language model pre-training 의 효율성을 개선하려는 시도가 있었지만, 대부분은 sample-efficient self-supervised task 를 제안하거나, pre-training 에 알맞는 efficient Transformer architecture 를 제안하는데 그친다. 이러한 연구들은 매우 효율적이고 도움이 되지만, FLOP 측면에서 한 자리수 정도를 줄이는데 그친다. Distillation 으로 PLM 의 size 를 줄이려는 시도들([DistilBERT](https://arxiv.org/pdf/1910.01108.pdf), [TinyBERT](https://aclanthology.org/2020.findings-emnlp.372.pdf)) 도 있었지만, 이러한 시도는 학습을 위해, 거대한 PLM training 이 필요하다는 단점이 있다. 그리고 아직까지 distilled version 의 PLM 은 RoBERTa-Large 같은 기존 PLM에 비해 성능이 많이 떨어진다.

<span style='background-color: #dcffe4'> 이 연구에서는 performance drop 없이  drastic efficiency improvement 를 갖는 완전히 새로운 pretraining-finetuning framework 를 제안한다.</span> 연구자들은 간단하고(simple), 효율적이고(efficient), **pre-training-free** framework 인 **T**ask-driven **L**anguage **M**odeling (**TLM**) 기법을 제안한다. Large general corpus 와 some labeled task data 가 주어졌을 때, TLM 은 PLM 에 의존하지 않고 model 을 from scratch 로 학습을 시작한다. TLM 은 두 가지 key idea 에서 motivate 되었다. 첫 번째로, 인간은 시험공부 벼락치기를 위해, 모든 책을 다 보지 않고 단지 몇 개의 chapter 만을 본다. 저자들은 specific 한 task 를 푸는데 있어서 large corpus 를 다 보는 것은 큰 redunduncy 가 있다고 가정한다. 두 번째로, supervised labeled data 를 직접 학습하는 것이, unlabeled data 로 부터 language modeling objective 를 최적화하는 것보다, downstream performance 에 더 효과적이다. 이러한 점들로부터, TLM 은 task data 를 query 로 하여, general corpus 의 tiny subset 을 retrieve 한다. 이후, retrieved data 와 task data를  supervised task objective 와 languge modeling objective 를 jointly optimizing 한다. 

4 개 domain - news, review, computer science, biomedical science - 의 8 개 데이터셋 (실험 세팅 : [[2]](https://aclanthology.org/2020.acl-main.740.pdf))에서, TLM 은 [BERT](https://aclanthology.org/N19-1423.pdf) 와 [RoBERTa](https://arxiv.org/pdf/1907.11692.pdf) 보다 좋거나 유사한 성능을 보이면서, 무려 **2 개 자리수(two orders of magnitude)나 적은 FLOPs** 를 사용한다. 

# Related Works
<span style='color:green;font-weight:bold'> Pre-trained Language Models </span>
<br>
BERT 이후로 많은 PLM 모델들이 등장하였고, 이들은 많은 NLP 문제들의 de-facto solution 이 되었다.
이들은 거의 대부분 pre-training 으로 large corpus 에서 contextualized token representation 을 배우고, specific task 에 labeled data 를 fine-tuning 해서 학습한다. 
BERT 는 16 G English corpora 를 MLM 을 이용해 학습하고, RoBERTa 는 BERT 와 구조가 같지만, 160G 의 English text 를 large batch size 와 dynamic token masking 등을 이용해 학습한다.
이 연구에서는 BERT 와 RoBERTa 를 baseline 으로 사용한다.

<span style='color:green;font-weight:bold'> Efficient Pretraining for NLP </span>
<br>
Languge model 의 pre-training 의 efficiency 를 향상시키기 위한 연구가 많이 있었다. [You et al.](https://arxiv.org/pdf/1904.00962.pdf) 과 [Megatron-LM](https://arxiv.org/pdf/1909.08053.pdf) 에서는 pre-training process를 가속화 하기위해, 데이터 병렬과 모델 병렬처리를 활용한다. 하지만, 병렬처리를 활용한 가속화는 FLOP 측면에서 전혀 줄어들지 않는다. EarlyBERT 와 Prier 에서는 lottery ticket hypothesis 와 Neural Architecture Search 를 이용한 efficient neural network 를 찾았다. 이는 FLOP 측면에서 50% ~ 70% 의 computational cost 를 줄였다. ELCTRA 와 [DeBERTa](https://arxiv.org/pdf/2006.03654.pdf) 는 adversarial training 과 disentagled representation of content and position 이라는 새로운 LM pre-training mechanism 을 직접 design 하여 50% ~ 75% 의 computation cost 개선을 가져왔다. [Train-no-evil](https://aclanthology.org/2020.emnlp-main.566.pdf) 에서는 selective masking 을 활용한 task-guided pre-training 으로 50% 의 computational cost reduction 을 얻었다. 
이 연구에서는 이러한 연구들과는 독립적으로(orthogonal), **training data redundancy 를 줄이는 방법** 을 통해, efficiency  를 향상시킨다. 
이 연구가 훨씬 더 drastic improvement 를 가져온다.

<span style='color:green;font-weight:bold'> Efficient Inference of Pretrained Models </span>
<br>
PLM 연구의 다른 한 줄기는 inference efficiency 를 향상시키는 방향의 연구들이다. 
[DistilBERT](https://arxiv.org/pdf/1910.01108.pdf), [TinyBERT](https://aclanthology.org/2020.findings-emnlp.372.pdf), [MobileBERT](https://arxiv.org/abs/2004.02984), [FastBERT](https://arxiv.org/abs/2004.02178), [BORT](https://arxiv.org/pdf/2010.10499.pdf), 그리고 [BERT-of-Theseus](https://aclanthology.org/2020.emnlp-main.633/) 같은 연구들에서는 **small-sized model** 을 통해 inference efficiency 를 추구한다.
[Q8-BERT](https://ieeexplore.ieee.org/document/9463531), [Q-BERT](https://ojs.aaai.org//index.php/AAAI/article/view/6409), [I-BERT](https://proceedings.mlr.press/v139/kim21d.html) 등에서는 **quantizing 기법**을 이용하여 low-precision representation 을 통해 inference 를 향상시킨다.
**Pruning 기법** 을 활용하여 small size PLM 을 inference 를 위해 사용하는 연구들([[3]](https://papers.nips.cc/paper/2019/hash/2c601ad9d2ff9bc8b282670cdd54f69f-Abstract.html), [[4]](https://aclanthology.org/2020.emnlp-main.496/), [[5]](https://aclanthology.org/2020.repl4nlp-1.18.pdf)) 도 있다.
<span style='background-color: 	#F4C2C2'> 그러나 이러한 model compression 기법을 이용한 방법들은 large PLM 에 의존할 뿐 아니라, 성능도 꽤 큰 차이로 떨어지게 된다.</span> 
이 연구에서 제시하는 방법은 PLM 에 의존하지 않을 뿐더러, 성능 역시 비슷하거나 좋아진다.

<span style='color:green;font-weight:bold'> Domain and Task Adaptation for Pretrained Models </span>
<br>
Domain-adaptive fine-tuning 은 pre-trained model 을 in-domain data 에 language modeling obejctive 로 fine-tune 하는 것이다.
이 방법은 domain/task adaptation 에서 좋은 성능이 있음이 밝혀졌다. ([[6]](https://aclanthology.org/N19-1189/), [[7]](https://aclanthology.org/2020.acl-main.740/), [[8]](https://arxiv.org/abs/2009.04984), [[9]](https://academic.oup.com/bioinformatics/article/36/4/1234/5566506)) 
TLM 과의 차이점은, TLM 은 additional domain data 를 필요로 하지 않고, 단지 BERT 와 RoBERTa 의 corpora 만 활용한다.
그리고 기존의 domain-adaptive fine-tuning 방식은 pre-trained model 을 필요로 하지만, TLM 은 그렇지 않다는 차이점이 있다. 

<span style='color:green;font-weight:bold'> Co-training for Semi-supervised Learning and DataDensity-Based Active Learning </span>
<br>
TLM 과의 유사성을 갖는 연구로 두 가지가 있다.
첫 번째는 **Co-Training (CT)**  ([[10]](https://link.springer.com/chapter/10.1007/978-3-030-01267-0_9), [[11]](https://ieeexplore.ieee.org/document/9710117)) 이고, 두 번째는 **Data-Density-Based Active Learning (DAL)** ([[12]](https://dl.acm.org/doi/10.1109/TASL.2009.2033421),[[13]](https://www.sciencedirect.com/science/article/abs/pii/S095741741730369X))이다.
CT 와 TLM 모두 unlabeled data 를 certain task 학습을 위해 활용하는 것은 같지만, 2 가지 측면에서 차이점이 있다.
첫 번째는 CT 는 unlabeled data 를 다양한 view 에서 보기 위한 여러가지 distinct model 들이 필요하지만, TLM 은 single model 을 train 한다.
두 번째로 TLM은 unlabeled data 의 selection process 가 있지만, CT 에서는 이 process 가 고려되지 않는다.

TLM 과 DAL 은 unlabeled data 에서 representative instance 를 찾는 flavor 는 동일하다.
그러나, DAL 의 경우 모든 unlabled data 가 task 의 definition 으로 label 될 수 있다는 가정이 있어야 하지만, TLM 은 그것이 필요하지 않다.
그리고, DAL 은 전체 unlabeled data 로 부터 iteratively critical instance 를 찾기 위해 노력하지만, TLM 은 labeld data 와 관련이 있는 relevant instance  를 one-shot 으로 한 번만 찾기 때문에 훨씬 효율적이다. 
따라서 TLM 이 classic DAL 알고리즘 보다 훨씬 효율적이다.

# Method
# TLM : Task-Driven Language Modeling
인간은 제한된 시간과 노력으로 빠르게 특정한 task 를 master 할 수 있는 능력을 지니고 있다. 
예를 들어, 시험 벼락치기를 할 때, 전세계의 모든 책을 보는 것이 아니라 단지 몇 개의 chapter 만을 공부하지만, 시험을 잘 볼 수 있다.
이 관찰로부터, 저자들은 **빠르고 정확하기 task-relevant information 을 locate 하는 것이 key aspect** 라고 가정한다.
결국, TLM 은 <span style='background-color: #dcffe4'> (1) general corpora 로부터 relevant training data 를 automatically retreive 하고, (2) retrieved data 와 task data 를 결합하여 학습한다.</span>

수식적으로 보면, general corpus $D = \lbrace d_i \rbrace_i$ where $d_i$ is document, labled task data, $T = {(x_i,y_i)}_i$ where $x_i$ is text and $y_i \in Y$ is a label 에 대해, 목표는 coniditional probability for classification $f(x)=\hat{p}(y \vert x)$ 를 추정하는 model $f$ 를 학습하는 것이다. 

![image](https://user-images.githubusercontent.com/42200027/201535488-a9f75e21-aaf2-46e9-a6e3-4a70756f7873.png)

TLM 은 위의 그림과 같이 두 가지 step 으로 이뤄져 있다.
(1) General corpora 로부터 task data 를 query 로 하여 data 를 retrieve 하는 step
(2) Retrieved data 와 Task data 를 language modeling objective 와 task objective 를 이용하여 jointly optimizing 하는 step

<span style='color:green;font-weight:bold'> Retrieval From General Corpus </span>
<br>
Task data 로 부터, top-K document 를 추출한 뒤, combine 한 뒤 subset $S$를 만든다.
Subset $S$ 는 general corpus $D$의 tiny subset 이다.

저자들은 효율적인 retrieval 을 [BM25](https://www.staff.city.ac.uk/~sbrp622/papers/foundations_bm25_review.pdf) 를 활용한다.
Embedding-based dense retriever ([[13]](https://aclanthology.org/2020.emnlp-main.550/)) 을 활용하면 좋은 retrieval 결과를 얻을 수 있지만, 저자들은 최대한 simple 한 방법을 구사하기 위해 사용하지 않았다. 
Embedding-based dense retriever 은 additional computational cost 도 필요로 한다.
Retrieval performance 와 computational cost 사이의 trade-off 에 대한 연구는 future work 로 남긴다.
그리고, extremely long text 에 대한 retrieval 에서 [RAKE](https://onlinelibrary.wiley.com/doi/10.1002/9780470689646.ch1) 알고리즘같이 keyward 을 query 로 하여 retreival 하는 것이 전체 input sequence 를 query 로 하는 것보다 더 성능이 좋음을 확인한다. 
앞으로, retrieved data 인 $S$ 를 external data, text data $T$ 를 internal data 로 여긴다.

[Note] 이 방법은 task-agnostic 하다.
이 방법은 오로지 input text $x$ 에만 의존하고, label $y$ 에는 의존하지 않기 때문이다. 
그리고 retrieval procedure 역시 domain-specific data 접근을 가정하지 않는다.

<span style='color:green;font-weight:bold'> Joint Training </span>
<br> 
![image](https://user-images.githubusercontent.com/42200027/201536205-eea0de9d-626e-4fe5-a9f3-5c8bfc15aca3.png)

$L_mlm(x)$ 는 BERT 와 같은 masked language modeling loss 이고, $L_task(f(x),y)$ 는 task-specific loss function 이다.
$\rho_1$ 과 $\rho_2$ 는 hyperparameter 이다. 
Network architecture 는 BERT 와 같으며, CLS head 를 classification 으로, LM head 를 MLM 으로 사용한다.
TLM 은 BERT 외의 다른 구조로도 extend 될 수 있다.

학습은 두 stage 로 이뤄진다.
첫 번째 stage 에서, Loss 의 첫 번째줄인 $\rho_1$ batch 의 external data 학습에 두 번째줄인 1개의 batch size 의 internal data 를 끼운다. 
두 번째 stage 에서는 $\rho_1$ 과 $\rho_2$ 를 모두 0 으로하여, task-objective 를 이용하여 internal data 만을 finetuning 한다.

# Comparison between TLM and PLMs
TLM 과 PLM 의 pretraining-finetuning 모두 두 stage 를 갖는다.
사실, TLM 의 두 번째 stage 는 PLM 의 finetuning stage 와 완전히 동일하다.
두 framework 의 차이는 아래 표에서 볼 수 있다.

![image](https://user-images.githubusercontent.com/42200027/201536655-c3e55c7e-ce0a-4867-b4d0-0274981e3b5e.png)

PLM 은 task-agnostic knowledge 를 최대한 extremely high cost 를 활용해 배우지만, TLM 은 매우 적은 cost 로 task-related data 만을 학습한다.
앞으로는 TLM 의 pros and cons 를 살펴본다.

<span style='color:green;font-weight:bold'> Democratizing NLP  </span>
<br> 
기존의 Pretraining-finetuning paradigm 에서, fine-tuning performance 는 pertrained model 에 largely upper-bound 되어 있었다.
그러나 대부분의 NLP 연구자들은, computation resource 의 한계로, large-scale LM 을 training 하려는 엄두조차 낼 수 없었고, fine-tuning 알고리즘을 손보는 것에 기댈 수 밖 에 없었다. 
PLM 의 디자인 choice 나 pre-training loss 같은 것에 대한 연구는 소수의 연구자들에게만 주어진 혜택이었다. 
이러한 점은  PLM 의 연구 및 발전에 대해 속도를 저하시키는 위해 요소가 될 수 있다.
<span style='background-color: #dcffe4'> 이러한 점에서 TLM 은 NLP 를 민주화(Democratizing) 하고, 많은 연구자로부터 LM architecture, loss function, 알고리즘 등 LM 연구를 가속화 시킬 수 있다. </span>

<span style='color:green;font-weight:bold'> Efficiency </span>
<br> 
TLM 은 per-task FLOPs 측면에서 PLMs 을 압도적으로 상회한다.
대부분의 경우에서, target task 는 몇 개 없기 때문에 (few), TLM 은 cost 측면에서 선호된다.
예를 들어, 4 개의 NLI task 를 푼다던지, 하나의 추천 시스템을 푸는 경우는 TLM 의 선택이 reasonable 하다.
하지만, 1,000 개 task 를 푼다고 한다면 (회사에서 NLP platform 을 build 하는 경우 등) PLM 이 아직 더 효과적일 것이다.

<span style='color:green;font-weight:bold'> Flexibility </span>
<br> 
TLM 은 task-driven 이기 때문에, flexibility 가 높다.
연구자들은 tokenization, sequence length, data representation, hyper parameter tuning 등에서 custom strategy 를 활용할 수 있다.

<span style='color:green;font-weight:bold'> Generality </span>
<br> 
<span style='background-color: 	#F4C2C2'> TLM 은 Efficiency 와 Generality 에서 큰 trade-off 가 발생한다. </span>
PLM 은 task-agnostic general representation 을 배울 수 있지만, TLM 은 오로지 하나의 task-specific representation 만을 배울 수 있다. 
TLM 의 generality 를 증가시키는 연구는 future work 이다.
저자들은 multi-task learning 이 돌파구가 될 것이라고 예상하고 있다.

# Experiments 
실험 setting 은 [Gururangan et al.](https://aclanthology.org/2020.acl-main.740/) 을 따라간다.
<span style='color:green;font-weight:bold'> Datasets </span>
<br> 
4 개 domain 의 8개 dataset 에 대하여 실험한다.
High-resource data 는 5K 이상의 task data 로, AGNews, IMDB, RCT, 그리고 Helpfulness 이다.
Low-resource data 는 ChemProt, ACL-ARC, SciERC, HyperPartisan 이다. 
General corpora 로는 BERT 의 training corpora 와 RoBERTa 의 training Corpora 를 활용한다.

<span style='color:green;font-weight:bold'> Baselines </span>
<br> 
Baseline 은 BERT 와 RoBERTa 이다. 
각각 base scale 과 large scale 를 활용한다.
TLM 은 number of total training token (products of training step, batch size, sequence length) 을 기준으로 *small, medium, large* 세 버전을 활용한다.
이 버전은 BERT 와 RoBERTa 의 버전들과 computation cost 가 동일하다.

<span style='color:green;font-weight:bold'> Main Results </span>
<br> 

![image](https://user-images.githubusercontent.com/42200027/201537554-5c5aef60-5cee-4876-8016-bdf3e0327c6c.png)

TLM 은 training data size 와 computation cost 를 엄청나게 줄이면서도 유사하거나 더 좋은 성능을 보인다.
특별히, *small* scale 에서, TLM 은 BERT-Large 보다 1/33 의 FLOPs 과 1/16 의 training corpus 만 사용하고 유사한 성능을 보였다.
*medium* 과 *large* scale 에서, TLM 은 0.59, 0.24 point 더 좋은 성능을 보였지만, FLOPs 과 training size 에서 두 자리수(two order)  나 적은 cost 를 사용하였다. 
<span style='background-color: #dcffe4'> 결과적으로, TLM 이 highly accurate and much more efficient than PLM 이라고 할 수 있다.</span>
특히, *large* scale 에서 이러한 점이 더 두드러지는데, 저자들은 large scale PLM 들이 general knowledge 를 너무 많이 학습하여, speficic task 에 대해서 useful 하지 않다고 말한다.

<span style='color:green;font-weight:bold'> Ablation Study </span>
<br> 

![image](https://user-images.githubusercontent.com/42200027/201537740-0220c2ff-47f1-4960-a94d-fd5d45a2f079.png)

위의 표는 BM25 와 random retrieval 같은 retrieval method 에 대한 비교 결과와 general corpus 의 size 에 대한 비교 결과이다. 
같은 general corpora 에 대해서 BM25 가 가장 좋은 성능을 낸 것을 볼 수 있다. 
특별히, BM 25 가 IMDB 에서 random retrieval 보다 1 점 더 좋은 성능을, 나머지 두 low-resource data 에서는 3~4 점이 더 좋은 점수를 보였다.
Low-resource data 일 수록 external data 에 더 rely 한다는 저자들의 intuition 이 드러난다고 한다.

General corpora size 비교를 위해 BERT corpora 와 RoBERTa corpora 를 보면, 세 데이터셋에서 모두, general corpora 가 클 때 (RoBERTa corpora 일 때) 성능이 향상되었다. 
이 gain (10 배 corpora 에 대한 1점정도의 향상) 은 PLM 에서의 발견과 유사하다. 
이 결과를 통해, efficiency 를 유지하면서도, larger general corpora 를 통해 더 높은 성능을 얻을 수 있는 가능성을 볼 수 있다.
반대로, random retrieval 에서는 이러한 효과를 볼 수 없어서, corpus size 에 sensitive 하지 않다는 것을 알 수 있다.

![image](https://user-images.githubusercontent.com/42200027/201538158-7d13f7c2-8297-4c68-9f3b-394e587ac6e6.png)

Top-K retrieval 에서 K 에 대한 실험은 위의 표에서 볼 수 있다.
High-resource data 인 AGNews 에서는 K 값에 크게 상관이 없었지만, Low-resource 에서는 K 값이 크면 클 수록 좋은 결과를 얻었다.
따라서, 저자들의 intuituion 이었던, low-resource task 는 joint training 을 통한 external data 에 더 의존한다는 것을 실험적으로 증명할 수 있었다.

<span style='color:green;font-weight:bold'> LANGUAGE MODELING WEIGHTS $\rho_1$ AND $\rho_2$ </span>
<br> 
![image](https://user-images.githubusercontent.com/42200027/201538303-cead1665-00a4-4893-a660-9c5f35b50244.png)

먼저, $\rho_1$ 에 대해서 보면, high-resource 인 Helpfulness 같은 경우, smaller $\rho_1$ 에서, low-resource task 인 SciERC 나 ChemProt 에서는 higher $\rho_1$ 에서 좋은 결과가 있었다. 
이는 low-resource 는 external data 에 대한 의존도가 크다는 이전 결과와 유사한 해석을 할 수 있다.
그리고 task data 를 사용하지 않고 external data 만을 활용했을 때는, 좋지 않은 성능이 나왔으며, small task data 의 필수불가결성을 확인할 수 있었다.

아래 표에서는 language modeling 이 필수적임을 확인할 수 있다. 
$\rho_2$ 가 0 이 아닐 때, (정확히는 20 혹은 100 일 때 ) 가장 좋은 성능을 보인다.

<span style='color:green;font-weight:bold'> Seoncd stage of training </span>
<br> 
![image](https://user-images.githubusercontent.com/42200027/201538509-31a5df1e-ee8f-4758-9231-a6672140bb2d.png)

위의 표에서 볼 수 있듯이, two-stage training 일 때 성능이 좋았다.
Second stage 를 제거하면 최종 성능이 지속적으로 나빠졌고, second stage 가 필수불가결(indispensability) 하다는 것을 알 수 있다.
Low-resource 에서는 특히나 second stage 가 큰 영향력을 미친다. 

<span style='color:green;font-weight:bold'> MLM loss on task data </span>
<br> 
![image](https://user-images.githubusercontent.com/42200027/201538570-a5a20346-25e0-4805-ab16-8124018757a4.png)

First stage 에서 TLM 은 masekd langugae modeling loss 를 task data 에 활용한다.
이 것이 영향이 있는지 확인하기 위해, PLM 에 task data 에 대한 MLM 을 추가하였을 때, 위의 표에서 보듯이 큰 영향이 없는 것을 볼 수 있다.
저자들은 task data MLM 보다 TLM 의 relevant retrieved data 의 MLM 이 PLM 의 general corpora MLM 보다 더 좋다고 이야기하고 있다.

# Analysis
<span style='color:green;font-weight:bold'> Attention weight visualization </span>
<br> 
![image](https://user-images.githubusercontent.com/42200027/201538716-7724a060-a58d-44ce-a4a3-6ed0abc24236.png)

저자들은 TLM 과 PLM (pretraining-finetuning framework) model 의 behavior differnce 를 attention weight visualization 으로 본다.
[Voita et al.](https://aclanthology.org/P19-1580.pdf)은 최소한 90% 의 maximum attention wieght 이 인접한(adjacent) token 에 assign 되어있는 "positional head" 같은 specific kind of head 가 final prediction 에 지대한 영향을 끼친다는 것을 보였다.
또 다른 중요한 head 는 [CLS], [SEP], period token ('.') 에 maximum attnetion weight 이 부여되어 있는 head 가 적은 semantic/syntatic information 을 enocde 할 가능성이 있다고 말한다.
이러한 head 를 "vertical head" 라고 명명한다.
위 그림에서, TLM 에서 더 많은 "positional head" 가 발견되고, 더 적은 "vertical head" 가 발견된다. 
이는 다른 task 에서도 똑같이 관측이 되는데, TLM 이 PLM 과는 다른 pattern 의 attention 을 학습하며, 저자들의 주장으로는 more informative 한 attention 을 배운다고 주장한다.

<span style='color:green;font-weight:bold'> Case study on retrieved data </span>
<br> 
![image](https://user-images.githubusercontent.com/42200027/201538860-49b1fdf4-3815-4d50-8ffd-2f8fa961c7fe.png)

Case study 는 위의 표에서 볼 수 있다.
BM25 는 sparse feature 에 기반하므로, semantic 유사도 보다는 lexcial 유사도에 더 focus 되어 있다.
이는 특정 noun 이 많이 발견되는 professional domain 에 더욱 beneficial 하다. (ex. SciERC for Computer sciecne, ChemPort for biomedical science)
이런 professional domain 외의 domain 에서도 BM25 가 잘 하는 것을 볼 수 있다.

<span style='color:green;font-weight:bold'> Results on More Datasets </span>
<br> 
![image](https://user-images.githubusercontent.com/42200027/201538891-d8933502-e34f-4149-8477-bcbe21c2e03a.png)

지금까지는 [Gururangan et al.](https://aclanthology.org/2020.acl-main.740/) 의 실험세팅을 따라했으나, 더 많은 실험결과를 위해 BERT 에서 사용한 [GLUE](https://aclanthology.org/W18-5446.pdf) benchmark 에 대해서 실험을 진행한다. 
위의 실험에서 *small* scale setting 에서 실험을 진행했을 때, cost 는 압도적으로 줄이면서 BERT-base 와 거의 모든 benchmark 에서 유사한 성능을 보였다. 

