---
layout: post
title:  "[ICML2022] NLP From Scratch Without Large-Scale Pretraining: A Simple and Efficient Framework"
date:   2022-11-12 17:38:00 +0900
use_math: true
categories: [Transformer, PLM]
---
[[pdf]](https://proceedings.mlr.press/v162/yao22c/yao22c.pdf)  &emsp;
[[github]](https://github.com/yaoxingcheng/TLM) <br>

**Xingcheng Yao<sup>* 1</sup>, Yanan Zheng<sup>* 2</sup>, Xiaocong Yang<sup>3 4</sup>, Zhilin Yang<sup>1 5 4</sup>**
<br><sup>*</sup>Equal Contribution, <sup>1</sup> Institute for Interdisciplinary Information Sciences, Tsinghua University, <sup>2</sup>Department of Computer Science and Technology, Tsinghua University, <sup>3</sup> School of Economics and Management, Tsinghua University, <sup>4</sup> Recuurent AI, Inc, <sup>5</sup> Shanghai Qi Zhi Institute.      &emsp; 

![image](https://user-images.githubusercontent.com/42200027/201530852-d42832d4-ee65-47d1-92bd-e127c1648c0a.png)

# Abstract
- (Motivation) Pre-trained Language Model (PLM) 이 NLP task 를 푸는 굉장히 강력한 standard 가 되었지만, train 하기에는 computation cost 가 너무 비싸다. 
- (Solution) 이 연구에서는 simple and efficient learning framework **TLM** 을 제안하여, large-scale pretraining 에 rely 하지 않는 학습 방법을 제안한다. 
- (Method) Labeled task data 와 large general corpus 에 대하여, TLM 은 task data 를 Query 로 하여 general corpus 로부터 tiny subset 을 retrieval 한 후, task objective 를 jointly optimize 한다.  
- (Result) 4개 domain 의 8개 데이터셋에 대한 실험 결과, TLM 은 PLM 과 비교하여 FLOP 은 두 자리수나 적으면서 성능은 더 좋거나 유사한 성능을 보인다. 

# Introduction
Pre-trained Language Models (PLMs) 들이 NLP 에서 큰 성공을 거두고 있다. Large general corpora 에 Masked Language Modeling (MLM; [BERT](https://aclanthology.org/N19-1423.pdf), [RoBERTa](https://arxiv.org/pdf/1907.11692.pdf), [T5](https://arxiv.org/pdf/1910.10683.pdf)), autoregressive language modeling([GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf), [GPT-3](https://arxiv.org/pdf/2005.14165.pdf)), permutation language modeling([XLNet](https://papers.nips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf)) 등의 self-supervised language modeling task 을 활용하여 pre-train 하고, 적은 양의 downstream task 에 대하여 fine-tuning 하는 PLM 은 많은 NLP task 에서 압도적인 성능을 보이고 있다. 

그러나, 이러한 PLM 들은 computationally expensive 하다. 예를 들어 [RoBERTa-Large](https://arxiv.org/abs/1907.11692) 의 경우, 4.36 x $10^21$ 이라는 엄청난 FLOPs 을 요구하며, 이는 무려 1,000 대의 32GB V100 GPU 로 하루를 계산해야하는 양이다. 더 큰 Large Language Model (LLM) 으로 가게 되면, [GPT-3](https://arxiv.org/pdf/2005.14165.pdf) 의 경우, 이 RoBERTa-Large 보다도 50배나 더 많은 계산량이 학습에 요구된다. 이러한 엄청난 계산량은 연구계, 특히 학교단위의 연구계에서 새로운 architecture 탐구나, customized LM 탐구, 개선된 pre-training loss 탐구 등의 연구를 limited budget 문제로 불가능하게 만든다. 현재 대부분의 NLP 연구자들은 fine-tuning alogrithm 을 발전시키는데 기대고 있지만, 이는 pre-training procedure 에 대개 upper-bound 될 수 밖에 없다. 

기존의 몇몇 연구들([ELECTRA](https://arxiv.org/pdf/2003.10555.pdf), [Primer](https://arxiv.org/pdf/2109.08668.pdf), [[1]](https://arxiv.org/pdf/2109.10686.pdf), [EarlyBERT](https://aclanthology.org/2021.acl-long.171.pdf)) 에서 language model pre-training 의 효율성을 개선하려는 시도가 있었지만, 대부분은 sample-efficient self-supervised task 를 제안하거나, pre-training 에 알맞는 efficient Transformer architecture 를 제안하는데 그친다. 이러한 연구들은 매우 효율적이고 도움이 되지만, FLOP 측면에서 한 자리수 정도를 줄이는데 그친다. Distillation 으로 PLM 의 size 를 줄이려는 시도들([DistilBERT](https://arxiv.org/pdf/1910.01108.pdf), [TinyBERT](https://aclanthology.org/2020.findings-emnlp.372.pdf)) 도 있었지만, 이러한 시도는 학습을 위해, 거대한 PLM training 이 필요하다는 단점이 있다. 그리고 아직까지 distilled version 의 PLM 은 RoBERTa-Large 같은 기존 PLM에 비해 성능이 많이 떨어진다.

<span style='background-color: #dcffe4'> 이 연구에서는 performance drop 없이  drastic efficiency improvement 를 갖는 완전히 새로운 pretraining-finetuning framework 를 제안한다.</span> 연구자들은 간단하고(simple), 효율적이고(efficient), **pre-training-free** framework 인 **T**ask-driven **L**anguage **M**odeling (**TLM**) 기법을 제안한다. Large general corpus 와 some labeled task data 가 주어졌을 때, TLM 은 PLM 에 의존하지 않고 model 을 from scratch 로 학습을 시작한다. TLM 은 두 가지 key idea 에서 motivate 되었다. 첫 번째로, 인간은 시험공부 벼락치기를 위해, 모든 책을 다 보지 않고 단지 몇 개의 chapter 만을 본다. 저자들은 specific 한 task 를 푸는데 있어서 large corpus 를 다 보는 것은 큰 redunduncy 가 있다고 가정한다. 두 번째로, supervised labeled data 를 직접 학습하는 것이, unlabeled data 로 부터 language modeling objective 를 최적화하는 것보다, downstream performance 에 더 효과적이다. 이러한 점들로부터, TLM 은 task data 를 query 로 하여, general corpus 의 tiny subset 을 retrieve 한다. 이후, retrieved data 와 task data를  supervised task objective 와 languge modeling objective 를 jointly optimizing 한다. 

4 개 domain - news, review, computer science, biomedical science - 의 8 개 데이터셋 (실험 세팅 : [[2]](https://aclanthology.org/2020.acl-main.740.pdf))에서, TLM 은 [BERT](https://aclanthology.org/N19-1423.pdf) 와 [RoBERTa](https://arxiv.org/pdf/1907.11692.pdf) 보다 좋거나 유사한 성능을 보이면서, 무려 **2 개 자리수(two orders of magnitude)나 적은 FLOPs** 를 사용한다. 

# Related Works
<span style='color:green;font-weight:bold'> Pre-trained Language Models </span>
<br>
BERT 이후로 많은 PLM 모델들이 등장하였고, 이들은 많은 NLP 문제들의 de-facto solution 이 되었다.
이들은 거의 대부분 pre-training 으로 large corpus 에서 contextualized token representation 을 배우고, specific task 에 labeled data 를 fine-tuning 해서 학습한다. 
BERT 는 16 G English corpora 를 MLM 을 이용해 학습하고, RoBERTa 는 BERT 와 구조가 같지만, 160G 의 English text 를 large batch size 와 dynamic token masking 등을 이용해 학습한다.
이 연구에서는 BERT 와 RoBERTa 를 baseline 으로 사용한다.

<span style='color:green;font-weight:bold'> Efficient Pretraining for NLP </span>
<br>
Languge model 의 pre-training 의 efficiency 를 향상시키기 위한 연구가 많이 있었다. [You et al.](https://arxiv.org/pdf/1904.00962.pdf) 과 [Megatron-LM](https://arxiv.org/pdf/1909.08053.pdf) 에서는 pre-training process를 가속화 하기위해, 데이터 병렬과 모델 병렬처리를 활용한다. 하지만, 병렬처리를 활용한 가속화는 FLOP 측면에서 전혀 줄어들지 않는다. EarlyBERT 와 Prier 에서는 lottery ticket hypothesis 와 Neural Architecture Search 를 이용한 efficient neural network 를 찾았다. 이는 FLOP 측면에서 50% ~ 70% 의 computational cost 를 줄였다. ELCTRA 와 [DeBERTa](https://arxiv.org/pdf/2006.03654.pdf) 는 adversarial training 과 disentagled representation of content and position 이라는 새로운 LM pre-training mechanism 을 직접 design 하여 50% ~ 75% 의 computation cost 개선을 가져왔다. [Train-no-evil](https://aclanthology.org/2020.emnlp-main.566.pdf) 에서는 selective masking 을 활용한 task-guided pre-training 으로 50% 의 computational cost reduction 을 얻었다. 
이 연구에서는 이러한 연구들과는 독립적으로(orthogonal), **training data redundancy 를 줄이는 방법** 을 통해, efficiency  를 향상시킨다. 
이 연구가 훨씬 더 drastic improvement 를 가져온다.

<span style='color:green;font-weight:bold'> Efficient Inference of Pretrained Models </span>
<br>
PLM 연구의 다른 한 줄기는 inference efficiency 를 향상시키는 방향의 연구들이다. 
[DistilBERT](https://arxiv.org/pdf/1910.01108.pdf), [TinyBERT](https://aclanthology.org/2020.findings-emnlp.372.pdf), [MobileBERT](https://arxiv.org/abs/2004.02984), [FastBERT](https://arxiv.org/abs/2004.02178), [BORT](https://arxiv.org/pdf/2010.10499.pdf), 그리고 [BERT-of-Theseus](https://aclanthology.org/2020.emnlp-main.633/) 같은 연구들에서는 **small-sized model** 을 통해 inference efficiency 를 추구한다.
[Q8-BERT](https://ieeexplore.ieee.org/document/9463531), [Q-BERT](https://ojs.aaai.org//index.php/AAAI/article/view/6409), [I-BERT](https://proceedings.mlr.press/v139/kim21d.html) 등에서는 **quantizing 기법**을 이용하여 low-precision representation 을 통해 inference 를 향상시킨다.
**Pruning 기법** 을 활용하여 small size PLM 을 inference 를 위해 사용하는 연구들([[3]](https://papers.nips.cc/paper/2019/hash/2c601ad9d2ff9bc8b282670cdd54f69f-Abstract.html), [[4]](https://aclanthology.org/2020.emnlp-main.496/), [[5]](https://aclanthology.org/2020.repl4nlp-1.18.pdf)) 도 있다.
<span style='background-color: red'> 그러나 이러한 model compression 기법을 이용한 방법들은 large PLM 에 의존할 뿐 아니라, 성능도 꽤 큰 차이로 떨어지게 된다.</span> 
이 연구에서 제시하는 방법은 PLM 에 의존하지 않을 뿐더러, 성능 역시 비슷하거나 좋아진다.

<span style='color:green;font-weight:bold'> Domain and Task Adaptation for Pretrained Models </span>
<br>
Domain-adaptive fine-tuning 은 pre-trained model 을 in-domain data 에 language modeling obejctive 로 fine-tune 하는 것이다.
이 방법은 domain/task adaptation 에서 좋은 성능이 있음이 밝혀졌다. ([[6]](https://aclanthology.org/N19-1189/), [[7]](https://aclanthology.org/2020.acl-main.740/), [[8]](https://arxiv.org/abs/2009.04984), [[9]](https://academic.oup.com/bioinformatics/article/36/4/1234/5566506)) 
TLM 과의 차이점은, TLM 은 additional domain data 를 필요로 하지 않고, 단지 BERT 와 RoBERTa 의 corpora 만 활용한다.
그리고 기존의 domain-adaptive fine-tuning 방식은 pre-trained model 을 필요로 하지만, TLM 은 그렇지 않다는 차이점이 있다. 

<span style='color:green;font-weight:bold'> Co-training for Semi-supervised Learning and DataDensity-Based Active Learning </span>
<br>
TLM 과의 유사성을 갖는 연구로 두 가지가 있다.
첫 번째는 **Co-Training (CT)**  ([[10]](https://link.springer.com/chapter/10.1007/978-3-030-01267-0_9), [[11]](https://ieeexplore.ieee.org/document/9710117)) 이고, 두 번째는 **Data-Density-Based Active Learning (DAL)** ([[12]](https://dl.acm.org/doi/10.1109/TASL.2009.2033421),[[13]](https://www.sciencedirect.com/science/article/abs/pii/S095741741730369X))이다.
CT 와 TLM 모두 unlabeled data 를 certain task 학습을 위해 활용하는 것은 같지만, 2 가지 측면에서 차이점이 있다.
첫 번째는 CT 는 unlabeled data 를 다양한 view 에서 보기 위한 여러가지 distinct model 들이 필요하지만, TLM 은 single model 을 train 한다.
두 번째로 TLM은 unlabeled data 의 selection process 가 있지만, CT 에서는 이 process 가 고려되지 않는다.

TLM 과 DAL 은 unlabeled data 에서 representative instance 를 찾는 flavor 는 동일하다.
그러나, DAL 의 경우 모든 unlabled data 가 task 의 definition 으로 label 될 수 있다는 가정이 있어야 하지만, TLM 은 그것이 필요하지 않다.
그리고, DAL 은 전체 unlabeled data 로 부터 iteratively critical instance 를 찾기 위해 노력하지만, TLM 은 labeld data 와 관련이 있는 relevant instance  를 one-shot 으로 한 번만 찾기 때문에 훨씬 효율적이다. 
따라서 TLM 이 classic DAL 알고리즘 보다 훨씬 효율적이다.

# Method

