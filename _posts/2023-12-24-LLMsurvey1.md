---
layout: post
title:  "A Survey of Large Language Models (1)"
date:   2023-12-24 10:45:00 +0900
use_math: true
categories: [Transformer, PLM, LLM]
---

[[pdf]](https://arxiv.org/pdf/2303.18223.pdf)
[[github]](https://github.com/RUCAIBox/LLMSurvey)

**Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie and Ji-Rong Wen**

<span style='background-color: #dcffe4'> 이 글은 Large Language Model (LLM) 의 survey 논문으로 cited paper 의 link 는 생략한다. </span>


## Abstract
  
- LLM:Large Langauge Model 은 tens or hundreds of billions of params 를 가지는 언어모델로, in-context learning 등의 몇몇 special ability 를 보인다는 측면에서 PLM:Pre-trained Langauge Model 과 차이를 보인다.
- 이 연구에서는 최신 LLM 연구를 **pre-training, adaptation tuning, utilization, capacity evaluation** 네 가지 측면에서 조사한 survey 논문이다.
 
## 1. Introduction

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/e11e045f-e2b7-4622-bbba-23326484827c)


Machine 에게 있어 인간이 comminucate 하는 것과 유사하게, read, write 하는 기술을 갖게하는 것은 오랜 목표이다.

Langauge Modeling (LM) 은 machine 에게 언어 지능을 가르치는 major 한 방법으로, 다음 단어의 확률을 예측하도록 generative likelihood 를 model 하도록 학습시킨다.
LM 의 연구분야는 시대에 따라 크게 네 가지로 나뉜다.
- Statistical Language Models (SLM) : n-gram 기반으로 markov assumption 으로 word prediction 을 진행하는 통계적 방식이다. curse of dimension 문제가 발생한다.
- Neural Language Models (NLM) : MLP:multi-layer perceptron 이나 RNN:Recurrent Neural Network 등을 활용하여 word prob 을 예측한다. **NLP 연구의 매우 중요한 impact 를 가져온 연구들**이다.
- Pre-trained Language Models (PLM) : "pre-training" and "fine-tuning" 패러다임. ELMO, BERT, GPT-2, BART 등.
- Large Langauge Models (LLM) : [Scaling Law](https://arxiv.org/abs/2001.08361) 논문을 기반으로 PLM 의 성능이 scale 이 커짐에 따라 좋아진다는 연구가 있었다. 175-B GPT-3, 540-B PaLM 등이 그것인데, 이들은 성능이 그저 좋아지는 점을 넘어, complex task 를 푸는 special ability 를 보인다 (*in-context learning 등*)

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/070d3e95-d14a-49fb-b757-2ac501d835fc)

특히나, 위의 그림에서 보는 것처럼 <span style='color:green;font-weight:bold'> chatGPT </span> 의 등장 이후 LLM 연구가 매우 활발하다.
LLM 연구는 기존의 text data 를 model 하고 generate 하는 연구와 다르게, <span style='background-color: #dcffe4'>  complext task solving 을 하는데 치중되어 있다. (From langauge modeling to task sloving) </span>

<span style='color:green;font-weight:bold'> Differences between LLM and PLM </span>
<br>
LLM 은 PLM 과 비교하여 크게 아래 세 가지의 차이점을 보인다.
- LLM 은 PLM 에 비교하여 전례없는 powerful 한 성능을 보인다. (특히 complex task 에서)
- GPT-4 API 처럼 prompting interface 를 통해 인간이 AI 시스템을 사용하는데 혁명을 불러왔다.
- 압도적인 크기로 인해, research 와 engineering 의 영역을 무너뜨렸다.

<span style='color:green;font-weight:bold'> LLM 의 단점 </span>
<br>
그러나 이러한 LLM 의 underlying principle 은 여전히 explored 되지 않았다. LLM 이 PLM 보다 압도적인 성능을 언제부터, 어떻게 내어놓는지에 대한 연구가 더 필요하다. 그리고, LLM 은 압도적인 크기로 인해 몇몇 industry 에서만 활용 가능하며, data collection and cleaning 등의 중요한 training detail 은 공개되지 않는다. 마지막으로 <span style='background-color: #dcffe4'> LLM 은 toxic, fictitious, harmful content 를 생성한다.  </span>

<span style='color:green;font-weight:bold'> LLM 연구의 필요성 </span>
<br>
따라서 이러한 문제를 극복하기 위하여 LLM 에 대한 더욱 깊은 연구가 필요하다.
이 survey 에서는 네 가지 관점에서 연구들을 정리한다.
- pre-training (how to pretrain a capable LLM)
- adaptation (how to effectively adapt pre-trained LLMs for better use)
- utilization (how to use LLMs for solving various downstream tasks)
- capability evaluation (how to evaluate the abilities of LLMs and existing empirical findings)

이후 추가적으로, **some useful prompt design**, **LLM application in specific-domain** 등을 다룬다.

## 2. Overview

# 2.1. Background for LLMs
<span style='color:green;font-weight:bold'> (1) Scaling Laws for LLMs </span>
<br>
LLM 은 기본적으로 Transformer 를 기반으로 하지만, model size, data size, total computation cost 등에서 매우 압도적으로 크다. 여러 연구에서 scaling 이 model 의 capacity 를 키운다는 것을 발견했다. 여기서는 두 가지 scaling law 를 소개한다.
- KM scaling law

2020 년 OpenAI 팀의 Kaplan et al. 은 model size (N), dataset size (D), amount of training compute (C) 에 대해, 다음 세 가지 scaling law 를 보였다.
  ![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/5d3d75ab-fd4a-492b-8a49-4411f17b05b8)

이들은, model performance 가 세 가지 factor 에 strong dependence 를 갖는 것을 보였다. 이후, follow-up 연구에서 OpenAI 팀은 LM loss 를 두 가지로 구분하였는데, 하나는 *irreducible loss*(the entropy of the trud data distribution) 이고, 다른 하나는 *reducible loss*(an estimate of the KL divergence between the true and model distributions) 이다.

- Chinchilla scaling law

Google DeepMind team 의 Hoffmann et al. 은 다른 형태의 scaling law 를 제안하였다. 여러 모델 사이즈와 여러 데이터 사이즈를 통해 아래의 식을 경험적으로 찾아낸다. 

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/1f2a99d0-5a8a-4334-ace1-2bdda103d089)

Compute budget 이 커질 떄, KM scaling law 는 모델 사이즈를 키우는 것을 더 favor 하는 반면, Chinchilla scaling law 는 모델 사이즈와 데이터셋 사이즈 모두 equal scale 로 올려야 한다고 주장한다.

<span style='color:green;font-weight:bold'>  (2) Discussion on Scaling Laws </span>
<br>
Scaling law 를 두 가지 측면에서 분석할 수 있다.
- Predictable Scaling.

<span style='background-color: #dcffe4'> Scaling law 를 기반으로 smaller model 을 기반으로 larget model 의 performance 를 estimate 하는 것이 feasible 하다고 볼 수 있다. </span>
너무 큰 모델은 그 성능을 측정하는 것조차 버거울 수 있는데, small model 로 부터 얻은 경험이 적용이 될 수 있다는 점은 매우 큰 장점이 된다.
그리고, LLM 을 학습시킬 때, training spike 같은 abnormal performance 가 있을 수 있는데, scaling law 는 가 LLM 의 training status 를 monitor 하는데 employ 될 수 있다는 장점이 된다.
또한, 모델의 크기가 커짐에 따라, LLM 학습을 위한 public dataset 이 "exhausted" 될 수 있으며, data scarcity 해결을 위한 data augmentation 기법이 매우 필요함을 의미하기도 한다.

- Task-level Predictability.

LLM 의 scaling law 는 LM loss 에 치중되어 있다.
그러나 실제로 LM loss 의 감소가 모델의 performance 증가와 같은 말을 의미한다고 볼 수는 없다. 
GPT-4 는 coding ability 를 포함한 몇몇 capability 의 경우, scaling law 에 따라 정확하게 예측할 수 있다고 한다.
그러나 많은 경우 *inverse scaling* 이라 불리는 현상이 있으며, 이는 LM loss 가 감소함에도 task performacne 는 오히려 나빠지는 경우이다. 
그리고 in-context learning 같이, scaling law 로는 예측할 수 없는 능력도 있다.


<span style='color:green;font-weight:bold'>  (3) Emergent Abilities of LLMs </span>
<br>
"Emergent ability" 는 smaller model 에는 나타나지 않지만, large model 에 갑자기 나타난 능력을 의미한다.
특히 이러한 능력은 copmlex task 에서 나타난다. 
- In-context learning.

In-context learning (ICL) 은 GPT-3 에서 처음 제안된 개념으로, 추가적인 training 이나 gradient update 없이 주어진 instruction 에 따라 문장을 완성하는 능력을 말한다. ICL 은 task 에 따라 천차만별이며, arithmetic task 의 경우, 13B 정도의 GPT-3 에서도 잘하지만, Persian QA task 는 175-B 도 잘하지 못한다.

- Instruction Following (Instruction Tuning).

흔히 **Instruction Tuning** 이라고 부르는, 자연어 decription 을 통한 multi-task fine-tuning 을 통해, LLM 은 instruction form 을 이용하여 explicit example 없이도 unseen task 를 잘 풀어낸다. 대표적인 예시로, LaMDA-PT 는 68B 에서 unseen task 를 잘 해결하며, PaLM 의 경우 62B 부터 MMLU, BBH, TyDiQA, MGSM 같은 eval benchmark 에서 좋은 성능을 보인다. 

- step-by-step reasoning (Chain-of-Thought|CoT).

CoT prompting 을 이용한 grounding 능력은 100B 이상은 되어야 효과적이다. 

<span style='color:green;font-weight:bold'>  (4) How Emergent Abilities Relate to Scaling Laws </span>
<br>
Scaling Law 와 Emergent ability 는 전혀 상반된 결과이다. 하나는 continous improvement 에 대한 내용이며, 하나는 sharp performance leap 에 관한 내용이다. 이에 대한 연구는 더욱 필요하지만, 이는 인간이 언어를 배우는 것과 유사하다고 한다. 인간은 몇몇 단어만 말하다가 '갑자기 어느순간' discontinuous 하게 문장을 구사하게 되는데, 이러한 것이 LLM 이 emergent ability 능력을 가지는 것과 유사하다고 본다.

<span style='color:green;font-weight:bold'>  (5) Key Techniques for LLMs </span>
<br>
LLM 이 *general and capable* learner 가 되게 하는 성공요소는 아래와 같다.
- Scaling.

앞서 언급했듯이, LLM 은 Transformer 모델을 scaling 한 것이다. GPT-3 가 175B 까지, PaLM 이 540B 에서 scaling limit 을 경험했듯이, compute budget 이 정해진 상황에서는 scaling limit 이 있다. 이런 상황에서 **Scaling Law 는 compute-efficent allocation 을 수행하기 위해 더 고려되어야 한다**. Chinchilla 는 Gopher 보다 같은 compute budget 에서 모델 사이즈 대신 더 많은 training token 을 써서, 더 좋은 성능을 보인다. 추가적으로, data scaling 은 careful cleaning process 가 필요한데, pre-training data 의 quality 는 모델에 매우 큰 영향을 미친다.

- Training.

LLM 은 크기가 매우 크기 때문에 distributed training 알고리즘을 요한다. 이렇나 병렬적인 학습을 위해 여러 **optimization framework** 들이 등장했는데, DeepSpeed 나 Megatron-LM 등이 그 예시이다. 또한, training loss spike 극복을 위한 restarting 기법이나, mixed precision training 같은 기법들도 고려되어야 한다. GPT-4 는 독자적인 infrastructure 와 optimization method 를 제안하여, 작은 모델로 큰 모델의 성능을 예측할 수 있는 방법을 제안하였다.

- Ability eliciting.

LLM 이 학습된 이후에는 instruction tuning 이나 CoT prompting 같은 techinical approach 를 통해 LLM 의 능력을 이끌어내는 (eliciting) 것이 중요하다. 

- Alignment tuning.

LLM 은 toxic, biased, harmful content 를 생성해낼 수 있다. InstructGPT 에서 제안되었듯이, helpful, honest, harmless (3h) 세 human value 에 LLM 이 align 되어야 한다. InstrucGPT 에서는 <span style='color:green;font-weight:bold'> RLHF(Reinforcement Learning with Human Feedback) </span> 을 통해 이를 해결하고자 하였다. 

- Tools manipulation.

LLM 은 parameter 내에 정보를 배우는 형식이기 때문에, pre-training data 안에 능력이 한정될 수 밖에 없고, out-date information 을 생성할 수 밖에 없다. 이를 해결하기 위해, external tool 을 활용하여 LLM 의 결점을 극복하고자 하는 시도가 있다. 즉 LLM 에 *"eyes and ears"* 를 달아주는 것이다. 

# 2.2. Technical Evolution for GPT-series Models

<span style='background-color: #dcffe4'> 초록 배경색 </span>
<span style='color:green;font-weight:bold'> 초록 볼드체 </span>
