---
layout: post
title:  "[ACL2023] Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions"
date:   2023-09-12 00:10:00 +0900
use_math: true
categories: [LLM, PLM]
---
[[pdf]](https://aclanthology.org/2023.acl-long.360.pdf) &emsp;
[[github]](https://github.com/Zeng-WH/FutureTOD)

**Harsh Trivedi <sup>1</sup>, Niranjan Balasubramanian <sup>1</sup>, Tushar Khot <sup>2</sup>, Ashish Sabharwal <sup>2</sup>**
<br><sup>1</sup> Stony Brook University, Stony Brook, U.S.A. <sup>2</sup> Allen Institute for AI, Seattle, U.S.A.  &emsp;

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/f0c75ad4-b481-4551-aa5b-33cb66c50b2c)

# Abstract
- (**LLM and** <span style='color:red;font-weight:bold'> Weakness </span>) 최근 LLM 이 natural language reasoning 혹은 Multi-step QA 를 위한 Chain-of-Thought (CoT) 에 매우 강력한 성능을 보인다. 그러나, 이들은 necessary knowledge 가 unavailable 하거나, up-to-date 하지 않은 경우 parameter 속에 그 것을 가지고 있기 힘들다.
- (**One-step retrieval and** <span style='color:red;font-weight:bold'> Weakness </span>) 이에 따라 최근, external knowledge 로 부터 relevant text 를 retrieve 해서 활용하는 one-step retrieve-and-read approach 가 연구되었지만, 이는 multi-step QA 를 풀기에는 부족하다. 
- (**IRCoT**) 이에 저자들은 *what to retrieve* 는 *what has already been derived* 에 depend 한다는 점에 착안하여, CoT 에 retrieval 을 interleave(끼우는) 하는  <span style='color:green;font-weight:bold'> IRCoT </span> 를 제안한다. 
- (**Experiment**) IRCoT 를 GPT-3 에 적용하였을 때, retreival 성능이 매우 향상되었으며, downstream QA dataset 4 개: HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC 에 대하여도 매우 큰 성능 향상을 보인다. 추가적으로, out-of-domain(OOD) setting 에서도 smaller model 에 적용했을 때 매우 좋은 성능을 보인다.
   
# Introduction
