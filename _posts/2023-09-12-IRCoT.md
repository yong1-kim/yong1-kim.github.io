---
layout: post
title:  "[ACL2023] Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions"
date:   2023-09-12 00:10:00 +0900
use_math: true
categories: [LLM, PLM]
---
[[pdf]](https://aclanthology.org/2023.acl-long.360.pdf) &emsp;
[[github]](https://github.com/Zeng-WH/FutureTOD)

**Harsh Trivedi <sup>1</sup>, Niranjan Balasubramanian <sup>1</sup>, Tushar Khot <sup>2</sup>, Ashish Sabharwal <sup>2</sup>**
<br><sup>1</sup> Stony Brook University, Stony Brook, U.S.A. <sup>2</sup> Allen Institute for AI, Seattle, U.S.A.  &emsp;

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/f0c75ad4-b481-4551-aa5b-33cb66c50b2c)

# Abstract
- (**LLM and** <span style='color:red;font-weight:bold'> Weakness </span>) 최근 LLM 이 natural language reasoning 혹은 Multi-step QA 를 위한 Chain-of-Thought (CoT) 에 매우 강력한 성능을 보인다. 그러나, 이들은 necessary knowledge 가 unavailable 하거나, up-to-date 하지 않은 경우 parameter 속에 그 것을 가지고 있기 힘들다.
- (**One-step retrieval and** <span style='color:red;font-weight:bold'> Weakness </span>) 이에 따라 최근, external knowledge 로 부터 relevant text 를 retrieve 해서 활용하는 one-step retrieve-and-read approach 가 연구되었지만, 이는 multi-step QA 를 풀기에는 부족하다. 
- (**IRCoT**) 이에 저자들은 *what to retrieve* 는 *what has already been derived* 에 depend 한다는 점에 착안하여, CoT 에 retrieval 을 interleave(끼우는) 하는  <span style='color:green;font-weight:bold'> IRCoT </span> 를 제안한다. 
- (**Experiment**) IRCoT 를 GPT-3 에 적용하였을 때, retreival 성능이 매우 향상되었으며, downstream QA dataset 4 개: HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC 에 대하여도 매우 큰 성능 향상을 보인다. 추가적으로, out-of-domain(OOD) setting 에서도 smaller model 에 적용했을 때 매우 좋은 성능을 보인다.
   
# Introduction
최근 Large Language Model (LLM) 은 complex question 에 대하여 step-by-step 으로 natural language reasoning 을 하는, 이른바 [Chain-of-Thoughts (CoT)](https://openreview.net/pdf?id=_VjQlMeSB_J)https://openreview.net/pdf?id=_VjQlMeSB_J를 수행할 수 있다.
이 접근법은 질문에 답을 할 수 있는 모든 정보가 parameter 내에 존재해야만 적용가능하다.
<span style='background-color: #ffdce0'> 그러나 많은 open-domain quesition 에 대하여, 대부분의 required knowledge 는 model 의 parameter 속에 존재하지 않는다.([[1]](https://arxiv.org/abs/2203.05115)https://arxiv.org/abs/2203.05115, [[2]](https://arxiv.org/abs/2207.13332)https://arxiv.org/abs/2207.13332 </span>)
<br>

<span style='color:green;font-weight:bold'> How can we augment chain-of-thought prompting for open-domain, knowledge-intensive tasks that require complex, multi-step reasoning? </span>

<br> 

*one-shot* retrieval 을 통해 LM 을 augment 하는 방법이 relevant knowledge 를 활용하고 많은 factoid(뇌피셜) task 를 해결하였지만 ([[3]](https://proceedings.neurips.cc/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf), [[4]](https://proceedings.mlr.press/v119/guu20a.html, [[5]](https://proceedings.mlr.press/v162/borgeaud22a.html), [[6]](https://arxiv.org/abs/2208.03299)), 이 방법들은 complex multi-step reasoning question 들을 푸는데는 분명한 한계점이 존재한다.
몇몇의 question 에 대하여 이러한 방법들은 partial knowledge 를 추출하거나, partial reasoning 을 수행하거나, partial reasoning 에 의한 outcome 에 필요한 additional 정보를 추출해오고 반복한다.
예를 들어, 위의 Figure 1 에서 *"In what country was Lost Gravity manufactured?"* 라는 질문에 대하여 한 번만 retrieval 해 올 경우, Mack Rides 라는 company 까지는 가져오지만 어느 나라인지는 가져올 수가 없다.

따라서, retrieval 과 reasoning step 은 반드시 함께 가야한다.
retrieval 없이는 model 은 incorrect reasoning step 을 할 수 밖에 없어 **hallucination** 이 발생한다.
마찬가지로, first reasoning step 을 거치지 않으면, second step 이 identify 되지 않는다.
다시 말해, 우리는 correct reasoning step 을 위해 retreived fact 가 필요하고, relevant fact 를 retrieve 하기 위해 reasoning step 이 필요하다.

이 intuition 을 통해 저자들은 <span style='color:green;font-weight:bold'> Interleaving Retrieval to CoT (IRCoT) </span> 를 제안한다. Figure 1 이 IRCoT 의 overview를 잘 나타낸다. 우선, question 을 query 로 하여 base paragraph set 을 retrieval 한다. 이후, <span style='background-color: #dcffe4'> (i) extent CoT : question, 지금까지의 paragraph, 그리고 지금까지 생성된 CoT sentence 를 통해 다음 CoT sentence 를 생성하고, (ii) exapnd retreived information : 마지막 CoT sentence 를 통해 최종적으로 information retrieval 을 해와 collected set 을 구성한다. </span> CoT 문장이 정답을 추출하거나, maximum allowed number of reasoning step 이 될 때 까지 이 행동을 반복하다가, termination 과 함께 collected paragraph 가 retrieval outcome 으로 함께 나오고, 이 것들을 모두 context 로 활용하여 QA prompting ([GPT-3](https://openreview.net/forum?id=_VjQlMeSB_J)) 혹은 CoT prompting ([Zero-shot CoT](https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html))을 통해 결과를 도출한다.

4 개의 multi-step reasoning dataset 인 [HotpotQA](https://aclanthology.org/D18-1259/), [2WikiMultihopQA](https://aclanthology.org/2020.coling-main.580/), [MusSiQue](https://aclanthology.org/2022.tacl-1.31/),  그리고 [IIRC](https://aclanthology.org/2020.emnlp-main.86/) 에 대해 code-davinci-002 를 활용하였을 때, 매우 큰 성능향상을 보인다.
또한, Flan-T5 11B, 3B, 700M 같은 작은 모델에 대하여도 비슷한 성능을 보인다.
특히, Flan-T5-XL (3B) 모델에 대하여, IRCoT 를 적용할 경우, 58배 큰 GPT-3 with one-step retrieval 방법보다 더 좋은 성능을 보인다.
게다가, 이 성능향상은 out-of-distribution (OOD) 에서도 같은 경향을 보인다. 
마지막으로, 최근 few-shot open-domain QA (ODQA) 의 그 어떤 모델들 보다도 훨씬 QA score 가 높다. ([DecomP](https://openreview.net/pdf?id=_nGgzQjzaRy), [Self-ask](https://arxiv.org/pdf/2210.03350.pdf), [ReAct](https://arxiv.org/abs/2210.03629))

# Chain-of-Thought-Guided Retrieval and Open-Domain QA
Goal 은 **Knowledge-intensive multi-step reasoning question Q** 를 few-shot setting 으로 해결하는 것이다.  
이를 위해 [_retreive-and-read_](https://arxiv.org/abs/2101.00774) paradigm 을 활용한다. 이는 retriever 가 먼저 knowledge source 로 부터 document 를 retrieval 해온 뒤, QA model 이 answer 를 생성한다. 
IRCoT 방법론은 주로 retrieve step 에 치중되어 있고, read step 에서는 standard prompting startegy 를 활용한다.

<span style='color:green;font-weight:bold'> Interleaving Retrieval with Chain-of-Thought Reasoning </span><br>
<br>
![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/b13bfd3d-4d0b-4680-b47e-34b8334c63c2)
<span style='color:green;font-weight:bold'> IRCoT </span> 은 세 가지로 구성되어 있다. (i) base retriever : query 를 받아 knowledge source 로 부터 paragraph 를 추출한다. (ii) zero/few-shot CoT 가 가능한 LLM (iii) reasoning step 을 통해 answer 에 도달할 수 있는 annotated CoT question 들이다.
우선, 위의 그림처럼 base retriever 가 query Q 를 통해 K 개의 paragraph 를 retrieval 해 온다. 
이후 _reason_ 과 _retrieve_ 라는 two step 를 iteratively interleave 한다. (termination criterion 이 될 때 까지)
