---
layout: post
title:  "[ACL2022] An Interpretable Neuro-Symbolic Reasoning Framework for Task-Oriented Dialogue Generation"
date:   2023-02-28 19:21:00 +0900
use_math: true
categories: [Dialogue]
---
[[pdf]](https://aclanthology.org/2022.acl-long.338.pdf) &emsp;
[[github]](https://github.com/shiquanyang/NS-Dial)

**Shiquan Yang<sup>1</sup>, Rui Zhang<sup>2</sup>, Sarah Erfani<sup>1</sup>, Jey Han Lau<sup>1</sup>**
<br><sup>1</sup> The University of Melbourne ,<br><sup>2</sup> www.ruizhang.info &emsp;

![image](https://user-images.githubusercontent.com/42200027/221837156-917dce11-fb59-4a68-8786-7e23689f0429.png)

# Abstract
- (Motivation) Task-oriented dialouge system 의 interpretability 에 대한 연구가 필요하다.
- (Method) Transparent reasoning process 를 얻기 위하여, *explicit* resaoning chain 을 통한 neuro-symbolic 을 소개한다.
- (Limitation) 기존의 neuro-symbolic 방법은 one-phase design 으로 인해 multi-hop reasoning 과정에서 error-propagation 이 있다.
- (Solution) 이를 해결하기 위하여 Hypohesis generator 와 Reasoner 의 two-stage approach 를 택한다. 우선, hypothesis generator 를 통해 multiple hypotheses 를 얻고, 이후 reasoner 에 의해 평가되어 최종적으로 final prediction 을 위해 하나의 hypothesis 가 선택된다. 모든 과정은 별도의 reasoning chain annotation 없이 텍스트만을 통해 이루어진다.
- (Experiment) 두 public benchmark 에 대하여 좋은 성능을 얻었을 뿐 아니라, interpretable decision process 를 얻었다.

# Introduction
Task-Oriented Dialogue System (TOD) 은 눈부시게 발전하고 있지만, deep learning 의 black-box 적인 특성 때문에, explainability 를 갖추고 있지 못하다.
이러한 *implicit* reasoning 특성 때문에, 만약 knowledge base (KB) 에서 잘못된 추론을 통해 잘못된 정보를 가지고 올 때, 어디서 어떤 문제가 발생했는지를 알 수 없다.
본 논문에서는, <span style='background-color: #dcffe4'> interpretable KB reasoning </span> 을 통해 useful information 을 제공할 뿐 아니라, interpretability 도 갖추는 연구를 제안한다.
이를 위하여 **N**euro-**S**ymbolic **Dial**ogue framework (**NS-Dial**) 을 제안한다.
NS-Dial 은 neural network 의 representation capacity 와 symblic approach 의 explicit reasoning 을 combine 한 novel 한 방법론이다.
기존의 Neuro-symbolic 방법[[1]](https://proceedings.mlr.press/v97/vedantam19a.html)[[2]](https://openreview.net/pdf?id=ryxjnREFwH)은 pre-diefined human interpretable neural module 로 구성된 tree-structued program 을 통해 final prediction 을 얻는 **one-pahse** procedure 이다.
그러나 KB resoning task 의 경우, reasoning process 가 multiple triplet 에 걸쳐 diverse 하게 spanning 되기 때문에, 이러한 one-phase 구조는 error-propagation 이 되기 쉽고, sub-optimal 한 결과를 얻게 된다.

이에 저자들은 **two-phase** procedure 를 통해 이 error propagation의 효과를 경감시킨다.
첫 번째로, multiple hypotheses 를 생성한 후, 이 것을 평가하여 final prediction 을 위한 final hypothesis 를 고른다.
여기서 hypothesis 는 <span style='background-color: #dcffe4'> dialogue context 에서 언급된 entity, KB 속의 entity, 그리고 그 사이의 관계로 이루어진 triplet 의 형태 </span> 이다.
이 중 groun truth triplet 은 ground-truth response 에 언급된 entity 를 포함한다.
예를 들어, 위의 그림에서, hypothesis generator 는 "Cityroom, located_in, Leichhardt" 와 "Gonville_hotel, Located_in, Leichhardt" 의 두 triplet 을 생성할 것이고, reasoner 가 proof tree를 통해 reasoning chain 을 통하여 "Cityroom, located_in, Leichhardt" 가 valid 한 triplet 임을 확인할 것이다. 
이 과정은 end2end 로 raw dialogue 만을 통해 이루어지고, 어떠한 additional intermediate label 도 필요로 하지 않는다.

# preliminary
본 연구에서는 KB 를 통해 dialogue response generation 에 focus 한다.
Dialogue history $X$ 와 knowledge base $B% 가 주어졌을 때, system response $Y$ 를 word-by-word 로 생성한다.
![image](https://user-images.githubusercontent.com/42200027/221843136-1579ab2c-4c49-4cf8-9c74-7e163c5d6c77.png)

$y_t$ 는 response $Y$ 의 t-th token 이다.

![image](https://user-images.githubusercontent.com/42200027/221843313-4cbdfaf4-4328-4a09-b437-f2f7089c1e8e.png)

전체적인 모델 그림은 위와 같고 우선 standard module 을 살펴 본 뒤 두 가지 novel module 을 살펴본다.

<span style='color:green;font-weight:bold'> Dialogue Encoding </span>
<br>
우선 BERT 를 통해, dialogue history 에 대한 distributed representation 을 얻는다.
[CLS] token 을 history token 들 앞에 추가한 뒤 , input token $X = ([CLS],x_1,...,x_M)$ 에 대하여, hidden state $H_{enc} = (h_{CLS},h_1,...,h_M)$ 는 아래와 같이 계산된다. 
![image](https://user-images.githubusercontent.com/42200027/221844350-cb636050-abec-4fe6-b9ce-484d9c32b5e4.png)

<span style='color:green;font-weight:bold'> Response Generation </span>
<br>
우선 linear layer 를 통해 decoder dimension 으로 dialogue history 의 BERT embedding hidden state 을 맞춰준다.
$H\'_{enc} = (h\'_{CLS},h\'_1,...,h\'_M)$ 은 decoder dimension 으로 projected 된 hidden state 이다.

