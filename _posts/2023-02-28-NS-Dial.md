---
layout: post
title:  "[ACL2022] An Interpretable Neuro-Symbolic Reasoning Framework for Task-Oriented Dialogue Generation"
date:   2023-02-28 19:21:00 +0900
use_math: true
categories: [Dialogue]
---
[[pdf]](https://aclanthology.org/2022.acl-long.338.pdf) &emsp;
[[github]](https://github.com/shiquanyang/NS-Dial)

**Shiquan Yang<sup>1</sup>, Rui Zhang<sup>2</sup>, Sarah Erfani<sup>1</sup>, Jey Han Lau<sup>1</sup>**
<br><sup>1</sup> The University of Melbourne ,<br><sup>2</sup> www.ruizhang.info &emsp;

![image](https://user-images.githubusercontent.com/42200027/221837156-917dce11-fb59-4a68-8786-7e23689f0429.png)

# Abstract
- (Motivation) Task-oriented dialouge system 의 interpretability 에 대한 연구가 필요하다.
- (Method) Transparent reasoning process 를 얻기 위하여, *explicit* resaoning chain 을 통한 neuro-symbolic 을 소개한다.
- (Limitation) 기존의 neuro-symbolic 방법은 one-phase design 으로 인해 multi-hop reasoning 과정에서 error-propagation 이 있다.
- (Solution) 이를 해결하기 위하여 Hypohesis generator 와 Reasoner 의 two-stage approach 를 택한다. 우선, hypothesis generator 를 통해 multiple hypotheses 를 얻고, 이후 reasoner 에 의해 평가되어 최종적으로 final prediction 을 위해 하나의 hypothesis 가 선택된다. 모든 과정은 별도의 reasoning chain annotation 없이 텍스트만을 통해 이루어진다.
- (Experiment) 두 public benchmark 에 대하여 좋은 성능을 얻었을 뿐 아니라, interpretable decision process 를 얻었다.

# Introduction
Task-Oriented Dialogue System (TOD) 은 눈부시게 발전하고 있지만, deep learning 의 black-box 적인 특성 때문에, explainability 를 갖추고 있지 못하다.
이러한 *implicit* reasoning 특성 때문에, 만약 knowledge base (KB) 에서 잘못된 추론을 통해 잘못된 정보를 가지고 올 때, 어디서 어떤 문제가 발생했는지를 알 수 없다.
본 논문에서는, <span style='background-color: #dcffe4'> interpretable KB reasoning </span> 을 통해 useful information 을 제공할 뿐 아니라, interpretability 도 갖추는 연구를 제안한다.
이를 위하여 **N**euro-**S**ymbolic **Dial**ogue framework (**NS-Dial**) 을 제안한다.
NS-Dial 은 neural network 의 representation capacity 와 symblic approach 의 explicit reasoning 을 combine 한 novel 한 방법론이다.
기존의 Neuro-symbolic 방법[[1]](https://proceedings.mlr.press/v97/vedantam19a.html)[[2]](https://openreview.net/pdf?id=ryxjnREFwH)은 pre-diefined human interpretable neural module 로 구성된 tree-structued program 을 통해 final prediction 을 얻는 **one-pahse** procedure 이다.
그러나 KB resoning task 의 경우, reasoning process 가 multiple triplet 에 걸쳐 diverse 하게 spanning 되기 때문에, 이러한 one-phase 구조는 error-propagation 이 되기 쉽고, sub-optimal 한 결과를 얻게 된다.

이에 저자들은 **two-phase** procedure 를 통해 이 error propagation의 효과를 경감시킨다.
첫 번째로, multiple hypotheses 를 생성한 후, 이 것을 평가하여 final prediction 을 위한 final hypothesis 를 고른다.
여기서 hypothesis 는 <span style='background-color: #dcffe4'> dialogue context 에서 언급된 entity, KB 속의 entity, 그리고 그 사이의 관계로 이루어진 triplet 의 형태 </span> 이다.
이 중 groun truth triplet 은 ground-truth response 에 언급된 entity 를 포함한다.
예를 들어, 위의 그림에서, hypothesis generator 는 "Cityroom, located_in, Leichhardt" 와 "Gonville_hotel, Located_in, Leichhardt" 의 두 triplet 을 생성할 것이고, reasoner 가 proof tree를 통해 reasoning chain 을 통하여 "Cityroom, located_in, Leichhardt" 가 valid 한 triplet 임을 확인할 것이다. 
이 과정은 end2end 로 raw dialogue 만을 통해 이루어지고, 어떠한 additional intermediate label 도 필요로 하지 않는다.

# preliminary
본 연구에서는 KB 를 통해 dialogue response generation 에 focus 한다.
Dialogue history $X$ 와 knowledge base $B% 가 주어졌을 때, system response $Y$ 를 word-by-word 로 생성한다.
![image](https://user-images.githubusercontent.com/42200027/221843136-1579ab2c-4c49-4cf8-9c74-7e163c5d6c77.png)

$y_t$ 는 response $Y$ 의 t-th token 이다.

![image](https://user-images.githubusercontent.com/42200027/221843313-4cbdfaf4-4328-4a09-b437-f2f7089c1e8e.png)

전체적인 모델 그림은 위와 같고 우선 standard module 을 살펴 본 뒤 두 가지 novel module 을 살펴본다.

<span style='color:green;font-weight:bold'> Dialogue Encoding </span>
<br>
우선 BERT 를 통해, dialogue history 에 대한 distributed representation 을 얻는다.
[CLS] token 을 history token 들 앞에 추가한 뒤 , input token $X = ([CLS],x_1,...,x_M)$ 에 대하여, hidden state $H_{enc} = (h_{CLS},h_1,...,h_M)$ 는 아래와 같이 계산된다. 
![image](https://user-images.githubusercontent.com/42200027/221844350-cb636050-abec-4fe6-b9ce-484d9c32b5e4.png)

<span style='color:green;font-weight:bold'> Response Generation </span>
<br>
우선 linear layer 를 통해 decoder dimension 으로 dialogue history 의 BERT embedding hidden state 을 맞춰준다.
$H\prime_{enc} = (h\prime_{CLS},h\prime_1,...,h\prime_M)$ 은 decoder dimension 으로 projected 된 hidden state 이다.
이 $h\prime_{CLS}$ 을 통해 decoder 를 시작하여, $h_{dec,t}$ 를 얻고, 이것과 $H\prime_{enc}$ 를 attention 하여, $h\prime_{dec,t}$를 얻는다.
이후 이를 concat 하여 context vector C 를 얻고, vocabulary space $V$ 로 project 한다.
![image](https://user-images.githubusercontent.com/42200027/221847360-1f65211f-795f-4230-baab-0c97a0a62b3c.png)

이후, KB distribution $P_{kb,t}$ (KB 속의 entity 들의 probability distribution) 를 interpretable way 로 estimate 하기 위하여, $P_{vocab,t}$ 와 $P_{kb,t}$ 를 fuse 한 뒤 final output token 을 생성한다. 
[See et al.](https://aclanthology.org/P17-1099.pdf) 을 따라, soft-swtich mechanism 을 통해 두 확률 분포를 fuse 하여 $y_t$ output token 을 생성한다.
구체적으로는 generation probability $p_{gen} \in [0,1]$ 을 아래와 같이 계산한 뒤, 
![image](https://user-images.githubusercontent.com/42200027/221848142-e064aba8-5633-42bd-a79e-e84a657db653.png)

아래의 수식을 통해 probability distribution $P(w)$ 를 만들어, greedy sampling 을 통해 $y_t$ 를 생성한다.
![image](https://user-images.githubusercontent.com/42200027/221848159-071e8fdb-49df-4d18-a703-92c77f08fd97.png)

이제 가장 중요한 $P_{kb,t}$ 를 어떻게 얻는지, 두 가지 novel 한 모듈 (1) hypothesis generator, (2) reasoner 을 통해 설명한다.

# Neuro-Symbolic Reasoning for Task-Oriented Dialogue

KB distribution $P_{kb,t}$ 를 얻기 위하여 Hypothesis generator (HG) 와 hierarchical reasoning engine (HRE)를 구성한다.
HG module 의 input 으로 위에서의 context vector C 를 input 으로 사용하고, K 개의 hypotheses 로 이뤄진 집합 $HYP$ 를 얻는다.
각각의 hypothesis 들은 HRE 에 feed 되어 logical reasoning chain 을 생성하고 belief score 가 매겨진다.
계산된 belief score 가 $P_{kb,t}$ 로써 제공되고, KB 의 eneity 들에 대한 distribution 이 된다.

<span style='color:green;font-weight:bold'>  Hypothesis Generator </span>
<br>
Hypothesis triplet "[H,R,T]" 에 대해서 H 는 Head entity, T 는 Tail entity, R 은 relation 이라고 표기할 때, 세 가지 type 의 hypothesis 를 고려한다 : H-hypothesis, R-hypothesis, T-hypothesis.
H-hypothesis 는 R 과 T 는 dialogue context 에서 infer 될 수 있지만, H 가 unknown 이라 KB 에서 추출되어야 하는 경우이다. 
따라서 H-hypothesis 는 "[▷,R,T]" 의 형태를 갖는다. 
R-hypothesis, 와 T-hypothesis 는 이와 같은 줄기를 갖는 형태이다.
HG 는 여러 ▷을 채우는 hypothesis 들을 생성하여 HRE 로 넘긴다.

직관적으로, hypothesis 은 content 와 sturcutre 로 부터 결정되어야 한다.
structure 는 hypothesis 의 template form 을 말하고, content 는 template 을 채우는 것을 말한다.
예를 들어, H-hypothesis 라면 "[▷,R,T]" 형태를 갖게 될 것이고, content 는 candidate entity "▷" 과 query state "R", "T" 로 이루어진다.
이를 위해 저자들은 divide-and-conquer 방식을 택해, structure prediction, querty state prediction, 그리고 candidates prediction 세 가지 sub-component 를 구성한다.

<span style='color:green;font-weight:bold'> Structure Prediction (SP) </span>
<br>
SP 의 궁극적인 목표는 H/T/R-hypothesis 중 어떤 hypothesis 인지 결정하는 것이다.
context vector C 를 input 으로 shared transformeration layer 를 거친 뒤, task-agnostic fature 인 $h_share$ 는 아래와 같이 구성된다.
![image](https://user-images.githubusercontent.com/42200027/221851526-5c41ebd8-9898-42f2-9348-fbb9a4d20fda.png)

