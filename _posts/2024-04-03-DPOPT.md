![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/a5ce9ce0-85d5-44a5-91a5-c1c29febbd21)---
layout: post
title: "[ICLR2024] DP-OPT: MAKE LARGE LANGUAGE MODEL YOUR PRIVACY-PRESERVING PROMPT ENGINEER"
date: 2024-04-03 16:30:00 +0900
use_math: true
categories: [LLM, PLM]
---

[[pdf]](https://openreview.net/attachment?id=Ifz3IgsEPX&name=pdf) &emsp;
[[github]](https://github.com/VITA-Group/DP-OPT)

*Junyuan Hong<sup>1</sup>, Jiachen T. Wang<sup>2</sup>, Chenhui Zhang<sup>3</sup>, Zhangheng Li<sup>1</sup>, Bo Li<sup>4</sup>, Zhangyang Wang<sup>1</sup>**
<br> <sup>1</sup> University of Texas at Austin, <sup>2</sup> Princeton University, <sup>3</sup> MIT, <sup>4</sup> University of Chicago &emsp;
 
![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/240dce34-38db-4011-8f7f-c4b92ad59ea1)

## Abstract
- (**Privacy issue in LLM**) LLM 은 prompt tuning 을 통해 많은 task 에서 압도적인 성능을 보여준다. 그러나, 민감한 개인 정보에 dependency 가 있는 경우 문제가 생길 수 있다. 하나의 방법은 local LLM 을 host 하여 prompt 에 녹이는 방법이지만, closed-source 일 경우 불가능하다.
- (<span style='color:green;font-weight:bold'> DP-OPT </span>) 이 논문에서는 Differentially-Private Offsite Prompt Tuning (DP-OPT) 라는 방법론을 통해 문제를 해결한다. 이 방법론은 client side 에서 prompt 를 처리하고, 이 처리된 discrete prompt 를 cloud model 에 보내서 학습을 하는 방법이다. 저자들은 이 방법론이 성능 타협 없이 prompt 를 cloud model 에 잘 전달함을 보인다.
- (**Differentially-private (DP) ensemble**) Prompt 가 개인 정보를 누출(leak)하지 않음을 보장하기 위하여, private prompt generation 메커니즘인 Differentially-private (DP) ensemble 방법을 제안한다.
- (**Experiment**) DP-OPT 방법은 Vicuna-7B 를 통해 privacy-preserving prompt 를 쓰면서도, (private 정보를 쓰지 않은) GPT3.5 혹은 local private prompt tuning 방법과 유사하거나 좋은 성능을 보인다.

## 1. INTRODUCTIONS

<span style='color:green;font-weight:bold'> ▶ Prompt Engineering </span>
<br>
Large Language Model (LLM) 이 강력한 pre-training 으로 방대한 task 에서 매우 압도적인 성능을 보여주지만, prompt engineering 은 cost-efficient 하게 downstream task 에 adatable 하게 할 수 있는 방법이다.
Model parameter 를 resource-heavy 하게 optimize 하는 대신, prompt engineering 은 API access 등을 통해 prompts 만을 iteratively refine 해주면 된다.
**Manual Prompt Engineering** 은 많은 task 에서 매우 인상적인 성능을 보여줬지만, legal judgement, healthcare, art 등의 전문가적인 downstream task 에 대해서는 domain knowledge 에 기반한 prompt design 에 human experience 가 개입되어야 하는 단점이 있다.
이를 위해, data-driven prompt tuning 인 soft prompt tuning 이 고안되었고, 이 방법은 prompt 를 trainable embedding vector 로 표현한뒤 training instance 에 따라 embedding vector 를 refine 한다.

<span style='color:green;font-weight:bold'> ▶ Data Privacy Issue </span>
<br>
<span style='background-color: #ffdce0'> 그러나 prompt tuning 의 적용의 major한 장벽이 되는 것이 data privacy 문제이다. </span>
ChatGPT 와 같은 LLM API 에 prompt 를 넣을 때, privacy-sensitive 한 정보를 넣게 되면 문제가 발생한다. 예를 들어 _1) Data Confidentiality_ (Confidential data 가 입력이 되는 경우) 나 _2)Information Leakage_ (누출되면 안되는 정보가 누출되는 경우) 등이다.
이름, 주소, 전화번호 같은 개인정보가 pre-training phase 나 fine-tuning data 에 포함된다면, 특정 parmaeter 를 통해 retrieve 될 수 있다.

<span style='color:green;font-weight:bold'> 이 문제 해결을 위한 Straighforward 접근은 local device 에서 entire prompt process 를 진행하는 것이다. </span>
그러나, GPT 시리즈와 같이 closed-source 모델의 경우, substantial cost 는 말할 것도 없이 local hosting 자체가 불가능하다.

<span style='color:green;font-weight:bold'> ▶ Differentially-Private Offsite Prompt Tuning (DP-OPT)  </span>
<br>

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/d47b9c57-ab89-4bf1-88d0-31d5e1001f3b)

이 문제 해결을 위해 저자들은 Differentially-Private Offsite Prompt Tuning (DP-OPT) 방법론을 제안한다.
이 방법론은 LLM 으로 하여금 private and transferable prompt 를 cloud-hosted LLM 을 위해 가공할 수 있게 한다.
위의 그림과 같이, privacy protection 의 중요한 부분 (crux) 은 client 에서만 운용된다.
Confidential datatset 으로, DP-OPT 는 적은 sample 만으로 local LLM 이 prompt 를 생성할 수 있다.
이 local assistant LLM 은 coud-based LLM 에 비해 상대적으로 매우 작다.
또한, <span style='background-color: #dcffe4'> 이 prompt generation process 는 Differentially-Private (DP) ensemble of in-context learning 으로 가능하다. </span>
실험 결과, 여러 언어처리 태스크에서, open-source VIcuna-7B 에 tuned 된 prompt 가  closed-source 인 GPT-3.5 나 LLama-2 보다 강력한 성능을 보인다.

## 2. PRELIMINARIES
# 2.1. Large Language Models (LLMs) and Prompt Tuning.
GPT, Llama, OPT 와 같은 LLM 은 이전의 context 로 부터 다음 token 을 생성한다.
수식적으로는 conditional probability $p_{LM}^t(y|x)$ 를 생성한다. 여기서 $x$는 prompt 이고, $y$는 output, $t$ 는 temperature 이다.
이때, task intsruction 과 같은 front-end prompt $\pi$를 사용한다면, **prompt tuning** 은 $F(\pi,x)$ 에서 $\pi$를 potimize 하여, 최종적으로, $p_{LM}^t(y|F(\pi,x))$ 를 향상시키는 것을 목적으로 한다.

# 2.2. Differential Privacy

<span style='color:green;font-weight:bold'> Definition 3.1 (Differential Privacy ([Dwork et al., 2006](https://link.springer.com/chapter/10.1007/11681878_14))). </span>
<br>

## 3. METHOD

<span style='color:green;font-weight:bold'> Assumptions </span>
<br>

<span style='color:green;font-weight:bold'> Threat Model </span>
<br>

<span style='color:green;font-weight:bold'> Main Idea </span>
<br>

# 3.1. TRANSFERABLE DISCRETE PROMPTS ENABLE OFFSITE PROMPT TUNING

<span style='color:green;font-weight:bold'> Make LLM Prompt Engineer </span>
<br>

<span style='color:green;font-weight:bold'> LLM-Engineered Prompts Are Transferrable </span>
<br>

# 3.2. DIFFERENTIALLY-PRIVATE OFFSITE PROMPT TUNING (DP-OPT)

<span style='color:green;font-weight:bold'> Private Prompt Generation </span>
<br>


<span style='color:green;font-weight:bold'> Private Selection among Generated Prompts </span>
<br>

## 4. EXPERIMENTS

<span style='color:green;font-weight:bold'> Tasks </span>
<br>


<span style='color:green;font-weight:bold'> Setup </span>
<br>

# 4.1. PRIVATE OFFSITE PROMPT TUNING

# 4.2. ABLATION STUDIES


<span style='color:green;font-weight:bold'> Examples of Privacy Leakage in Generated Prompts </span>
<br>

## DISCUSSION AND CONCLUSION
```
With the rising popularity of prompt tuning, our research endeavors to extend this tool to applications with heightened privacy concerns. We introduce the pioneering end-to-end system designed to derive differentially-private prompts from confidential training datasets and deploy these prompts on cloud models. Our approach is underpinned by theoretical validations of its privacy assurances, and through empirical analysis, we highlight the advantageous balance it strikes between utility and data privacy caused by the strong performance of scaled LLMs.
```


<span style='color:green;font-weight:bold'> 초록색볼드체 </span>
<br>
<span style='background-color: #dcffe4'> 초록색배경 </span>
<span style='background-color: #ffdce0'> 빨간색배경 </span>

<span style='color:green;font-weight:bold'> ▶ </span>
<br>
