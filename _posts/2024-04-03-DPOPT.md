---
layout: post
title: "[ICLR2024] DP-OPT: MAKE LARGE LANGUAGE MODEL YOUR PRIVACY-PRESERVING PROMPT ENGINEER"
date: 2024-04-03 16:30:00 +0900
use_math: true
categories: [LLM, PLM]
---

[[pdf]](https://openreview.net/attachment?id=Ifz3IgsEPX&name=pdf) &emsp;
[[github]](https://github.com/VITA-Group/DP-OPT)

*Junyuan Hong<sup>1</sup>, Jiachen T. Wang<sup>2</sup>, Chenhui Zhang<sup>3</sup>, Zhangheng Li<sup>1</sup>, Bo Li<sup>4</sup>, Zhangyang Wang<sup>1</sup>**
<br> <sup>1</sup> University of Texas at Austin, <sup>2</sup> Princeton University, <sup>3</sup> MIT, <sup>4</sup> University of Chicago &emsp;
 
![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/240dce34-38db-4011-8f7f-c4b92ad59ea1)

## Abstract
- (**Privacy issue in LLM**) LLM 은 prompt tuning 을 통해 많은 task 에서 압도적인 성능을 보여준다. 그러나, 민감한 개인 정보에 dependency 가 있는 경우 문제가 생길 수 있다. 하나의 방법은 local LLM 을 host 하여 prompt 에 녹이는 방법이지만, closed-source 일 경우 불가능하다.
- (<span style='color:green;font-weight:bold'> DP-OPT </span>) 이 논문에서는 Differentially-Private Offsite Prompt Tuning (DP-OPT) 라는 방법론을 통해 문제를 해결한다. 이 방법론은 client side 에서 prompt 를 처리하고, 이 처리된 discrete prompt 를 cloud model 에 보내서 학습을 하는 방법이다. 저자들은 이 방법론이 성능 타협 없이 prompt 를 cloud model 에 잘 전달함을 보인다.
- (**Differentially-private (DP) ensemble**) Prompt 가 개인 정보를 누출(leak)하지 않음을 보장하기 위하여, private prompt generation 메커니즘인 Differentially-private (DP) ensemble 방법을 제안한다.
- (**Experiment**) DP-OPT 방법은 Vicuna-7B 를 통해 privacy-preserving prompt 를 쓰면서도, (private 정보를 쓰지 않은) GPT3.5 혹은 local private prompt tuning 방법과 유사하거나 좋은 성능을 보인다.

## 1. INTRODUCTIONS

<span style='color:green;font-weight:bold'> ▶ </span>
<br>

## 2. PRELIMINARIES
# 2.1. Large Language Models (LLMs) and Prompt Tuning.

# 2.2. Differentail Privacy

## 3. METHOD

<span style='color:green;font-weight:bold'> Assumptions </span>
<br>

<span style='color:green;font-weight:bold'> Threat Model </span>
<br>

<span style='color:green;font-weight:bold'> Main Idea </span>
<br>

# 3.1. TRANSFERABLE DISCRETE PROMPTS ENABLE OFFSITE PROMPT TUNING

<span style='color:green;font-weight:bold'> Make LLM Prompt Engineer </span>
<br>

<span style='color:green;font-weight:bold'> LLM-Engineered Prompts Are Transferrable </span>
<br>

# 3.2. DIFFERENTIALLY-PRIVATE OFFSITE PROMPT TUNING (DP-OPT)

<span style='color:green;font-weight:bold'> Private Prompt Generation </span>
<br>


<span style='color:green;font-weight:bold'> Private Selection among Generated Prompts </span>
<br>

## 4. EXPERIMENTS

<span style='color:green;font-weight:bold'> Tasks </span>
<br>


<span style='color:green;font-weight:bold'> Setup </span>
<br>

# 4.1. PRIVATE OFFSITE PROMPT TUNING

# 4.2. ABLATION STUDIES


<span style='color:green;font-weight:bold'> Examples of Privacy Leakage in Generated Prompts </span>
<br>

## DISCUSSION AND CONCLUSION
```
With the rising popularity of prompt tuning, our research endeavors to extend this tool to applications with heightened privacy concerns. We introduce the pioneering end-to-end system designed to derive differentially-private prompts from confidential training datasets and deploy these prompts on cloud models. Our approach is underpinned by theoretical validations of its privacy assurances, and through empirical analysis, we highlight the advantageous balance it strikes between utility and data privacy caused by the strong performance of scaled LLMs.
```


<span style='color:green;font-weight:bold'> 초록색볼드체 </span>
<br>
<span style='background-color: #dcffe4'> 초록색배경 </span>
<span style='background-color: #ffdce0'> 빨간색배경 </span>

<span style='color:green;font-weight:bold'> ▶ </span>
<br>
