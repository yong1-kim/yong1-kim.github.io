---
layout: post
title:  "[EMNLP2021 best paper] Visually Grounded Reasoning across Languages and Cultures"
date:   2024-03-22 13:32:00 +0900
use_math: true
categories: [Vision-and-Language]
---
[[pdf]](https://aclanthology.org/2021.emnlp-main.818.pdf) &emsp;
[[blog]](https://marvl-challenge.github.io/) &emsp;
[[github]](https://github.com/marvl-challenge/marvl-code) 

**Fanyou Liu<sup>*1</sup>, Emanuele Bugliarello<sup>*2</sup>, Edoardo Maria Ponti<sup>3,4</sup>, Siva Reddy<sup>3,4</sup>, Nigel Collier<sup>1</sup>, Desmond Elliott<sup>2</sup>**
<br><sup>1</sup> University of Cambridge, <sup>2</sup> University of Copenhagen, <sup>3</sup> Mila - Quebec AI institute, <sup>4</sup> McGill University  &emsp;

![image](https://user-images.githubusercontent.com/42200027/226248111-01c818d4-696f-450c-9b9a-9b15988e8510.png)

# Abstract
- (Motivation) ImageNet을 바탕으로 한 데이터셋과 인코더들은 대부분 영어 기반으로 되어 있어 북미나 서유럽에서 가져온 자료가 대부분이다.
- (Dataset) 이를 해결하기 위해, 인도네시아어, 중국어, 스와힐리어, 타밀어, 터키어와 같은 다양한 언어를 대상으로 새로운 프로토콜을 도입하여 <span style='color:green;font-weight:bold'> MaRVL </span> 이라는 새로운 다국어 데이터셋을 구성했다. 
- 이 데이터셋은 이미지 Pair에 대한 지역문화를 반영한 답변을 수집하였으며, 다양한 언어 간 전이 학습 결과를 평가했다.
- 이를 통해, 다국어 및 다문화 시스템 개발에 새로운 도전과 발전 가능성을 제시한다.

# Introduction
ImageNet 은 컴퓨터 비전 연구의 기초를 제공했다. 이 데이터셋은 WordNet의 개념에서 선택된 개념 계층을 기반으로 한다. 이 데이터셋을 기반으로 NLVR2, MS-COCO, Visual Genome과 같은 다른 데이터셋이 구축되었고, "ResNet"과 같은 시각 데이터를 전처리하는 데 사용되는 사전 학습된 Encoder 도 만들어졌다. <span style='background-color: #dcffe4'> ImageNet에 포함된 개념과 이미지가, 이것이 만들어진 영어권과 북미, 유럽 문화를 넘어서서 얼마나 적합한가? </span>  이들의 이상적인 분포를 정의하는 것은 어려울 수 있으며, 목적에 따라 다양할 수 있다. 그러나, 전 세계적인 대표성을 목표로 한다면, 이 데이터의 기원과 내용이 편향되어 있다는 증거가 있다. 이를 해결하기 위해, [Yang et al.](https://dl.acm.org/doi/10.1145/3351095.3375709)은 데이터에 개입하여 일부 범주를 필터링하고 재균형을 제안했다. 그러나, 원래 분포의 범위가 다양한 언어와 문화를 포괄하지 않는 한, 이것은 여전히 부족하다. 따라서, 다중 모달 기술의 글로벌 아웃리치를 확대하기 위해서는, 보다 근본적인 계층 구조의 개편이 필요하다. 사실, 가장 두드러진 개념과 그들의 prototypical 멤버들과 시각적 표현은 문화나 환경적 요인에 따라 달라질 수 있다. 이러한 변화는 언어별 리소스에서 개념을 (무작위로) 선택하거나 웹 쿼리에서 이미지를 자동으로 수집하는 데이터셋 생성의 일반적인 관행으로 인해 흐려질 수 있다.

저자는 이 연구에서, 기존의 프로토콜을 개선하여 다문화 및 다언어 데이터셋을 만드는 데 도움이 되는 편향을 완화했다. 특히, 원어민의 구성원들이 선정한 개념과 이미지를 선택하도록 했다. 인도네시아어, 스와힐리어, 타밀어, 터키어, 중국어 등 다언어와 다양한 문화에 초점을 맞추었으며, 원어민이 작성한 이미지 쌍을 비교하도록 요청하여 그라운드된 기술적 설명을 수집했다. 이를 통해 매칭 기반보다는 깊은 언어적 이해가 필요하며, 모달리티 정보 통합이 필요한 작업을 선택하였다. 이 연구에서 제시된 <span style='background-color: #dcffe4'> 'Multicultural Reasoning over Vision and Language (MaRVL)' </span> 데이터셋의 예시는 위의 그림에서 볼 수 있다.

저자는 최신 시각언어 모델(Liu et al., 2019; Chen et al., 2020)을 MaRVL 데이터셋에서 평가하였다. 이를 위해 제로샷 및 번역기를 사용한 다국어 전이 학습을 수행하였으나, 성능이 영어 데이터셋(NLVR2; Suhr et al., 2019)에 비해 현저히 떨어졌다는 결과를 얻었다. 이러한 실패 원인을 조사한 결과, MaRVL은 이미지, 언어의 다양성 및 개념의 도메인 변화로 인해 매우 어려워졌다는 것을 발견하였다. 따라서, 현재 기준으로 MaRVL 데이터셋은 기존 벤치마크 대비 최신 모델의 일반화 능력을 더 신뢰할 수 있는 추정치를 제공할 수 있으며, 데이터셋, 주석 지침, 코드 및 모델은 [marvl-challenge.github.io](https://github.com/marvl-challenge/marvl-challenge.github.io)에서 제공된다.

# Motivation

ILSRVC1K (ImageNet Large-Scale Visual Recogntion)는 컴퓨터 비전 분야에서의 중요한 평가 지표인데, 이는 ImageNet에서 추출한 1,000개의 개념을 기반으로 한다. 그러나 이러한 데이터셋이 다양한 언어와 문화를 대표할 수 있는지에 대한 의문이 제기되어, 개념을 보다 정확히 정의하는 것이 필요하다.

<span style='color:green;font-weight:bold'>  Concepts: Basic Level and Prototypes </span><br>
<br>
저자는 *concept* 이란 category (e.g. BIRD)의 정신적 표현(mentral represenatation)이라고 하며, 비슷한 특성을 가진 객체와 사건의 인스턴스가 함께 그룹화된다. 그러나 모든 카테고리 멤버는 동등한 지위를 가지지 않으며, 일부는 다른 멤버들보다 prototypical에 가깝다(e.g. PENGUIS are more atypical BRIDS than ROBINS). 이러한 분류는 문화나 개인의 선호에 의해 제한될 수 있다. 따라서, prototypical, basic-level 카테고리 및 도메인에 대한 카테고리 수는 인지, 문화, 환경적 요소 및 개인적 선호에 의해 제한된다.

<span style='color:green;font-weight:bold'> Limitations of ImageNet </span><br>
<br>
ImageNet의 원래 anntotaion이 '개념이 보편적이고 기본 수준에 있는가'를 확인하기 위한 것은 아니었지만, 이러한 디자인 선택은 많은 언어와 문화에서 일상 생활 시나리오를 추론할 수 있는 다중 모달 시스템을 가능하게 하는 데 중요한 제한 사항으로 나타날 수 있다.
![image](https://user-images.githubusercontent.com/42200027/227844409-5f1c86f2-b7de-41b6-bed2-e527a2e35113.png)
 
<span style='color:green;font-weight:bold'> ImageNet concepts are not universal. </span><br>
<br>
이미지넷은 영어 WordNet에 기반하여 만들어졌으며, 그 결과, 영어권에서는 익숙한 개념이지만 다른 문화권에서는 낯설거나 전혀 알지 못하는 개념도 포함되어 있다. 또한 다른 문화에서의 개념도 포함하지 못할 수 있다. 따라서 이미지넷 개념이 언어별로 얼마나 관련성이 있는지 측정하기 위해, 개별적으로 각 Synset을 Wikipedia 페이지에 매핑하고, 사용 가능한 언어를 추출하였다. 이 결과, 대부분의 Synset은 30개 이하의 언어에서만 존재하며, "universal"한 개념은 매우 적다는 것을 보여주고 있다. 또한 WALS 데이터베이스를 사용하여 언어 가족에도 동일한 논리가 적용되며, 대부분의 언어는 유라시아 대륙에서 나온 것임을 보여준다.

<span style='color:green;font-weight:bold'> ImageNet concepts are overly specifc to English. </span><br>
<br>
이미지넷(ImageNet)은 WordNet의 리프 노드에 속하는 BLENHEIM SϿANIEL 같은 지나치게 구체적인 개념을 포함하고 있으며, 이는 개(DOG)와 같은 기본 수준의 개념보다 더 구체적이다. 또한 이미지에 대한 사람들의 라벨에 사용된 용어의 깊이와 Ordonez 등(2013)의 일부 ImageNet 개념의 WordNet 내 깊이를 비교하여 ImageNet이 보다 미세한 Synsets를 선호하는 것을 확인할 수 있다. 이러한 문제는 영어뿐 아니라 다른 문화권에서 더욱 악화될 수 있다. 일본 악기 '코토'는 영어 사용자들은 '악기'라고 간단히 표현하는 반면, 일본어 사용자들은 더 정확한 표현인 '箏' (코토)를 사용할 것으로 예상된다는 것을 저자들은 발견했다.

<span style='color:green;font-weight:bold'> Sources of Bias </span><br>
<br>
앞서 살펴본 편향의 잠재적 원인들에 대해 살펴본다. 특히, ImageNet, ILSVRC 1K 및 NLVR2와 같은 데이터셋 생성의 각 단계를 따로 검토한다. 이는 1) 개념 선택, 2) 후보 이미지 검색 및 3) 수동 정리 단계를 의미한다. 설계 단계에서 생길 수 있는 편향성 중 첫 번째는 개념의 선택이다. ImageNet은 WordNet으로부터 12개 하위 트리와 총 5,247개의 synset을 선택했다. 그 중에서도 보다 미세한 synset을 선호하여 "밀집된 의미적 계층"을 얻고자 했다. 그 중에서도 ILSVRC 2012-2017 공유 과제를 위해 1,000개의 개념이 임의로 선택되었다. 따라서 1,000개의 개념은 비기본적인 수준으로 편향될 가능성이 있다(예: 147개의 synset은 개 종류다).

저자는 Bias 의 두 번째 원인으로 후보 이미지 검색을 지적한다. 검색 엔진(Flickr와 ILSVRC 1K의 다른 지정되지 않은 엔진, NLVR2의 Google 이미지)에서 얻은 이미지는 성별(Kay 등, 2015)과 인종(Noble, 2018) 등 현실 세계의 분포를 따르지 않는다. 또한, 이들은 사용자의 프로필과 지역에 따라 결과를 사용자 정의한다. ImageNet의 검색어는 다시 영어로 표현되었으며, 일부는(지정되지 않음) 스페인어, 네덜란드어, 이탈리아어 및 중국어(만다린)로 표현되었으며, 이 중 후자만 서구 유럽 언어이다.

저자는 세 번째로, 이미지 필터링에도 추가적인 편향성이 존재할 수 있다고 말한다. 이는 검색 쿼리의 10%만이 적절한 품질을 가지기 때문에 필요하다. ImageNet에서는 Amazon Mechanical Turk를 통해 정리가 이루어졌다. Annotation 작업자들의 언어와 문화에 대한 정보가 없지만, 그들이 전 세계적 다양성을 대표할 수 있다는 근거는 없다. 또한 합의 없이 주석이 되지 않은 부분은 제거되어, 문화적 차이가 사라질 가능성이 있다. (의견이 상이한 것이 그저 다른 기본 수준이나 프로토타입을 나타낼 수도 있음에 유의)

# MaRVL : Dataset Annotation
ImageNet 데이터셋에 내재된 편향성을 고려하여, 저자는 언어 원어민들이 생활 경험에 따라 발생한 개념으로 이루어진 데이터를 수집하기 위한 프로토콜을 정의한다. 이 데이터셋 생성은 다음과 같은 다섯 가지 단계로 이루어진다: 1) 언어 선택; 2) 범용적 개념 선택; 3) 언어별 개념 선택; 4) 이미지 선택; 5) 캡션 주석화.

<span style='color:green;font-weight:bold'> Selection of Languages </span><br>
<br>
본 연구에서는 인도네시아어, 스와힐리어, 타밀어, 터키어, 중국어(간체) 등 다양한 언어를 선택하여 그 언어권에서 일반적으로 사용되는 단어와 각 언어의 특정한 문화적, 지리적 배경에서 자주 사용되는 단어를 모은 데이터셋을 구축한다. 이를 통해 언어와 문화적으로 다양한 세계의 모습을 반영하고, 이들 데이터셋이 보다 보편적으로 사용될 수 있도록 하는 것이 목적이다.

<span style='color:green;font-weight:bold'>  Selection of Universal Concepts </span><br>
<br>
저자는 다양한 언어와 문화에 대한 데이터셋을 만들기 위해 전 세계의 언어에서 공통적으로 존재하는 단어들을 선택했다. 이를 위해 인류학적 연구와 비교 언어학에서 유니버설한 개념들을 모은 리스트가 있고, 저자는 이 중 Intercontinental Dictionary Series를 선택하여 선정된 18개의 의미 분야에서 콘크리트한 객체와 사건을 다루는 단어들을 공유 풀로 사용하였다.

<span style='color:green;font-weight:bold'>  Selection of Language-Specific Concepts </span><br>
<br>
각 언어에 대해 5명의 모어 화자 주석자를 고용하여, 각 의미 분야마다 그 문화에서의 5-10개의 구체적인 개념에 대한 위키피디아 페이지 링크를 제공하도록 한다. 각 의미 분야의 개념은 <span style='background-color: #dcffe4'> 해당 언어를 사용하는 인구에서 흔하게 볼 수 있거나 대표적인 것 </span>이어야 하며 "이상적으로는 물리적이고 구체적"이어야 한다. 그 결과, 각 언어마다 86-96개의 구체적인 개념을 얻을 수 있었다. Annotator 들 사이의 높은 합의는 선택된 개념들이 해당 문화에서 대표적임을 시사한다.

<span style='color:green;font-weight:bold'> Selection of Images </span><br>
<br>
각 언어별로 개념에 대한 이미지를 선택하기 위해 네이티브 어노테이터 2명을 고용한다. NLVR2의 이미지 선택 요구사항을 따르며, 여러 가지 개념이 포함된 이미지, 개념이 다른 물체와 상호작용하고 있는 이미지, 개념이 활동을 수행하고 있는 이미지, 다양한 물체나 특징을 나타내는 이미지를 선택해야 한다. 이미지는 CC 라이센스를 가진 자연스러운 이미지여야 하며, 각 언어의 사용자들이 일상적으로 볼 수 있는 이미지여야 한다. 이를 위해, 각 언어의 어노테이터는 다양한 소스를 사용하여 이미지를 모아야 한다. 이 과정에서, 8 개 미만의 유효한 이미지가 있는 개념은 제외된다.
![image](https://user-images.githubusercontent.com/42200027/227846761-d807a3d8-cb8c-4ba6-8913-b26cbd9aad75.png)

<span style='color:green;font-weight:bold'> Annotation of Captions </span><br>
<br>
저자는 각 개념에 대해 8개의 이미지를 랜덤으로 선택하고, 이를 4개씩 묶어서 4개의 어노테이션을 만든다. 각 어노테이션은 두 개의 Pair는 참이고 나머지 두 개의 ㅖPair는 거짓인 설명문을 작성하도록 하며, 설명문은 "Theme Concept"을 중심으로 작성되도록 한다. 이후 검증자들은 설명문에 True/False 레이블을 지정하고, 오타나 문법적 오류를 체크한다. 레이블이 상이한 경우는 원래의 어노테이터가 재검토하도록 하며, 최종적으로 네이티브 스피커가 마지막 점검을 한다. 이렇게 생성된 데이터셋은 이미지 2개, 설명문, True/False 레이블로 이루어진다.

![image](https://user-images.githubusercontent.com/42200027/227847271-9571753f-2c70-435e-b08b-6ceda05a060c.png)

# Dataset Analaysis
<span style='color:green;font-weight:bold'> Human Validation </span><br>
<br>
저자는 최종 라운드 평가를 진행하여 인간의 정확도와 주석자 간 합의를 report한다(최종적으로 확정된 캡션을 변경하지 않고). 각 언어마다 데이터 세트에서 무작위로 200개의 예제를 추출한다. 저자는 True/False 라벨을 가리고, 두 명의 새로운 평가자에게 예제를 재평가하도록 요청한다 (Fig. 3, right 와 동일). 모든 언어에서, 세 명의 Annotator (캡션 작성자와 두 명의 최종 라운드 평가자) 간의 kappas는 최소 0.887 이다(Tab. 1). 이 점수는, Landis and Koch (1977)에 따르면, 거의 완벽한 주석자 간 합의를 나타낸다. 캡션 작성자가 제공한 라벨이 올바른 경우, 평균 인간 정확도 점수는 대부분 높은 90%대에서 나타나며, 스와힐리어는 (93.0%) 제외하고도 매우 높다.

<span style='color:green;font-weight:bold'> Concept and image statistics. </span><br>
<br>
데이터셋에 대한 자세한 통계 정보를 Tab. 2에서 확인할 수 있다. 이미지 수집 이후, 각 언어별로 평균 5개의 개념이 걸러졌다. 최종적으로 선정된 개념 중 일부는 영어 WordNet에 없는데, 예를 들면 yağlı güreş (OIL WRESTLING)와 같은 스포츠, 四合院 (SIHEYUAN)와 같은 건축물, 그리고 ࣰࣱࣕ࣪ࣚ) DOSA)와 같은 음식이 있다.

<span style='color:green;font-weight:bold'> Caption statistics. </span><br>
<br>
MaRVL 캡션의 주요 통계 및 무작위로 추출된 250개의 NLVR2 캡션의 통계를 Tab. 3에서 볼 수 있다. 

![image](https://user-images.githubusercontent.com/42200027/227847303-c009529d-4fb3-4937-aaaf-9768eca01b55.png)

<span style='color:green;font-weight:bold'> Image distribution. </span><br>
<br>
저자는 MaRVL 이미지의 분포와 이것이 NLVR2와 어떻게 다른지 이해하기 위해 (1) MaRVL 이미지와 (2) 1,000개의 NLVR2 이미지의 특징을 추출하여 ImageNet 사전 학습 ResNet50 (He et al., 2016)를 사용하여 임베딩 분포를 UMAP (McInnes et al., 2018)를 사용하여 시각화한다. 상단의 그림 4에서 보여지듯, 중국어 이미지는 (NLVR2에서 온) 영어 이미지와 매우 다른 분포를 가지고 있다. 특히, 영어 NLVR2 이미지의 많은 클러스터가 각기 다른 종류의 개이다. 이는 ImageNet이 가져오는 문제로 인한 것이다. 그림 4 의 하단에서는 MaRVL의 두 언어 (인도네시아어와 스와힐리어)의 이미지 분포를 비교한다. MaRVL 내에서 이미지 분포가 여전히 언어별로 다양하다는 것을 알 수 있다. 이것은 대부분 다른 개념 세트 때문에 발생한다. 그림에서 보여지듯이, 서로 다른 클러스터는 두 지역이 매우 다른 동물 종을 가지고 있기 때문이다. ResNet50가 ImageNet에서 사전 학습되었으므로 형성된 클러스터는 ImageNet 개념으로 편향될 수 있다. 그림 4 (상단)에서 제안된 것처럼, NLVR2 이미지는 일반적으로 MaRVL의 중국어 이미지보다 더 잘 클러스터링된다.

<span style='color:green;font-weight:bold'> Multilingual and multicultural statistics </span><br>
<br>
저자는 MaRVL의 다중 언어 및 다문화 개념의 주요 통계를 ImageNet 및 NLVR2의 개념과 비교한 Fig. 2를 제시한다. MaRVL의 개념은 언어별로 구분되지만 ImageNet 및 NLVR2의 개념보다 더 많은 언어에서 발견된다. 저자는 이것이 MaRVL의 개념이 더 원형적이며, 더 많은 이웃 문화를 반영하기 때문이라고 추측한다. Fig. 2 의 중간 및 오른쪽 그래프는 MaRVL의 더 많은 개념이 더 많은 언어 군과 매크로 지역에서 발견되는 것을 보여줌으로써 이를 검증한다.

<span style='color:green;font-weight:bold'> Limitations. </span><br>
<br>
저자들은 가장 많은 언어를 커버하는 국제 주석 플랫폼 (proz.com 및 prolific.co)을 선택했지만, 저조한 자원을 가진 언어를 구사하는 사용자를 모집하는 것이 여전히 어려운 문제로 남아있다. 캡션 작성을 위해 언어 당 2-4명의 자격이 있는 주석 작성자를 찾을 수 있었다. 이는 개별 주석 작성자의 편향을 더 크게 나타낼 수 있다. 본 연구의 저자들 중에는 일부 언어를 원어민으로 구사하지 못하는 경우도 있다. 또한, 모든 개념은 위키백과 페이지에 매핑되어 있다. 자원이 적은 언어의 경우, 일부 개념에 대한 위키백과 페이지가 누락될 수 있다. 마지막으로, 각 의미 분야당 대략 5개의 개념만 선택된다. 이는 불균형적으로 자주 등장하는 개념이 서로 다른 범주에 분배되어 편향을 유발할 수 있다. 일반적으로, MaRVL 프로토콜은 여전히 개선될 여지가 있지만, 데이터셋 제작자의 편견을 최소화하기 위한 목표는 부분적으로 달성되었다.

# Baselines

Vision-and-Language 작업을 위한 여러 사전 훈련된 Transformer 모델들이 제안되었다. 이들은 BERT 구조에서 영감을 받아 다중 모달 입력을 처리하도록 재설계되었다. 이들은 대개 영어로만 제공되는 대규모 이미지-텍스트 말뭉치(Sharma et al., 2018)에서 사전 훈련된다. M3P 모델(Ni et al., 2021)은 Unicoder-VL(Li et al., 2020a)을 확장하여 다국어 입력을 인코딩하는 BERT와 유사한 아키텍처 중 첫 번째 다국어 다중 모달 BERT 아키텍처를 만들었다. 사전 훈련은 다중 모달 영어 데이터와 텍스트만 있는 다국어 데이터를 모델링하는 것을 번갈아 수행한다. 이 논문에서는 이 접근 방식을 따르고, mBERT(Devlin et al., 2019)로 UNITER를 초기화하여 얻은 mUNITER와 XLM-RBASE(Conneau et al., 2020)로 UNITER를 초기화하여 얻은 xUNITER의 두 가지 다국어 변형을 제안한다.

UNITER 아키텍처는 BERT-BASE와 유사한 Transformer 계층 스택으로 구성되어 있으며, 입력은 언어와 비전 임베딩의 concatenation 이다. 언어 입력은 먼저 서브워드 단위로 분할(Wu et al., 2016; Sennrich et al., 2016)되고, {[CLS], w1,...,wT , [SEP]}와 같이 두 개의 특수 토큰으로 둘러쌓인다. 언어 임베딩은 BERT 아키텍처와 동일하게 얻어진다. 비전 입력은 사전 훈련 된 객체 검출기로부터 주어진 일련의 시각적 특징으로 구성되며, 전체 이미지를 인코딩하는 특수 기능 [IMG]를 추가한다. {[IMG], v1,..., vK} 각 특징은 입력 위치로 바운딩 박스 좌표를 사용하여 BERT와 유사한 임베딩 계층을 사용하여 임베딩된다. 마지막으로, 이미지-텍스트 쌍에 대한 전역 표현은 곱셈 풀링(multiplicative pooling) (Lu et al., 2019)을 통해 얻어진다. 이 때, [CLS] 토큰에서 추출된 텍스트 모드의 풀링 표현과 [IMG] 특징에서 추출된 시각적 모드의 풀링 표현이 요소별로 곱해져 이미지-텍스트 쌍을 위한 단일 벡터가 생성된다.

저자는 VOLTA에서 모델을 코딩하고, Bugliarello et al. (2021)이 제안한 제어된 설정과 동일한 데이터와 하이퍼파라미터를 사용하여 Pre-train 한다. 이를 통해 다국어 버전의 성능을 해당 단일 언어 버전과 공정하게 비교할 수 있다. 그 후, 저자는 Lu et al. (2020)에서 처음 제안된 방법을 따라 NLVR2에서 모델을 Fine-tuning 한다. 영어 fine-tuning 후, 다국어 모델은 'zero-shot' cross-langugage transfer setting 에서 MaRVL에서 테스트된다. 또한 VOLTA에서 사용 가능한 다섯 개의 단일 언어 vision-and-language BERT 모델의 성능도 벤치마킹한다: UNITER, VL-BERT (Suet al., 2020), VisualBERT (Li et al., 2019a), ViLBERT (Lu et al., 2019) 및 LXMERT (Tan and Bansal, 2019). 이러한 모델들도 동일한 제어된 설정에서 pre-training되고, NLVR2의 영어 training set 에서 fine-tuning 된다. 교차 언어 전이에 대한 '번역 테스트' 접근 방식을 따라 (Banea et al., 2008; Conneau et al., 2018; Pontiet al., 2021b), 이들 모델은 MaRVL의 테스트 세트에서 영어로 자동 번역된 결과를 평가한다.

![image](https://user-images.githubusercontent.com/42200027/227849736-9a02f10e-670a-4489-a464-8ef726691d58.png)
![image](https://user-images.githubusercontent.com/42200027/227849764-2a18508a-cd3a-4e58-b567-a07ea6a4cccc.png)

# Results
Baseline 모델들의 MaRVL에서의 성능을 Tab. 4에서 볼 수 있다.  모든 예제에 대한 정확도와 모든 해당 이미지 쌍에 대한 예측이 올바른 고유 문장의 비율인 일관성이라는 두 가지 지표를 report 한다. 특정 전이 방법에 대한 모든 모델 간의 차이가 통계적으로 유의하지 않음을 알 수 있다. 이는 같은 양의 데이터에서 사전 학습된 경우, 신경망 구조를 다르게 하는 것이 성능에 큰 영향을 미치지 않음을 나타낸다.

<span style='color:green;font-weight:bold'> Zero-shot vs. translate test. </span><br>
<br>
다국어 및 단일 언어 모델 모두 영어(NLVR2)에서 비슷한 성능을 보인다. 그러나 MaRVL에서 평가할 때, 영어를 제외한 언어에서는 제로샷 다국어 기준선의 성능이 10-20% 포인트로 급격히 하락하여, 기회수준 이상의 성능을 보인다. 놀랍게도, 이는 레이블되지 않은 텍스트가 풍부한 만다린어(ZH)와 같은 자원이 풍부한 언어에도 해당된다. 번역 테스트 기준선은 다른 언어에서 4-15%의 향상을 보이며, 터키어가 가장 많이 개선되었다. 그러나, NLVR2의 영어 성능과 비교하면 10% 이상의 상당한 차이가 있다. 이는 MaRVL의 데이터가 분포 밖에 있기 때문이라고 추측할 수 있다.

<span style='color:green;font-weight:bold'> Disentangling shifts in distribution. </span><br>
<br>
 MaRVL이 어려운 이유가 크게 두 가지 있다:

1) *cross-lingual transfer* 와 2) 영어 데이터셋과 관련하여 이미지와 설명의 분포가 다른 *out-of-distribution* 이다. 이 두 가지 요소가 모델 성능에 미치는 영향을 평가하기 위해, 저자는 중국어 버전인 MaRVL-ZH를 대상으로 제어된 연구를 실시한다. 먼저, 저자는 MaRVL-ZH 를 수동으로 영어로 번역하여 기계 번역으로 인한 가능한 혼란을 제거한 후, Tab. 4에 나와있는 결과를 비교한다. Tab. 5(왼쪽 열)에 나와있는 것처럼, 번역 테스트 평가와 비교하여 mUNITER를 제외한 모든 모델이 정확도를 1-2%밖에 개선하지 못했기 때문에 번역은 꽤 신뢰할 수 있다고 결론을 내린다. 또한, 분포가 다른 개념들은 (평균적으로 정확도 10% 하락) 가장 많은 오류를 유발한다. 두 번째로, NLVR2 테스트 세트에서 1,000개의 데이터 포인트에 해당하는 250개의 고유한 설명을 샘플링하여 중국어로 수동 번역한다. 이를 NLVR21k라고 명명하고, 이 하위 집합에서 mUNITER와 xUNITER의 성능을 Tab. 5(오른쪽 열)에 나와있는 것처럼 나열한다. 모든 데이터 포인트가 도메인 안에 있지만, 저자의 다국어 모델 mUNITER와 xUNITER는 영어 NLVR2 1k 테스트 세트(중앙 열)와 비교하여 정확도가 16% 하락한다. 따라서 이 차이는 영어에서 중국어로의 다국어 전이로 설명될 수 있다.

<span style='color:green;font-weight:bold'> Translate train. </span><br>
<br>
마지막으로, '번역 훈련'이라는 세 번째 가능한 cross-language transfer 방법에 대한 베이스라인을 수립한다. 이를 위해 NLVR2의 training set을 중국어로 기계 번역하고, 이를 MaRVL-ZH에서 평가한다. mUNITER(62.5/18.7)와 xUNITER(61.8/16.7)의 성능은 '번역 테스트'에서 MaRVL-ZH를 영어로 기계 번역하는 경우와 거의 동일하다. 다시 한번 문화적으로 관련된 개념에 대한 접근 불가능성이 일반화를 방해하는 것으로 나타났다.

# Conclusions and Future Work
현재 존재하는 시각-언어 데이터셋의 이미지와 개념이 영어 외의 많은 언어와 유럽과 북아메리카 이외의 문화권에서는 중요하지 않거나 대표적이지 않다는 것을 밝혀내었다. 이러한 편향성을 완화하기 위해, 저자는 이미지와 캡션의 선택을 완전히 원어민들이 결정하는 새로운 주석 프로토콜을 개발했다. 또한, 인도네시아어, 중국어, 스와힐리어, 타밀어, 터키어의 다양한 언어에서 이미지 쌍을 비교하고 대조하는 설명을 수집하여 이를 기반으로 다양한 언어와 문화권에 대응하는 다문화 및 다국어 데이터셋인 MaRVL을 공개하였다. 이를 바탕으로, 다양한 다국어 및 다모달 베이스라인 모델을 개발하고 평가하여, 이 모델들의 성능이 영어 데이터셋과 비교해 꽤 낮은 수준이라는 것을 발견하였다. 이는 MaRVL이 영어 문화권 이외의 실제 적용 분야에서 모델의 적절성을 더 정확하게 평가할 수 있다는 것을 보여준다. 이에 따라, 앞으로는 MaRVL을 기반으로 객체 인식과 같은 다른 작업들에 대한 모델 성능 평가를 진행할 예정이며, 비교 학습을 기반으로한 다국어 확장 모델을 실험할 것이다.
