---
layout: post
title:  "[EMNLP2023] Ditto: A Simple and Efficient Approach to Improve Sentence Embeddings"
date:   2024-03-06 17:00:00 +0900
use_math: true
categories: [PLM]
---

**Qian Chen, Wen Wang, Qinglin Zhang, Siqi Zheng, Chong Deng, Hai Yu, Jiaqing Liu, Yukun Ma, Chong Zhang**
<br> Speech Lab, Alibaba Group &emsp;

[[pdf]](https://aclanthology.org/2023.emnlp-main.359.pdf)

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/aff30a3c-abc0-42e1-af28-647086efdf67)

## Abstract
- (**anisotropy bias in BERT sentence embedding**) 저자들은 BERT sentence embedding 이 uninofrmative word 에 대한 anisotropy bias 가 있어, semantic textual similarity (STS) task 를 수행하는데 어려움이 있음을 지적한다.
- (<span style='color:green;font-weight:bold'> Ditto </span>) 이것을 해결하기 위해 저자들은 **Di**agonal A**tt**ention P**o**oling (**Ditto**) 라는 unsupervised approach 를 제안한다. 이 방법은 model-based importance estimation 을 통해 word 의 weight 을 계산하고, 이후 이 weight 의 average 를 통해 sentence embedding 을 얻는다. Ditto 는 BERT 뿐 아니라 어떠한 PLM 에도 적용될 수 있다.
- (**No use of param**) 다른 sentence embedding 들과 다르게 Ditto 는 어떠한 추가적인 parameter 도 요구하지 않는다.
- (**Experiment**) Ditto 는 BERT 와 다르게 anisotropy bias 문제가 발생하지 않으며, 따라서 STS task 에서 좋은 성능을 보여준다.

## 1. Introduction
<span style='color:green;font-weight:bold'> ▶ Bias in BERT sentence embedding </span>
<br> 
BERT, RoBERTa, ELECTRA 등의 Pre-trained language models (PLMs) 이 매우 좋은 성능을 보여주는 것은 사실이지만, 여러 연구에서 BERT 의 sentence embedding 이 GloVe 보다도 좋지 못하다는 주장이 제시되었다.
특히 <span style='background-color: #ffdce0'> anisotropy bias </span> 이 심하다는 문제가 제기되었는데, 이는 original BERT 가 생성하는 sentence embedding 이 어느 pair 에 대도 높은 similarity 를 보인다는 문제점이다. 이는 BERT sentence embedding 을 활용하여 Semantic Textual Similarity (STS) task 를 푸는데 문제가 될 수 있다.

<span style='color:green;font-weight:bold'> ▶ Improving sentence embeddings from PLMs </span>
<br> 
PLM 의 sentence embedding 을 발전시키는 방법은 크게 세 가지로 분류 된다. 

- (1) learning-free method

anisotropy bias 가 token frequency 같은 tatic token embedding 에서 비롯되었다고 보고, *static remove biases avg* 방법론을 통해 top-frequency token 들을 없애고 남은 token 들의 average 로 embedding 을 구성하는 방법으로 해결한다.

이 방법은 BERT 의 contextualized representation 을 활용하지 않기 때문에, informative word 가 적을 수 있는 short sentence를 잘 표현하지 못한다는 단점이 있다. 

또한, prompt 를 이용한 learning-free method 가 존재하는데, 이는 "This sentence: [original sentence] means [MASK]" 라는 prompt 에서 MASK 토큰을 채우는 방식이지만, 이는 input length 가 길어져 cost 가 많이 들며, ELECTRA 같은 MASK 토큰을 쓰지 않는 모델에는 적용될 수 없으며, prompt 에 크게 의존하여 reliability 가 떨어진다는 단점이 있다.

- (2) extra-learning method

PLM 의 parameter 는 고정하고, 추가적인 학습을 통한 방법이 두 번째이다.
대표적으로, BERT-flow 가 있고, 이는 flow-based generataive model 을 도입하여 BERT 의 anisotropy problem 을 해결하는데, BERT sentence embdding distribution 을 smooth and isotropic Gaussian distribution 으로 transform 하는 방식이다.

- (3) updates parameter

마지막은 BERT 를 포함한 PLM 의 param 을 update 하는 방법이다.
특히, NLI 와 STS dataset 을 통한 추가학습으로 이것들을 잘하게끔 sentence embedding 을 유도 학습하는 방법이다. SimCSE 등이 대푲거인 방법이다.

이 논문에서는 위의 방법들과는 다른 새로운 learning-free method 인 Ditto 를 소개한다.

## 2. Analyze BERT Sentence Embeddings
<span style='color:green;font-weight:bold'> ▶ Observation 1: The compositionality of informative words is crucial for high-quality sentence embeddings. </span>
<br>
Perturbed masking 방법은 sentence 속의 token 두 개를 masking 하여, 각각의 토큰이 서로에게 어떠한 영향을 미치는지 분석하는 방법이다.
이 논문에서는 BERT 와 SBERT 에 대해서 분석을 해보는데, 아래의 그림과 같이

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/549f6854-92d5-4c68-8e32-d079abc7a6bd)

<span style='background-color: #dcffe4'> 
SBERT 의 경우, "social media", "Capitol Hill" 같은 informative word 에 prominent vertical line 이 있는 것을 볼 수 있다. BERT 에서는 이러한 현상이 관측되지 않기 때문에, 저자들은 informative token 이 high-quality sentence embedding 의 strong indicator 라는 것을 가정한다.
 </span>

 ![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/2b1f4f7f-d9a4-49fe-9cd2-e4eef2637a2b)

또한 위의 TF-IDF 에서 word 의 중요도(importance) 측정에서도 비슷환 경향성을 보인다. 
SBERT 의 impact matrix 가 더 높은 TF-IDF 와의 correlation 을 보인다.
ELECTRA 는 이 correlation 이 낮고, 역시 STS task 에서의 성능이 매우 안좋다.
이에 저자들은 <span style='background-color: #dcffe4'> BERT 와 ELECTRA 가 uninformative word 에 bias 되어있는 것이 문제라고 지적한다.  </span>

<span style='color:green;font-weight:bold'> Observation 2: Certain self-attention heads of BERT correspond to word importance. </span>
<br>

위의 표에서 TF-IDF 의 경우, BERT 는 ELECTRA 와 달리 준수한 correlation 을 보인다.
따라서 저자들은 BERT 에도 informative word 가 잘 encode 되어있지만, 외재적으로 발현이 되지 않았을 가능성을 지적한다.

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/956858cf-72de-4107-9390-830df89a6b6c)

위의 그림과 같이 BERT 를 분석한 결과, BERT 에서는 informative word 끼리 높은 "diagonal value" 를 가지는 것을 확인한다. 

## 3. Diagonal Attention Pooling

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/d99db8ec-de80-406c-b1a8-198dc6155ec6)

위의 두 발견에 따라, 저자들은 Diagonal Attention Pooling (Ditto)  방법을 제안한다.
위의 Figure 와 같이, 기존의 BERT 에서 last hidden layer 까지의 hidden state 를 average 하는 것과 달리, 첫 번째 hidden layer 만 쓰거나, 처음과 마지막의 hidden layer 의 평균을 사용하여 sentence embedding 을 사용한다. 
이후, Ditto 는 hidden state 를 특정 head 의  diaognal attention 을 이용하여 weight 하여 sentence embedding 을 구성한다.
따라서 Ditto 는 추가적인 학습 없이 sentence embedding 을 표현할 수 있는 learning-free method 라 효율적이다.

## 4. Experiments and Analysis

- Ditto 는 매우 효율적이면서도 성능이 좋은 sentence embedding 방법론이다.
 
![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/3fee60fe-b866-4229-8bb9-6b11f6ac37be)

- Head 별 Ditto 성능 비교.

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/fd7ef98e-31bd-4663-997f-6e1f729bed1c)

- Ditto 와 learning-free baseline 의 cosine similairty

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/bfea4aa7-353a-4c9c-8a6a-0886da2542ae)



## Conclusion
```
We propose a simple and learning-free Diagonal Attention Pooling (Ditto) approach to address the bias towards uninformative words in BERT sentence embeddings. Ditto weights words with modelbased importance estimations and can be easily applied to various PLMs. Experiments show that Ditto alleviates the anisotropy problem and improves strong sentence embedding baselines.
```
