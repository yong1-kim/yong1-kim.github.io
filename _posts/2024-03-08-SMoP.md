---
layout: post
title:  "[EMNLP2023] SMoP: Towards Efficient and Effective Prompt Tuning with Sparse Mixture-of-Prompts"
date:   2024-03-08 15:00:00 +0900
use_math: true
categories: [LLM, PLM]
---

[[pdf]](https://aclanthology.org/2023.emnlp-main.884.pdf) &emsp;
[[github]](https://github.com/jyjohnchoi/SMoP)

**Joon-Young Choi, Junho Kim, Jun-Hyung Park, Wing-Lam Mok, SangKeun Lee**
<br> Department of Artificial Intelligence, Korea University, Seoul, Republic of Korea &emsp;

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/df1bdcd1-e93a-466b-a511-7659476a7e3f)

## Abstract
- (**Ineffeciency in Prompt tuning**) Prompt tuning 은 finetuning 을 대체하는 효율적인 학습 방식이지만, 기존의 prompt tuning 은 100 token 이상을 사용하여 inefficiency 가 존재한다.
- (<span style='color:green;font-weight:bold'> SMoP </span>) 저자들은 SMoP (Sparse Mixture-of-Prompts )라는, short soft prompt 를 활용하는 방법론을 제안한다. SMoP 는 data 의 다른 subset 을 각각 specialized handling 하는 short soft prompt 여러 개를 gating mechanism 을 이용해 학습에 활용한다.
- (**Experiment**) 실험 결과, SMoP 는 training-inference cost 를 줄이면서 basline method 를 outperform 한다.

## 1. Introduction
<span style='color:green;font-weight:bold'> ▶ Prompt tuning </span>
<br>
Prompt tuning 은 Fine-tuning 을 대체할 parameter-efficient alternative tuning 방식으로 최근 주목을 받고 있다.
이 방식은 보통 기존의 LM param 은 freeze 하고 soft prompt 를 solely tuning 하여 mode input 앞단에 prepend 하는 방식으로, 효율적이면서도 강력한 성능을 보여준다.
여러 prompt tuning 기법이 제안이 되는 과정에서, <span style='background-color: #ffdce0'> 더 나은 성능을 보이기 위해 더 긴 prompt 가 사용이 되어 왔다. </span>
최근에는 typically 100 token 이 넘는 soft prompt length 가 model performance 향상에 좋다고 알려졌지만, 그 computational requirement 에 대한 고려는 거의 없었다.

<span style='color:green;font-weight:bold'> ▶ SMoP : Sparse Mixture-of-Prompts </span>
<br>
이에 저자들은 **SMoP**(**S**parse **M**ixture-**o**f-**P**rompts) 라는 방법을 제안한다.
SMoP 는 training 와 inference 단계에서 short soft prompt 를 활용한다.
<span style='background-color: #dcffe4'> 
Sparsely-Gated Mixture-of-Experts (MoE) 에 영감(inspriation) 을 받아서, 각각 data 의 subset 에 specialized handling 이 가능한 short soft prompt 여러개를 활용하는 방법이다.
 </span>

아래 그림에서, 기존의 prompt tuning 은 100 토큰이 될 때, 오히려 Training memory 를 finetuning 보다 더 사용하기도 한다. 그러나 SMoP 는 그러한 문제가 전혀 발생하지 않는 효율적이면서도 좋은 성능을 보이는 방법론이다.
 
![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/add1757a-8e3a-49fa-b0fc-e95bcc9b205c)

실험결과, SMoP는 SuperGLUE benchmark 에 대하여, T5-base 와 T5-large 에 대해, 기존의 prompt tuning 방법론보다, training time, memory, inference computation 에서 효율적이면서도 좋은 성능을 보인다.


![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/07b6d773-71f4-492e-a81d-efeabd2e05c0)


## 2. Method
# 2.1. Preliminaries

# 2.2. SMoP: SParse Mixture-of-Prompts

# 2.3. Router Perturbation

## 3. Experiments
# 3.1. Experimental Settings

# 3.2. Results

## Conclusion

## Limitations



<span style='color:green;font-weight:bold'> 초록색볼드체 </span>

<span style='background-color: #dcffe4'> 초록색배경 </span>
<span style='background-color: #ffdce0'> 빨간색배경 </span>
