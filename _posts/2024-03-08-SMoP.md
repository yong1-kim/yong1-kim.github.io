---
layout: post
title:  "[EMNLP2023] SMoP: Towards Efficient and Effective Prompt Tuning with Sparse Mixture-of-Prompts"
date:   2024-03-08 15:00:00 +0900
use_math: true
categories: [LLM, PLM]
---

[[pdf]](https://aclanthology.org/2023.emnlp-main.884.pdf) &emsp;
[[github]](https://github.com/jyjohnchoi/SMoP)

**Joon-Young Choi, Junho Kim, Jun-Hyung Park, Wing-Lam Mok, SangKeun Lee**
<br> Department of Artificial Intelligence, Korea University, Seoul, Republic of Korea &emsp;

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/df1bdcd1-e93a-466b-a511-7659476a7e3f)

## Abstract
- (**Ineffeciency in Prompt tuning**) Prompt tuning 은 finetuning 을 대체하는 효율적인 학습 방식이지만, 기존의 prompt tuning 은 100 token 이상을 사용하여 inefficiency 가 존재한다.
- (<span style='color:green;font-weight:bold'> SMoP </span>) 저자들은 SMoP (Sparse Mixture-of-Prompts )라는, short soft prompt 를 활용하는 방법론을 제안한다. SMoP 는 data 의 다른 subset 을 각각 specialized handling 하는 short soft prompt 여러 개를 gating mechanism 을 이용해 학습에 활용한다.
- (**Experiment**) 실험 결과, SMoP 는 training-inference cost 를 줄이면서 basline method 를 outperform 한다.

## 1. Introduction




<span style='color:green;font-weight:bold'> 초록색볼드체 </span>

<span style='background-color: #dcffe4'> 초록색배경 </span>
<span style='background-color: #ffdce0'> 빨간색배경 </span>
