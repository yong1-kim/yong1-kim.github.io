---
layout: post
title:  "[Arxiv 2401] Blinded by Generated Contexts: How Language Models Merge Generated and Retrieved Contexts for Open-Domain QA?"
date:   2024-03-13 22:08:00 +0900
use_math: true
categories: [Retrieval, LLM, PLM]
---

[[pdf]](https://arxiv.org/pdf/2401.11911.pdf)  &emsp;

**Hexiang Tan<sup>♠♡</sup>, Fei Sun<sup>♠†</sup>, Wanli Yang<sup>♠♢</sup>, Yuanzhuo Wang<sup>♠</sup>, Qi Cao<sup>♠</sup>, Xueqi Cheng<sup>♠♡</sup>**
<br><sup>♠</sup> CAS Key Laboratory of AI Safety & Security, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China <sup>♡</sup> University of Chinese Academy of Sciences, Beijing, China <sup>♢</sup> Nankai University, Tianjin, China &emsp;

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/efe5bdb0-6f48-44eb-b90b-13b14e7ac705)

# Abstract
- (**merging generated context with retrieved context**) Retrieval-augmented generation task 에 대하여, LLM 에 추가적인 정보를 위하여 LLM 스스로 혹은 다른 LLM 이 generated 한 context 를 merging 하려는 시도들이 증가하는데, 이에 대한 연구가 부족하다.
- (**Conflicting dataset**) 저자들은 generated context 와 retrieved context 중 하나에만 golden answer 아 있는 dataset 을 생성하여 reponse 의 origin 을 trace 하는 연구를 제안한다.
- (**Experiment**) 저자들은 실험에서 GPT-4/3.5, LLaMa2 에서 <span style='color:green;font-weight:bold'> generated context 를 favor 하는 significant bias </span> 를 발견한다. 또한, LLM-generated context 가 query 에 대해서는 훨씬 높은 relevancy 를 가지는 것을 발견한다.
- (**Takeaway**) LLM 이 diverse context 를 어떻게 merge 하는지 이해하며, 현재의 RALM 에 대한 진보에 기여할 수 있다.

## 1. Introduction
<span style='color:green;font-weight:bold'> ▶ Using Auxiliary info in LLMs </span>
<br>
최근 Knowledge-intensive task 에서 LLM 에 auxiliary information 을 활용하여 성능을 끌어올리는 연구들이 많이 존재한다. ([[1]](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00605/118118/In-Context-Retrieval-Augmented-Language-Models))
최근 여러 연구에서 Retrieval-augmented approach 를 대신하여, LLM 이 생성한 context 를 활용하는 generation-augmented apporach([[2]](https://aclanthology.org/2022.acl-long.225.pdf),[[3]](https://arxiv.org/pdf/2210.01296.pdf)) 를 차용한다.
대표적인 예시로, [GENREAD](https://arxiv.org/abs/2209.10063) 가 있다.

<span style='color:green;font-weight:bold'> ▶ Hybrid Approach </span>
<br>
최근 연구들([[4]](https://arxiv.org/abs/2209.10063),[[5]](https://aclanthology.org/2023.acl-long.546/)) 에서는 Retrieved context information 과 generated context information 을 합쳐서 넣는 hybrid approach 에 대한 방법론이 제시되고 있다.
그러나 이 hybrid approach 에는 significnat challenge 가 존재하는데, <span style='background-color: #ffdce0'> diverse source 의 conflict 가 information integration 의 effectiveness 를 impede 한다([[7]](https://aclanthology.org/2023.emnlp-main.286/))</span>는 것이다.
이 연구에서는 LLM 이 이 generated-retreived context 사이 conflict 를 어떻게 resolve 하는지를 탐구한다.

<span style='color:green;font-weight:bold'> ▶ How LLMs handle conflict between retrieved info and generated info </span>
<br>

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/35150b62-1d7b-4e4d-8b56-abcf139bf121)

저자들은 특별한 케이스에 대하여, hybrid approach 가 위의 그림처럼 실패하는 것을 보인다.
이 이유를 탐구하기 위해, <span style='background-color: #dcffe4'> LLM 이 merging 하는 과정을 나눠서 분석하는 systematic framework 을 제시한다. </span>
저자들은 generated and retrived context 중 하나에만 정답이 있는 conflicting dataset 을 의도적으로 생성한 뒤, LLM 이 어떤 context 를 고르는지를 탐구한다.

<span style='background-color: #dcffe4'> 
여러 실험 결과, GPT-4/3.5, LLaMa2 같은 SOTA LLM 들에서 generated context 를 favor 하는 siginficant bias 를 발견한다. </span>
추가적으로, 이 genreated context 가 LLM 스스로 만든 것이든, 다른 LLM 이 만든 것이든 상관없이(regardless) 같은 결과가 나온다는 것이다.
따라서, LLM 들이 parameter knowledge 와 external information 사이의 conflict 가 있을 때, 어떻게 merging 하여 사용할 것인가에 대해 critical challenge 가 있음을 보인다.
<span style='background-color: #dcffe4'> 이 과정에서 confirmation bias 가 아닌 text similiarity 가 LLM 이 context 를 선정하는 key factor 임을 보인다.
 </span> 

## 2. Background & Study Formulation
# 2.1 Background
Retrieval approach, generation-augmented approach, 그리고 hybrid approach 에 대한 도식은 아래와 같다.

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/ef36d6b0-8bf7-417c-938b-1c67fc630076)
  

# 2.2 Answer Tracing Task

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/f78e5d93-f36a-4902-a203-7d0b1134a5b8)

저자들은 answer 가 generated context 와 retrieved context 중 어떠한 것에서 비롯되는지를 탐구하는 answer tracing task 를 제안한다.
Task 를 풀 때는 LLM zero-shot setting 을 활용한다.

## 3. Experimental Setup
# 3.1 Context-Conflicting Datasets

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/8d684c80-a1d2-42d8-836f-a0684306fde7)

실험을 위해, retrieved context 와 generated context 사이에 정답이 하나만 존재하는 context-conflicting dataset 을 만든다.
그 생성 criteria 는 **Traceability** (ANSWER는 반드시 어떠한 context 에 support 된다) 와 **Exclusitvity** (ANSWER 는 반드시 둘 중 하나의 context 에만 support 된다) 이다.

데이터 생성에는 NaturalQuestion (NQ) 와 TriviaQA 의 golden answer 를 활용하였다.

<span style='color:green;font-weight:bold'> Step 1. Context Preparation </span>
<br>
Retriever 로는 [Contriever](https://arxiv.org/abs/2112.09118) 의 top-1 ranked passage 를 활용한다. 참고로 Contriever 는 최근 RALM 에도 사용되는 명실 상부 강력한 off-the-shelf retriever 중 하나이다.

Generator 로는 GENREAD framework 을 따라 LLM 을 활용한다. 재현성을 위해 temperature 는 0 으로 한다. 대부분 Retriver 가 100 work 정도 context 를 가져오는데 반해, generator 는 250 word 가 넘게 길게 생성하는데, <span style='background-color: #dcffe4'> 이 length discrepancy 도 하나의 potential effect 일 수 있으므로 3% 정도의 discrepancy 가 되게 length constraint 를 부여한다. </span>

<span style='color:green;font-weight:bold'> Step 2. Sample Flitering </span>
<br>
Traceability 를 확보하기 위한 filtering 과정을 거친다.
즉, ANSWER 가 Retrieved context 와 Generated Context 중 하나에라도 support 되는 것만 남기고 버려진다. ANSWER 가 둘 중 하나라도 support 되지 않고, intrinsic parameter knowledge 에 의존하는 경우는 버리는 것이다.

<span style='color:green;font-weight:bold'> Step 3. Building Dataset </span>
<br>
Exculsivity 를 확보하기 위해 ANSWSER 가 only one context 에 의존하는 case 만 남기고 filtering 한다.
이 떄, Retrieved context 에 의존하는 경우를 *AIR* 로, Generated context 에 의존하는 경우를 *AIG* 로 명명한다.

# 3.2 Statistics of Datasets

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/a3d3a3e4-7926-4d64-9d11-a5ca3ca27a56)

Generator 와 Reader 로 활용된 LLM 모델에 따른 statitsics 는 위의 표와 같다.
NQ 와 TriviaQA 의 10% 정도 내외의 작은 portion 만이 해당되는 것을 볼 수 있다.
**GPT-4 는 conflicting instance 의 양이 적은데, 이는 retrieved or generated context 를 활용하는 능력이 뛰어나기 때문이라고 해석한다**

# 3.3 Evaluation Metric

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/fa5f41e1-af48-4b0f-812a-09657ce1556a)

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/a4e156be-02d1-46da-8e30-6693d95c5b0f)

DiffGR 이라는 [-1,1] scale 의 metric 을 제안한다.
AIR 케이스, 즉 answer 가 retrieved context 에서 온 경우에 대하여, Ideal LLM 의 DiffGR 값은 -1 이 될 것이다.

## 4. How LLMs Merge Contexts?
# 4.1 LLMs Prefer Self-Generated Contexts
<span style='color:green;font-weight:bold'> EM Results </span>
<br>

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/6f5a6e25-f94e-4f15-886d-8b7603f7b0f5)

- <span style='background-color: #dcffe4'> LLM 이 AIR 데이터셋에 매우 낮은 성능을 보이면서 AIG 에서는 매우 높은 성능을 보여 generated context 에 매우 크게 의존함을 알 수 있다. </span>

<span style='color:green;font-weight:bold'> DiffGR Results </span>
<br>

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/0b9c98cb-ca61-4ecc-8031-e5ca8f892150)

- <span style='background-color: #dcffe4'> Ideal LLM 이라면  AIG 의 경우 1, AIR 의 경우 -1이 나와야 하는데, 위의 그래프에서 AIR 도 양수가 나오기 때문에, AIR 을 잘 못하고 Generated context 에 크게 의존함을 알 수 있다. </span>


# 4.2 LLMs Broadly Prefer Generated Contexts

4.1 의 결과는 LLM 이 스스로 만든 self-generated context 를 선호하는 경향성을 확인시킨다. 그렇다면 다른 LLM 이 만든 generated context 에도 의존할까?

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/acbdc40a-124c-4276-91e3-cac5050b49bd)

- <span style='background-color: #dcffe4'> ) LLMs also biased towards contexts
generated by other LLMs. </span>
- <span style='background-color: #dcffe4'> )LLMs usually exhibit a stronger bias to contexts generated by themselves.  </span>


## 5. Why LLMs Prefer Generated Contexts
이 절에서는 Confirmation bias, text similarity, context completeness 세 가지 측면에서 *why LLMs prefer generated contexts rather than retrieved contexts from several perspectives* 를 분석한다.

# 5.1 Effect of Confirmation Bias

한 연구([[9]](https://arxiv.org/abs/2305.13300)) 에서 parametric knowledge 에 consistent 한 context 를 선호한다는 발견이 있었다.
저자들은 single LLM 을 generator&reader 로 쓰는 경우, generated-context 를 paramteric kenowledge 라고 해석하고, confirmation bias 가 generated context preference 에 영향을 미치는지 분석한다.

저자들은 generated context 가 LLM's parametric knowledge 에 align 되는 것을 방해하고자, counter-memory context 를 만든다.
이 것은 original generated context 와 답이 다른 answer 로 이뤄진다. 이 counter-memory context 를 활용하여 DiffGR 을 새로 측정한다.

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/bd240fd2-aeb8-4803-9426-eecad608673a)

위의 표에서, LLM 의 parameteric knowledge 에 inconsistent 한 counter-memory 에서도, 여전히 generated context 를 선택하는 경향을 보인다. 따라서, <span style='background-color: #dcffe4'> confirmation bias 는 key factor 가 아님을 확인한다. </span>
특히, GPT-3.5 의 경우, TQA-AIR 에서 counter-memory 의 경우에서도 무려 0.8010 DiffGR 점수를 보여, 맹목적으로 generated context 를 좇는다는 것을 볼 수 있다.

# 5.2 Effect of Text Similarity

두 번째로, context 와 question 의 text similarity 가 영향을 미치는지 분석한다. Text similirity metric 으로는 BERTScore 와 Jaccard Similarity 로 semantic, lexical  similarity 를 모두 분석한다.

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/d2fdcabe-0b97-4d24-8b69-18785aab6630)

<span style='background-color: #dcffe4'> 위의 결과에서, retrieved context 의 similarity 가 모두 낮아, text similarity 와의 연관성이 큰 것을 확인할 수 있다. </span>
추가적으로, 아래의 simialrity gap 을 정의하여, 

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/512746f0-410d-4ace-85c1-e389459671fe)

실험한 결과는 아래와 같다.

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/a6ece662-bef4-4add-b8d4-ce5a13536911)

두 결과를 통해 아래의 결론을 낼 수 있다.
**"LLMs exhibit an increased bias to generated contexts on slices with a larger average similarity gap"**

# 5.3 Effect of Context Completeness

Retrieval 과정은 보통 fixed length truncation 을 차용하여 가져오기 때문에 context 의 완성도(completeness)가 혹시 LLM 의 선택에 영향을 미치는지 분석한다. 아래의 표에서 처럼, Nature 방법은 truncation 하지 않은 것과, 토큰 단위로 truncation (문장이 잘릴 수 있음), 문장 단위로 truncation (문장이 잘리지는 않음) 으로 실험을 세팅한다.
![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/c525ded3-bfc1-4bca-b1ea-86c3d0552cbd)

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/ea30cd30-b0bb-466a-8824-6abc6fcb9d23)

결과는 위의 표와 같다.
Truncation 과 S-Truncation 을 비교했을 때, 실험 결과가 크게 차이나지 않기 때문에, 문장의 완성도 자체는 key factor 가 아니다. 그러나, 앞서 언급했던 generated context 의 length contraint 를 없애고, retrieved context 에 비해 훨씬 긴 context 를 생성하여 활용하게 하였을 때, (Nature vs S-Trunc) 큰 차이를 보인다. 따라서 아래의 결론을 낼 수 있다.
**"LLMs tend to favor contexts with enhanced semantic completeness"
**
## Conclusion & Future Work
```
In this study, we propose a framework to investigate the underlying mechanisms by which LLMs merge retrieved and generated contexts. Our results reveal a pronounced bias towards generated contexts in several LLMs (GPT 3.5/4 and Llama2- 7b/13b). We further identify two key factors that may contribute to this bias: higher similarity between generated contexts and questions, and the semantic incompleteness of retrieved contexts.
Our insights highlight the critical need for advanced integration methods that can validate and leverage information from both sources, moving beyond the current overreliance on generated contexts. Additionally, we find that LLMs display significant sensitivity to the semantic completeness of input contexts. This sensitivity necessitates improved passage segmentation strategies in current retrievalaugmented systems, thereby ensuring the preservation of intended meaning and the maximization of utility. Finally, addressing the challenges posed by highly relevant yet incorrect information generated by LLMs is an important direction for future research. It is crucial to develop methods for detecting and discounting misleading information produced by LLMs, especially as the volume of such content continues to escalate.
```
## Limitation
```
Our work has the following limitations:
• This study is confined to open-domain question answering, a representative knowledge-intensive task. The behavior of LLMs across a broader spectrum of natural language processing tasks remains to be further explored.
• This work does not propose specific solutions to effectively mitigate the observed bias, as we focus on revealing the phenomena and analyzing the causes.
• To create a controlled environment conducive to analysis, we utilize a single instance for each context type. LLMs face increasingly intricate conflict scenarios when handling multiple contexts from each type. These conflicts emerge not only between retrieved and internally generated contexts but also among the various contexts originating from the same source (Chen et al., 2022; Xie et al., 2023).
```
