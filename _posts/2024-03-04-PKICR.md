---
layout: post
title:  "[EMNLP2023] PK-ICR: Persona-Knowledge Interactive Multi-Context Retrieval for
Grounded Dialogue"
date:   2024-03-04 21:48:00 +0900
use_math: true
categories: [Dialogue, PLM]
---

[[pdf]](https://aclanthology.org/2023.emnlp-main.1020.pdf) &emsp;
[[github] ](https://github.com/minsik-ai/PK-ICR)

**Minsik Oh, Joosung Lee, Jiwei Li, Guoyin Wang**
&emsp;

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/5a42171e-e756-4d43-9111-93c4d715f8da)


# Abstract
- (**PK-ICR**) Persona 와 knowledge 를 jointly idnetify 해야하는 새로운 task 인 Persona and Knowledge Dual Context Identification (PK-ICR)을 제안한다.
- (**Grounding Retrieval Method**) Dialog 내의 모든 context 를 활용할 수 있는 새로운 grounding retrieval method 를 제안한다. 이 방법은 기존 QA retrieval model 보다 효율적이다.
- (**Null-positive rank test**) 추가적으로, semantiaclly dissimilar sample 에 대한 ranking performance 를 측정할 수 있는 null-positive rank test 를 제안한다.


## 1. Introduction

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/ae655c31-0670-4fe5-a633-553232d98dd8)

<span style='color:green;font-weight:bold'> ▶ PK-ICR: Persona-Knowledge Interactive Multi-Context Retrieval for Grounded Dialogue </span>
<br>
기존의 Dialog 연구에서는 대부분 Persona 에 대한 연구와 Knowledge Grounding 에 대한 연구가 독립적으로 시행되었었다.
<span style='background-color: #dcffe4'> 
이 연구에서는 두 가지를 jointly 다뤄야하는 Persona-Knowledge Daul Context Identification task 를 새로 제안한다. </span>
위의 Figure 와 같이, Persona, knowledge, dialog 사이의 interaction 을 다룬다.

<span style='color:green;font-weight:bold'> ▶ Contributions </span>
<br>
이 논문의 contribution 은 세 가지이다.
1. Persona and knowledge dual context retrieval methodology.
2. Framework for cross-task adaptation of dialogue context interactions.
3. Evaluating the hard-negative trait of Persona-augmented Dialogue

## 2. Methodology
이 Task 의 목적은 conversation turn 의 모든 component 들의 interaction 을 maximize 하는 것이다.

# 2.1. Knowledge Retrieval
![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/974f5b4e-cc58-45cc-a057-7b913092ec44)

위의 그림과 같이 {Persona} {Dialogue} 의 질문에 {Knowledge} 로 답하는 형태의 knowledge retrieval 을 해야 한다.

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/0b3b26e1-eb04-4a07-91f6-da7830a61d91)

위의 식에서, E 는 input 이고, Q,A,P,K 는 QA candidate 과 persona, knowledge pair 이다. D 는 dialog 이다.

이 것을 활용하여, 모든 pair i,j 에서 best knowledge 를 찾아야 한다. 따라서, 아래와 같이

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/b8e0cb5d-a275-4109-aeee-ba5301a8e139)

를 수행한다.

# 2.2. Persona Retrieval

Augmented Persona 를 이용하여 QA retrieval model 을 finetuning 한다.

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/a13a6d7a-f151-4f33-8bd7-ea3963ae62ab)

(4) 식에서 2.1. section 의 결과를 토대로, E' 에 대하여, 이 것을 이용하여 Model M 을 Finetuning 한다.

이후 아래와 같이, finetuned model Mf 를 활용하여, persona likelihood score 를 infer 한다.

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/5000a461-6c1e-4428-8748-22c954d4b88c)

최종적으로, retrieved grounding information 은 아래와 같이 정리된다.

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/3295f227-4199-4aff-9305-c3fd1dfb48dd)


# 2.3. Null-positive Rank Test

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/1f076c9c-b4dc-41eb-b7e6-90423435864e)

Score output 과 관계없이 sample 들에 대한 discriminative performance 를 solely 평가한다.
Persona-augmented Dialog 를 hard-negative sampling 

<span style='background-color: #dcffe4'> 
Can the model rank null-positive sample correctly in relation to non-trivially dissimilar augmented samples? </span>

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/f4903d37-a677-4556-a941-b6ed2915dfff)

## 3. Experiment Setup

- 데이터셋은 Customized Conversation 을 활용한다.
- Model 은 MS MARCO dataset 을 학습한 QA model 여러 가지를 활용한다.

## 4. Results
# 4.1. Knowledge Retrieval

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/cf012b9a-8010-4ff8-a2f7-93566d7d8b2e)

- <span style='background-color: #dcffe4'> Table 1 shows strong performance increase for our prompt input from dialogue-only model, confirming that all components of dialogue is important. </span>


# 4.2. Persona Retrieval

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/2465774d-8492-4ee8-8c46-fd590a276fde)

- <span style='background-color: #dcffe4'> Table 2 shows that fine-tuned Pi + D model has the best performance </span>
- Non-fine-tuned Pi + D model 에서는 낮은 성능을 보이는데, true knowledge 가 likelihood score 에 영향을 받기 때문이다.

# 4.3. Null-positive Rank Test

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/a91a5869-8143-4ffb-8c0f-daf0250f6660)

- The performance of the model has increased in top-1 rank setting (0 threshold, 0-Acc) and all variants of non-triviality have improved for both models.

## Conclusion
```
We introduce persona-knowledge dual context retrieval method PK-ICR in this paper. We perform QA-informed prompt-augmentations of data that successfully exploit the interactions between multiple dialogue components. We perform zero-shot top-1 knowledge retrieval and precise persona scoring. We present a novel evaluation method of nullpositive rank test as to isolate the hard-negative effect of Persona-augmented Dialogue. We obtain SOTA results on both retrieval tasks of the Call For Customized Conversation benchmark and report the alignment of the non-triviality metric with threshold-free performance. With our research, we hope to stimulate readers to model dialogue context as an interactive whole of multiple components.
```

## Limitations
```
Our cross-task adaptation of dialogue grounding retrieval to QA task is limited in terms of the target task and our prompt construction. In addition, retrieval models informed by inductive bias for multi-context scenarios could further improve our methodology. We specifically study multi-context interactions and retrieval in dialogues, which is a relevant and novel problem for advancing broadly capable dialogue systems. As an extension to our research, future work could also report on modeling downstream generation tasks based on grounding interactions.
```
