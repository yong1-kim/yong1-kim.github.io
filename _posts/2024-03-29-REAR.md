---
layout: post
title:  "[Arxiv 2402] REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain Question Answering"
date:   2024-03-29 00:01:00 +0900
use_math: true
categories: [Retrieval, LLM, PLM]
---

[[pdf]](https://arxiv.org/pdf/2402.17497.pdf)  &emsp;
[[github]](https://github.com/RUCAIBox/REAR)

**Yuhao Wang<sup>1∗</sup>, Ruiyang Ren<sup>1∗</sup>, Junyi Li<sup>1,3</sup>, Wayne Xin Zhao<sup>1†</sup>, Jing Liu<sup>4†</sup>, Ji-Rong Wen<sup>1,2</sup>**
<br><sup>1</sup> Gaoling School of Artificial Intelligence, Renmin University of China <sup>2</sup> School of Information, Renmin University of China <sup>3</sup> DIRO, Université de Montréal <sup>4</sup> Baidu Inc. &emsp;

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/87f16237-fa93-4425-b0bf-cb7e00bd12be)

# Abstract
- (**RAG and Weakness**) Internal parametric knowledge 의 한계를 극복하기 위해, Retrieval-augmented generation (RAG) 이 활발히 연구되고 있다. 그러나, RAG 에서 LLM 이 retrieved document 의 relevance 를 정확하게 평가할 수 없어, 오히려 RAG 가 옳지 못한 결과를 추출하는 경우가 있다.
- (<span style='color:green;font-weight:bold'> REAR </span>) 이 문제를 해결하기 위해, 저자들은 REAR:Relevance-Aware Retireval augmented approach 를 제안한다. LLM 으로 하여금 source relevance 의 self-awareness 를 발전시키고, 이를 RAG 시스템에서 잘 활용할 수 있게 한다. 구체적으로, **rank head** 를 이용하는 방법을 활용한다.
- (**Experiment**) 네 개의 open-domain QA (ODQA) 에서 기존의 RAG 방법들 보다 압도적으로 뛰어난 성능을 보인다.

## 1. Introduction

<span style='color:green;font-weight:bold'> ▶ Retrieval-Augmented Generation (RAG)</span>
<br>
Large Language Model (LLM) 이 여러 task 에서 좋은 성능을 보여주지만, open-domain QA (ODQA) 와 같은 knowledge-intensive task 에 대해서는 고전(struggle)하는 경향이 있다.
이를 위해, external knowledge 를 retrieval 해와서 generation 에 도움을 주도록 하는 Retrieval-augmented generation (RAG) 에 대한 연구가 활발하다. 

그러나, RAG 를 활용한 점에도 단점이 존재한다.
첫째로, <span style='background-color: #ffdce0'> retrieved result 가 irrelevant content 를 포함하고 있 을 경우, LLM 을 mislead 하여 정확하지 않은 답변이 생성된다([[1]](https://aclanthology.org/2023.acl-long.546.pdf),[[2]](https://arxiv.org/pdf/2307.11019.pdf)) </span>
또한, <span style='background-color: #ffdce0'> 성능향상을 위해 여러 문서를 retrieval 해올 경우 noise 가 영향을 미치기도 한다.([[3]](https://arxiv.org/pdf/2307.03172.pdf),[[4]](https://arxiv.org/pdf/2302.00093.pdf)) </span>
**따라서, LLM 은 irrelevant document 를 filtering 하면서 noisy content 를 피해야하는 문제점에 직면하고 있다.**

<span style='color:green;font-weight:bold'> ▶ Enhancing Robustness of RAG system </span>
<br>
최근 여러 연구([SELF-RAG[5]](https://arxiv.org/pdf/2310.11511.pdf),[SAIL[6]](https://arxiv.org/pdf/2305.15225.pdf),[RobustLM[7]](https://arxiv.org/pdf/2310.01558.pdf)) 에서 RAG 의 robustness 를 발전시키려는 시도들이 있다.
Self-RAG 의 경우, special token 을 활용해 document 가 relevant 한지를 discriminate 하여 generation 단계에서 활용 여부를 결정하고, RobustLM 은 document 가 relevant 한지 discriminate 하도록 LLM 을 prompting 한 이후 generation 을 진행한다.
<span style='background-color: #ffdce0'> 그러나, 이 방식들은 document relevance 를 binary label 로 분류하기 때문에, highly sparse 하고 fine-grained relevance 를 capture 하지 못한다.  </span>

<span style='color:green;font-weight:bold'> ▶ REAR: Relevance-Aware Retireval-augmented Approach </span>
<br>
이에 저자들은 **REAR**:**RE**levance-**A**ware **R**etireval-augmented Approach 방법을 제안한다.
<span style='background-color: #dcffe4'> 이 방법론의 골자는 LLM 으로 하여금 source relevance 에 대한 self-awareness 를 발전시키고, LLM 이 스스로 external knowledge 를 adaptive 하게 활용하게 하는 것이다. </span>

REAR 는 모델 아키텍쳐와 모델 학습 방법에 main contribution 이 존재한다.
우선, Information Retrieval (IR) 필드에서 성공한 reranker 방식을 이용하여, LLM 에 **rank head** 를 디자인하여 relevance assessment 를 하게 하여, irrelevant external knowledge 로 부터의 distraction 을 피하도록 돕는 relevance signal 을 capture 한다.
두번째로, coarse-grained signal 인 bianry discriminative method 의 한계점을 극복하기 위하여, **bi-granularity relevance fusion** 방식을 적용하여 fine-grained supervision 을 포함시키고, noise-resistant training 을 통해 discrimination ability 를 향상시킨다.

실험 결과, 네 개의 open-domain QA (ODQA) task 에서 REAR 가 좋은 성능을 보여준다.

## 2. Related Work
# 2.1. Open-domain Question Answering
- [REALM](https://proceedings.mlr.press/v119/guu20a.html)
- [RAG](https://arxiv.org/abs/2005.11401)
- [RETRO](https://arxiv.org/abs/2112.04426)
- [Incontext RALM](https://arxiv.org/abs/2302.00083)

# 2.2. Retrieval-augmented LLMs
**Jointly train LM and Retriever**
- [Atlas](https://arxiv.org/abs/2208.03299)
- [RA-DIT](https://arxiv.org/abs/2310.01352)

**Enhancing retrieval quality**
- [Expanding knowledge source](https://arxiv.org/abs/2305.10998)
- [Query-rewriting](https://arxiv.org/abs/2310.06117)

**Enhancing Robustness of RAG system**
- [SELF-RAG](https://arxiv.org/pdf/2310.11511.pdf)
- [SAIL](https://arxiv.org/pdf/2305.15225.pdf)
- [RobustLM](https://arxiv.org/pdf/2310.01558.pdf)

## 3. Task Formulation
[Task]
<br>
- Open-domain Question Answering (ODQA)

[Notation]
<br>
- query $q$
- retrieved top-k document $D = [d_i]_{i=1}^k$
- answer set $A = [a_i]_{i=1}^k = [LLM(q,d_i)|d_i \in D]$

[key aspect]
<br>
- _Identifying relevant references_ : precise evaluation of relevance between queires and documents
- _Reducing the influence of irrelevant content_ : leveraging relevance singal for noise-resistant generation

## 4. Methodology
# 4.1. Relevance-Aware RAG Architecture
모델 아키텍쳐의 그림은 아래와 같으며, (1) Relevance Assessment, (2) Relevance-guided Generation, 그리고 (3) Final Answer Routing 세 가지 파트로 이뤄진다.

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/551d423f-340d-4ab8-84c1-829d25c48d6e)

<span style='color:green;font-weight:bold'> (1) Relevance Assessment </span>
<br>
<span style='background-color: #dcffe4'> 첫번째로, Reranker 모듈을 활용하여 query-document pair 들의 relevance score 를 평가하여 reranking 한다. </span>
구체적으로, LLM 에 **rank head** 를 도입하는데, 이것은 query 와 document 사이의 relevance signal 를 capture 하기 위한 장치이다.

우선 LLM decoder 가 query-document pair 를 input 으로 받아 embedding $v_{rel}$ 을 mapping 한다.

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/1af1d7af-224a-4ef5-9ab8-9b7eecc63e02)

이후 $v_{rel}$ 은 _RANKHEAD_ 를 통해 score $s_{rel}$ 로 변환된다.

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/c3c15373-e27d-4778-8772-7c237bfdc4d5)

여기서 _RANKHEAD_ 는 linear projection layer 를 통해 구현된다.
RANKHEAD 는 relevance assessment 에 특화되어 있으면서도, LLM 의 모듈을 건드리지 않는 장점이 있으며, $s_{rel}$ 은 LLM 의 interanal state 를 기반으로 직접적으로 optimize 될 수 있다.

<span style='color:green;font-weight:bold'> (2) Relevance-guided Generation </span>
<br>
REAR 는 LLM 으로 하여금 relevance assessment score 를 integrate 하도록 학습한다.
$s_{rel}$ 은 scalar value 이므로, 이것을 LLM 에게 전달하기 위해 추가적인 embedding layer 를 구성한다.

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/8e3a6e16-b894-4c89-948d-03d0aefea1a4)

이 임베딩 벡터는 LLM 으로 하여금 answer $a$ 를 생성할 수 있는 cue 가 되어, 아래와 같이 answer 가 generate 된다.

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/bf52f652-d2de-4712-85ab-79689ec0b4a5)

주목할 것은 $v_{rel}$을 굳이 학습에 사용하지 않고 $v_{guide}$를 학습시켰다는 점인데, 저잗르은 $v_{rel}$은 relevance assessment 에 집중하고, $v_{guide}$는 answer generation 에 집중하게 하기 위함이라고 설명한다.
추후 실험파트에서 둘을 나누는 것이, 즉 distinct 하게 가져가는 것이 효과적임을 보인다.

<span style='color:green;font-weight:bold'> (3) Final Answer Routing </span>
<br>
$k$ 개의 document $D = [d_i]_{i=1}^k$ 에 대해, 위의 식들을 활용하여 $k$ 개의 answer set $A = [a_i]_{i=1}^k$ 를 얻을 수 있다.
저자들은 여러 retrieved documents 로 부터의 generation process 를 <span style='background-color: #dcffe4'> navigating distinct reasoning paths </span> 로 보고 가장 reliable 한 routing path 를 찾아 final answer 를 완성한다.
두 가지 routing strategy 를 취하는데, 하나는 path-reliability 이고 다른 하나는 knowledge-consistency 이다.

- **Path-reliability routing**

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/94b4538b-5f97-4819-9572-fc155bbb7a55)

가장 직관적인 방법으로 가장 높은 relevance score 를 갖는 answer 를 추출하는 것이다.

- **Knowledge-consistency routing**

Chain-of-Thought 에서의 self-consistency([[9]](https://arxiv.org/pdf/2203.11171.pdf),[[10]](https://arxiv.org/pdf/2201.11903.pdf)) 에 영향을 받아, external knowledge 중 internal knowledge 와 가장 highly consistent 한 에 기반하여 answer 를 선택하는 방식이다.

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/bc8ce686-0785-4d59-a757-48f729f4bdc3)

우선, 위의 식처럼 proxy relevance $\hat{s_{rel}}$ 을 0 으로 설정하고 inverse perplexity 를 구한다. Relevance score 를 0으로 설정하였기 때문에, 이것은 <span style='background-color: #dcffe4'> internal knowledge 들에만 기반한 answer 를 생성한다. </span>
만약, **이 경우에도 answer 의 perplexity 가 작다면, 해당 external knowledge 가 internal knowledge 와 유사한 것이다**.

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/8613a613-44db-4223-9f9c-1368eee85221)

따라서 저자들은 위와 같이 특정 treshold $\gamma$ 를 넘는 document 들을 candidate 으로 하여, 

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/11ef6cb3-2138-4bae-a4e4-03db102fdf19)

위와 같이 knoweldge consistency score $c_i$를 활용하여 final answer 를 routing 한다.

# 4.2. Model Training

<span style='color:green;font-weight:bold'> (1) Bi-granularity Relevance Fusion </span>
<br>
Retrieved document 에 대한 믿을 수 있는 활용을 위해서는 정확한 relevance assessment 가 매우 중요하다.
<span style='background-color: #ffdce0'> 
기존의 방법들은 binary discrimination task 를 통해 coarse-grained 방식을 채택하기 때문에, complex ODQA task 를 풀기엔 충분한 evidence 를 제공하지 못한다(고 저자들은 주장한다.)  </span>
이에 저자들은 **fine-grained ranking optimization objective** 를 모델 학습에 추가한다.

우선, coarse-grained supervision 을 위해, document $D$ 들을 "irrelevant" 와 "relevant" 로 labeling 한 후 optimize 한다.

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/2a5fbd7a-e508-4ff6-a8e0-4df8411c0121)

$\sigma$ 는 LLM 이 예측한 query-document pair 의 assessment prob 의 normalized 버전이다.

이후, fine-grained supervision 을 위하여, $s_{rel}$ 을 활용하여 ranking preference constratints 를 준다.

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/edeeaf8f-2bb1-4e2a-9211-84670d2d1691)

최종적으로, 모델 학습을 위한 bi-granularity relevance fusion 은 아래와 같다.

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/14208ab1-b879-4d4c-aeb4-47445f55b16f)


<span style='color:green;font-weight:bold'> (2) Noise-resistant Training </span>
<br>

Relevant document 를 identifying 하는 것을 넘어, 저자들은 irrelevant content 가 noise 로써 LLM 의 generation 을 방해하는 것을 방지하고자 한다.
저자들은 negative document set 인 $D^{-}$를 정의하여 아래의 식으로 LLM 을 optimize 한다.

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/5a1d35d2-97d4-4300-b127-71c1696170cd)


최종적으로, 두 학습 방법을 모두 합친 REAR Framework 의 최종 loss 는 아래와 같다.

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/c275b4b7-0993-46e2-b9eb-0d463616a235)

<span style='color:green;font-weight:bold'> (3) Training Data Construction </span>
<br>
High-quality training data 를 얻기 위해 relevance label 을 얻는 과정과 negative sample 을 얻는 과정이 필요하다.

우선, $s_{rel}$ 을 얻기 위하여서는 cross-encoder 아키텍쳐가 각각 query 와 document 에 적용되어 relevance score $s_{ce}$ 로 계산된다.
이후 traditional binary annotating label $y$ 와 함께, generated score 는 아래와 같이 주어진다.

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/82cc46aa-42f3-46b2-89eb-3b8abe3258d7)

그리고, **irrevant document sampling** 을 위하여, [SimANS](https://aclanthology.org/2022.emnlp-industry.56.pdf) (Simple Ambiguous Negative Sampling) 를 정제하여 너무 쉽거나(uninformative하다), 너무 어려운 (false negative) negative sample 을 방지한다.

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/9925092c-46fa-4927-8a73-c742ef4f9db1)

## 5. Experiments
# 5.1. Experimental Setup
- **Datasets** : Natural Questions(NQ), TriviaQA, SQuAD, WebQuestions
- **Baselines1 | Retrieval augmentation based prompt methods** : Llama2-Chat, Mistral-Chat, Baichuan2-Chat, ChatGLM3 | Direct Retreival Augmented QA, Judge-then-generate, Rank-then-generate
- **Baselines2 | Specially designed RAG methods** : Self-RAG, RobustLM
- **Metrics** : JAcc, Hit, Exact Match(EM), F1
   
# 5.2. Main Results

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/933b770a-041a-40c2-825d-5e3e68d0d8b7)

- <span style='background-color: #dcffe4'> REAR 는 coarse-grained 보다 좋으며 (높은 JAcc), fine-grained 보다도 좋고 (높은 Hit), 가장 좋은 generation 성능 (높은 EM 과 F1) 을 보인다. </span>
- <span style='background-color: #dcffe4'> 같은 데이터셋을 학습한 RobustLM 과 비교하여 좋은 성능을 보인다. </span>
- <span style='background-color: #dcffe4'> RobustLM 과 REAR 모두 Self-RAG 보다 좋은 성능을 보여, 이 dataset construction 방법이 더 우수함을 검증한다. </span>
- <span style='background-color: #dcffe4'> ChatGLM3 와 Mistral 같은 LLM 들은 top-10 retrieval 에서 성능이 올랐는데, 이를 통해 irrelevant document 를 filtering 하는 것이 RAG 를 향상시키는데 매우 중요함을 검증한다.  </span>

# 5.3. Detaild Analysis
<span style='color:green;font-weight:bold'> (1)  Ablation Study </span>
<br>
![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/997bf617-09fc-464c-b32c-b5aa643c1ae6)


<span style='color:green;font-weight:bold'> (2)  Impact of Retrieved Documents </span>
<br>
**Single document Setting**

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/c3acd02c-d977-4c10-a147-e361af6af0d6)

- Top-1 retrieved document 만을 활용하는 single document setting 이다.
- <span style='background-color: #dcffe4'> Relevant Document 에 대해 Finetuned Self-RAG 와 finetuned REAR 가 모두 4-shot LLM 들 보다 훨씬 성능이 좋았다.  </span>
- <span style='background-color: #dcffe4'> 그러나 Irelevant Document 에 대해서는 REAR 만 성능이 좋아져, REAR 가 robustness 를 갖춤을 보인다. </span>

**Multi document Setting**

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/ae80f8e9-8041-40ef-a185-33e8ea74bcf5)

- Multiple document setting 으로 total number 와 relevance degree 의 impact 에 대한 분석을 진행한다.
- 왼쪽 figure는 document number, 오른쪽 figure 는 retriever capability 에 관한 분석이다.
- <span style='background-color: #dcffe4'> REAR 는 single document setting (top retrived one) 에서도 좋은 성능을 보인다. </span>
- <span style='background-color: #dcffe4'> REAR 는 BM25 같은 weakest retriever 를 해도 가장 좋은 dense retriever 를 활용한 baseline 들보다 좋은 성능을 보인다. </span>

## 6. Conclusion
```
In this paper, we aimed to enhance the selfawareness of source relevance in RAG systems, and proposed REAR, a RElevance-Aware Retrievalaugmented approach for open-domain question answering (QA). For model architecture, we explicitly incorporated a specially designed rank head to precisely capture the relevance signals, and employed it to guide the utilization of external knowledge. For model training, we designed an improved training method with bi-granularity relevance fusion and noise-resistant training, which enhance the capacities of fine-grained relevance assessment and adaptive use of retrieved documents. Extensive experiments on four datasets demonstrate the effectiveness of REAR’s relevance assessment and knowledge utilization.
As future work, we will extend the proposed approach REAR to dealing with more fine-grained source utilization (e.g., passage or sentence level augmentation), and also consider applying REAR to other knowledge-intensive tasks.
```
## Limitations
```
For LLMs, the challenge of being misled by irrelevant retrieved documents is a significant obstacle, underscoring the crucial need for enhancing LLMs’ ability to adaptively utilize retrieved documents. In response to this issue, our work has concentrated on refining the architecture and training methods to bolster the effective use of retrieved documents by LLMs. We have implemented document-level relevance assessment and dynamic utilization strategies, significantly boosting the factual accuracy of generated content by LLMs. However, our current approach has not delved into guiding LLMs to focus more granularly on key sentences or tokens within the retrieved documents.
Moreover, the applicability of our methods across a broader spectrum of RAG tasks, such as those encompassed by the KILT benchmark, remains to be thoroughly evaluated. This gap presents a pivotal area for our future investigations.
```

