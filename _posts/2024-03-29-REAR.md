---
layout: post
title:  "[Arxiv 2402] REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain Question Answering"
date:   2024-03-29 00:01:00 +0900
use_math: true
categories: [Retrieval, LLM, PLM]
---

[[pdf]](https://arxiv.org/pdf/2402.17497.pdf)  &emsp;
[[github]](https://github.com/RUCAIBox/REAR)

**Yuhao Wang<sup>1∗</sup>, Ruiyang Ren<sup>1∗</sup>, Junyi Li<sup>1,3</sup>, Wayne Xin Zhao<sup>1†</sup>, Jing Liu<sup>4†</sup>, Ji-Rong Wen<sup>1,2</sup>**
<br><sup>1</sup> Gaoling School of Artificial Intelligence, Renmin University of China <sup>2</sup> School of Information, Renmin University of China <sup>3</sup> DIRO, Université de Montréal <sup>4</sup> Baidu Inc. &emsp;

![image](https://github.com/yong1-kim/yong1-kim.github.io/assets/42200027/87f16237-fa93-4425-b0bf-cb7e00bd12be)

# Abstract
- (**RAG and Weakness**) Internal parametric knowledge 의 한계를 극복하기 위해, Retrieval-augmented generation (RAG) 이 활발히 연구되고 있다. 그러나, RAG 에서 LLM 이 retrieved document 의 relevance 를 정확하게 평가할 수 없어, 오히려 RAG 가 옳지 못한 결과를 추출하는 경우가 있다.
- (<span style='color:green;font-weight:bold'> REAR </span>) 이 문제를 해결하기 위해, 저자들은 REAR:Relevance-Aware Retireval augmented approach 를 제안한다. LLM 으로 하여금 source relevance 의 self-awareness 를 발전시키고, 이를 RAG 시스템에서 잘 활용할 수 있게 한다. 구체적으로, **rank head** 를 이용하는 방법을 활용한다.
- (**Experiment**) 네 개의 open-domain QA (ODQA) 에서 기존의 RAG 방법들 보다 압도적으로 뛰어난 성능을 보인다.

## 1. Introduction

## 2. Related Work
# 2.1. Open-domain Question Answering

# 2.2. Retrieval-augmented LLMs

## 3. Task Formulation

## 4. Methodology
# 4.1. Relevance-Aware RAG Architecture

# 4.2. Model Training

# 4.3. Model Discussion

## 5. Experiments
# 5.1. Experimental Setup

# 5.2. Main Results

# 5.3. Detaild Analysis

## 6. Conclusion
```
In this paper, we aimed to enhance the selfawareness of source relevance in RAG systems, and proposed REAR, a RElevance-Aware Retrievalaugmented approach for open-domain question answering (QA). For model architecture, we explicitly incorporated a specially designed rank head to precisely capture the relevance signals, and employed it to guide the utilization of external knowledge. For model training, we designed an improved training method with bi-granularity relevance fusion and noise-resistant training, which enhance the capacities of fine-grained relevance assessment and adaptive use of retrieved documents. Extensive experiments on four datasets demonstrate the effectiveness of REAR’s relevance assessment and knowledge utilization.
As future work, we will extend the proposed approach REAR to dealing with more fine-grained source utilization (e.g., passage or sentence level augmentation), and also consider applying REAR to other knowledge-intensive tasks.
```
## Limitations
```
For LLMs, the challenge of being misled by irrelevant retrieved documents is a significant obstacle, underscoring the crucial need for enhancing LLMs’ ability to adaptively utilize retrieved documents. In response to this issue, our work has concentrated on refining the architecture and training methods to bolster the effective use of retrieved documents by LLMs. We have implemented document-level relevance assessment and dynamic utilization strategies, significantly boosting the factual accuracy of generated content by LLMs. However, our current approach has not delved into guiding LLMs to focus more granularly on key sentences or tokens within the retrieved documents.
Moreover, the applicability of our methods across a broader spectrum of RAG tasks, such as those encompassed by the KILT benchmark, remains to be thoroughly evaluated. This gap presents a pivotal area for our future investigations.
```



<span style='color:green;font-weight:bold'> 초록색볼드체 </span>
<br>
<span style='background-color: #dcffe4'> 초록색배경 </span>
<span style='background-color: #ffdce0'> 빨간색배경 </span>

<span style='color:green;font-weight:bold'> ▶ </span>
<br>
